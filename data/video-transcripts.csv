video_id,datetime,title,transcript
nPQkBGf55YA,2025-05-25T15:01:12.000000,30 AI Buzzwords Explained For Entrepreneurs,n/a
OLmKFj-_5Uw,2025-05-18T15:00:40.000000,AI Essentials for Entrepreneurs (in 17 Minutes),n/a
BUTjcAjfMgY,2025-05-11T15:00:27.000000,ML Foundations for AI Engineers (in 34 Minutes),"Hey everyone, I'm Sha. In this video, I'll be covering machine learning fundamentals that every AI engineer needs to know. While you'll find endless textbooks and articles on this subject, my goal with this video is to give builders a short and accessible guide to the most critical concepts in machine learning. Here, I'm going to focus on five key points. I'll first talk about intelligence and world models. Then I'll talk about three different ways computers can learn world models through machine learning, deep learning, and reinforcement learning. Finally, I'll talk about the most important ingredient in machine learning, which is data. So let's talk about intelligence. Intelligence requires understanding how the world works. But of course, the world is a big and complicated place. So in order to understand it, you need to develop models of the world so that you can compress the complicated and vast reality that we live in into something you can fit into your head. And put simply, a model is something that allows you to make predictions. For example, if you were to look out the window and were to see dark clouds like this, your mental model of the world and understanding that dark clouds typically mean it's going to rain later will allow you to make a prediction that it's going to rain later. We all have mental models of the world that allow us to make predictions and to achieve desired outcomes. And there are two main ways that we develop these models. The first is learning from others like our parents or mentors. And the second is learning directly from our experiences. And as it turns out, computers can learn models of the world in very much the same way. Here I'll talk about three different ways computers can develop these models of the world. So here when I say the word learning, what I mean by that is getting computers to do things without explicit instructions. So this is in contrast to traditional software development where programmers use their own intelligence to develop step-by-step instructions for performing tasks or making decisions and then translating those instructions into computer code. Here I'm going to focus on three different ways we can get computers to do things without explicit instructions. The first is machine learning which is actually an umbrella term for everything that I'm going to talk about in this video. The second is deep learning, which is a special type of machine learning. And then finally, I'll talk about reinforcement learning, which is also a special type of machine learning, but often you'll see it combined with deep learning in modern applications. Starting with way number one, machine learning allows computers to learn tasks directly from data. And so machine learning consists of two key phases. The first phase is called training. And this involves gathering a training data set. So basically a set of examples. Passing this training data to a so-called learning algorithm and the result of this learning algorithm is a machine learning model. Once we have this machine learning model, we can use it to make predictions. So this brings us to phase two which is called inference where we give our machine learning model new data to make predictions which we can use in making decisions or solving problems. Let's dive into a bit more detail on how each of these two phases works. So I'll actually start with inference which is phase two because it's a little easier to understand. Inference involves using a model to make predictions. For example, suppose we have this simple linear model for predicting tomorrow's high temperature. The way this works is we take today's temperature high, which is represented by X. We plug it into this equation to make a prediction about tomorrow's high temperature, and this mapping between today's temperature and tomorrow's temperature is determined by these two so-called parameters M and B. The quality of our predictions are going to be determined on how well our parameters are aligned with reality. But you might be wondering how do we pick these parameters so that these are going to be accurate predictions. That brings us to the training phase which is where we fit a model's predictions to reality. The key intuition here is that in training, we want to minimize the discrepancy between our model's predictions and real world data. So the first step in doing this is quantifying the discrepancy between our model's predictions and reality. One way we can do this is through this equation shown here. There's a lot of math on this slide, but I'm going to go through it one step at a time. The first thing to look at is this first term here, which corresponds to the actual values of the quantity we're trying to predict. For example, if we're trying to predict tomorrow's temperature high, y is just going to be a collection of temperature recordings that we collect from reality. The second thing to look at is this term on the right, which are our predicted values. So this is exactly the same equation we saw on the previous slide, but now we've written it out in terms of multiple examples. Instead of just one input x, we have n inputs x1, x2 all the way to xn. So here are our input values. And then we have our two parameter values m and b. m gets multiplied by each value in this matrix. And then we just write out this matrix of one. So we can multiply b by each of these elements to make the math work out here. But you can just think of this as n different linear equations. So m * x1 + b, m * x2 + b and so on and so forth. By taking the difference between these actual values and our model's predictions of the values, we are quantifying the discrepancy between our model's predictions and reality. We call this quantity our loss function. Now that we've quantified this discrepancy, the goal of training is to find the parameter values m and b that corresponds to the smallest possible loss function. While we could just randomly guess a bunch of parameter values to try to find the one with the smallest loss function. It turns out that we can solve this problem in a closed form using math. The way that looks is we're just going to rewrite some terms. So we'll define this matrix X which will be all our input values and all these ones stacked side by side. Then we'll put our parameter values in their own little column matrix. Finally, we'll just define our column of actual values as this vector y. With these new terms, we can rewrite our loss function like this. So we have the norm squared of y - x theta. Writing this out a different way, it looks like this. So it's y - x theta transpose * y - x theta. And then we can expand this term out and it looks like this. So it looks all long and scary, but you don't really have to worry about that. The key thing to know here is that we can find the values of theta that minimize this loss function by computing its gradient, setting it equal to zero, and then solving for our parameter values. The way that looks is as follows. The gradient behaves a lot like a derivative which you probably learned in high school calculus. So if we're taking the gradient of L with respect to theta, we can see that this term goes away. This term is linear in theta. So this just becomes this. And then this term becomes this term here. This is our gradient of L. Then we can set it equal to zero which gives us this equation. And then rearranging it, it'll give us this expression here. And then finally we can just solve for theta and those values will be our optimal parameter values. And so if we assume that this matrix here is invertible then this becomes our optimal parameter values for theta. Okay. So what just happened? We just went through all this math and we computed this gradient. Did all this complicated stuff. But here's the punchline. We can get the optimal parameter values for our linear model simply from training examples. So what is X? X is a set of input data that we collect. Y is a set of target data that we collect and we can combine them to find our optimal parameter values. So put another way, the key point of machine learning is that computers can fit models to reality using data and math. While a linear model is one of the simplest models you can work with in machine learning, this idea of fitting model parameters to real world data is a common thread you'll see throughout all machine learning models. And just to list a few other popular machine learning techniques you might come across. There's also logistic regression, which is a lot like linear regression, but instead of predicting a continuous target, it predicts a binary target. So that's why it lets you do classification instead of regression. There's also decision tree which is a fundamentally different way of training models, but it's also very powerful and relatively easy to interpret. And then you have extensions of decision trees by combining many trees together into a so-called forest or boosting decision trees on top of each other like an XG boost as well as more traditional approaches like support vector machines which can be used for both regression and classification. So all the approaches that we see here is what we might call traditional machine learning. While you can go very far with them, they do have their limitations. One such limitation is that often to get these approaches to work well, it requires a significant amount of time and domain expertise to choose the right input variables for your model. For example, we used today's temperature to predict tomorrow's temperature. And so that's probably a pretty good predictor of tomorrow's daily high. However, if we used the number of cappuccinos I drank as our predictor, it might have some sort of correlation with tomorrow's temperature high, but it probably wouldn't do a very good job at this prediction task. So, this process of picking the right input variables to your machine learning model is called feature engineering, and it is something that can make or break your model's performance. However, the importance of feature engineering began to wne around 2020 with the rise of deep learning. Deep learning is a specific type of machine learning which involves training neural networks which can learn optimal features for a specific task all on their own. A cartoon diagram of this which CHBT helped me put together is let's say you're trying to create a deep learning model to do image classification. The way this would work is that you would just take raw image pixels as the inputs to your network. The early layers of the neural network would represent pretty basic features like edges and textures of images. The layers toward the middle would start to represent things like eyes or ears or whiskers, higher level features that might be in an image. And then the later layers would represent higher level concepts like entire objects or animals or something like that. These final layers make it a lot easier to do this prediction task because if you have a quantity that represents the catness of an image, you can basically use a linear model to make this prediction. This is the power of deep learning. By simply curating raw data with corresponding labels, you can train a neural network to do basically anything. The key technique in deep learning are neural networks, which are a series of operations that can approximate practically any function. The fundamental building block of a neural network is a so-called neuron, which involves the following steps. It takes a set of inputs, multiplies them by some corresponding weights, and sums them together. It applies some bias term. So just adds a scalar value to the summation. And then it passes this whole thing through a nonlinear function called an activation function. And then this will spit out the neurons output. These three stages of summing inputs, adding bias, and passing through an activation is fundamentally what a neuron is. Writing this out mathematically, Xi are the inputs to the neuron. W, I and B are the parameters of the neuron. G is this nonlinear function and Z is the output of the neuron. And so looking at this, we see that this looks a lot like the linear model that we saw in the previous slide. The key difference, however, is that we now have this nonlinear activation function. And so this is actually a critical aspect of a neural network because it is these nonlinearities that allow the network to be able to approximate practically any function. The way we can build up networks from neurons is we can take these inputs and we combine them into a neuron. So basically compressing everything we saw in the previous slide to this single circle. But we can combine multiple neurons together to form so-called layers. And then we can combine multiple layers together to form entire neural networks. So this is how we take that fundamental building block of a neuron, build them into layers, and then build layers into networks. But of course, neural networks in practice are much more sophisticated than the example I showed on the previous slide. So, while that vanilla neuron we saw in the previous slide is widely used, there's also another one called a long short-term memory neuron or LSTM, which is also pretty common when working with sequenced data. Also, there are a wide range of activation functions that are commonly used. There are many more than just these, but there's the relu, which is shown here. There's a sigmoid, which is kind of like a S-shaped thing. There's a tanh which is also a S-shaped thing but it has a different output range. And then there's a softmax which basically translates any inputs to a probability distribution. There are also several different types of layers that you'll commonly see in the wild. So before we just saw these fully connected layers where every neuron from a previous layer is connected to every neuron in a subsequent layer. But there are also recurrent layers, convolutional layers, attention layers which are critical in transformers which is probably the most popular architecture today. There also pooling layers, normalization layers and dropout layers. Finally, there are a zoo of different network architectures. Before we just saw a simple feed forward neural network which is the most basic architecture. There's also a recurrent neural network which handles sequential data. There's a convolutional neural network which is used a lot in image processing and then of course there's the transformer which has revolutionized natural language processing and the architecture behind large language models. So that's the anatomy of neural networks. But how do we actually train these objects on our data? At the most basic level just like how we fit our linear model to real world data we can also fit our neural network to real world data. However, the trouble with neural networks is that the loss function is going to be significantly more complicated than the one we saw for the linear model. Just to show you a cartoon diagram of this, the loss function of a neural network will have this jagged and messy terrain of different peaks and valleys which we have to figure out a way to navigate so that we can find the smallest value. Again, the fundamental goal of training is to minimize the discrepancy between our model's predictions and reality, which is equivalent to picking parameter values for our model, which minimizes the loss function. So, in terms of this picture here, we want to pick the parameter values that correspond with this point in the loss landscape. But unfortunately, since this lost landscape is so bumpy and perilous, we can't just simply write out a single equation that spits out the optimal values like we saw with the linear model. So in practice, what we need is some kind of algorithm to search this space of possible parameters in such a way that we get closer and closer to this minimal point that we're trying to reach. So it turns out we can again turn to our handy gradient to help us do this. The gradient is helpful because at any spot in this loss landscape if we compute the gradient it will point in the direction of greatest ascent. Basically it'll point in the direction where the slope of the lost landscape is steepest. So if we take the minus of the gradient basically pointing in the opposite direction this is going to point us in the direction of greatest descent. What that means is we can use the gradient of our loss function with respect to our parameters as a guide to point us toward the minimal value of this loss landscape. So, what that looks like is if we're at this spot here, we'll compute the gradient and then move in the direction of this black arrow and take a step and then we'll be in a new spot and then we'll do the same thing. We'll compute the gradient and then take a step in the opposite direction again, compute the gradient, take a step in the opposite direction, and then just keep doing that until we get closer and closer to this optimal value. This algorithm of computing the gradient, using it to decide what direction to move in, taking a step, and then repeating the process is called gradient descent. Put another way, it's an algorithm for updating model parameters to minimize the loss. The equation for this is shown here where theta i are the old parameters of our model. This will be a vector. So you can imagine we have hundreds, thousands, millions, billions of parameter values in our neural network. theta i1 are the new parameters. So we're going to update the old parameters in some way to generate the new ones. And the way we do that is using this term on the right here. There are two key components of how we update our model parameters. The first is the gradient which we just talked about in the previous slide. So the gradient is telling us which direction to go in. And then we have this other term here, gamma, also called the learning rate, which controls how big our steps are. A very large learning rate means we'll take really big leaps when following the gradient, while a really small learning rate corresponds to very small steps in this descent algorithm. The learning rate is something that you need to tweak and refine because if your learning rate is too big, you might overshoot the optimal value. While if the learning rate is too small, you may never get there. But in practice, it's rare people use gradient descent because can have convergence issues. The data you use in computing your loss function is the full data set and this causes some problems. At the other extreme, there's also stochastic gradient descent where basically the updates are based on picking a random data point in your training data set and doing the parameter updates based on that. What's much more common is to use so-called mini batches. Instead of using the full data set or just a single data point, you'll take a random sample of say 8 or 16 32 whatever examples and compute your loss based on those. But by far the most common optimizer or update strategy for neural networks today is atom, which is short for adaptive momentum estimation. This takes into account not just the gradient itself but this momentum term to make the trading process more stable and efficient. So a couple things came up when we were talking about optimizers. One is this idea of a learning rate which is a number that we have to choose as model developers but also this idea of the mini batch in which we choose the number of samples to include in our loss calculation. And so the learning rate and the mini batch size can have a significant impact on the success of your model training. This brings up the idea of hyperparameters which are values that guide the training process. So these are not like the model parameters which we learn from data but rather these are values that we set manually or that we tune through some other optimization strategy to develop the best final model. Some of the most common hyperparameters you'll see are epoch. So basically the number of epochs is the number of times that your optimizer goes through the entirety of your training data set. The learning rate we already talked about it controls the step size in your optimization process. The batch size we also talked about which is the number of samples you use to compute the loss. And then finally there's this dropout term. So we can introduce dropout layers into our neural network and then have some number maybe between 0 and 0.2 two, which is representing the fraction of neurons to randomly set to zero during the training process. And so, dropout is a very powerful approach to help avoid overfitting and making your models more robust. So far, in talking about machine learning and deep learning, we've focused on models that learn by example. In other words, we curate a training data set of inputs and outputs where we train a model to use the inputs to predict the output. This is a lot like how you and I might learn from like a teacher or a mentor who shows us how to perform a task through explicit examples. However, this isn't the only way that we can learn. In fact, a lot of times we don't learn from other people through examples, but rather we learn from our own personal experience of basically trying things, making mistakes and getting negative feedback from our environment or trying things and getting positive feedback from our environment. It turns out computers can learn in very much the same way. And this process is called reinforcement learning. This is where computers can learn through trial and error. So far we've talked about supervised learning where you have a human who curates these examples. So here's our training data set with a set of inputs and outputs and then we use these inputs and outputs to train a machine learning model. However, with reinforcement learning, the strategy is a bit different. We have a model that can interact directly with reality or some artificial environment that we construct and then the model gets rewarded for good actions which indirectly influences the model parameters and it just continues in this feedback loop until we have a model that does what we want it to do. While reinforcement learning is notoriously hard to get working well, it does hold a ton of promise and potential. And the central reason for this is that with reinforcement learning, models are not bound by human labeling or expertise. In supervised learning, the performance of our model is going to be bound by our ability to label highquality data or our existing knowledge or expertise. However, with reinforcement learning, since the model is interacting with the environment on its own, it can find its own ways of solving problems and potentially surpass even expert human performance. A real world example of this is Alph Go, which is a deep learning model that Deep Mind developed to play the game of Go. So there's actually a whole movie about this, but in this nature paper, they showed this plot here comparing a reinforcement learningbased deep neural network, a supervised learning based deep neural network, and the ELO rating of this Go Grandmaster. And we can see that the Grandmaster has just below a 4,000 ELO rating. And that the supervised learning model gets good really quickly because it's learning from experts and grandmasters. But you see that it never can quite surpass the grandmaster level. On the other hand, the reinforcement learning algorithm, even though it really sucks at the beginning, it learns to play the game of go not from experts, but by playing itself and iteratively and slowly getting rewarded for winning games. This allows the model to be completely decoupled from how humans play the game of Go and develop its own novel strategies for playing the game and eventually surpassing human grandmaster performance. Going one layer deeper into how reinforcement learning works. The basic idea here is to update model parameters to maximize some reward function. In the game of Go, the reward they used was winning the game, but this could be anything. More recently in training DeepSeek R1, DeepSseek used the correctness to math and coding solutions as the reward to their model. And so a key difference between reinforcement learning and the supervised learning approaches we've seen so far is in the quantity we're trying to optimize. So in linear regression and neural networks, we saw that we had a loss function that we were trying to minimize. But in reinforcement learning, we instead have a reward function that we're trying to maximize. One way we can write out this reward function is like this. And I know this is all scary complicated math, but let me walk through it one piece at a time. So J is our objective. This is the reward that we're trying to maximize. Log of pi theta is the model's output. And R is the reward at time t. The way to understand this is that in reinforcement learning, the model isn't producing a prediction. Rather, it's generating this probability distribution of different actions it might take given the current state of its environment. So, pi is just representing a probability distribution and pi theta is just saying that this probability distribution is parameterized by the parameters theta. A is representing the action at time t and then s is representing the state of the environment at time t. And the reason we use log here is that it makes the math a bit nicer to work with. And then this reward at time t is basically the feedback the model is getting for its specific action. So these probabilities and rewards are going to be summed over an entire episode or trajectory. And so if we're playing the game of go, an entire episode would be, let's say, an entire match of the game. Or if this was like a self-driving car, an episode would be from when the car starts moving to when it reaches its destination or crashes. And then a bit more details here. So again, we already said that pi theta is the probability of taking action a given state s based on model parameters. Tao is this full roll out of states and actions. So we have an initial state to which the model has an action to and gets a corresponding reward. Then there's another state which the model has an action to and gets another reward and so on and so forth. And then t is just the number of time steps in this full trajectory. So I know there's a lot of math and terms happening here. But here's the key intuition. What we want is to update these model parameters such that our model gives a high probability to actions which result in a high reward. So in that case you know if this probability is big when this reward is big then this whole term will be big and then once we average over the whole trajectory this objective will be as big as possible. So maximizing the objective is equivalent to having high probabilities for highreward actions. But from there we can update our model parameters in very much the same way we did in deep learning. Namely we have this parameter update rule where we have theta k which are the old parameters, theta ki which are the new parameters and then we have this update rule. So again, we take the gradient of our objective to serve as our guide in navigating this landscape and then we have the learning rate gamma. However, since we're not trying to minimize the reward and we're trying to maximize it, all we need to do is change the minus sign from gradient descent to a plus sign to give us this gradient ascent update rule. So everything I just described is based on this reinforce algorithm which is the reinforcement learning algorithm that underlies all modern reinforcement learning techniques. However, that came out in 1992 and since then there have been a lot of improvements and new approaches introduced. So here's a summary table of some popular ones. Reinforces what we've just been talking about in the previous slides. There's this trust region policy optimization approach which adds this KL divergence term and compares the current model action to the previous version of the model's action and this just improves training stability. There's PO which was used in reinforcement learning with human feedback which is what open AI used to make chat GBT and they basically do a similar thing as trust region policy optimization but they don't have this KL divergence term instead they have this clipping term to ensure that training is stable. Finally, there's this group relative policy optimization approach developed by DeepSeek and this is what they used in DeepSseek R1 and it basically combines PO with this KL divergence penalty to make training more stable. Additionally, they batch different examples into a group when computing this objective and updating the model parameters. For most of this video, I've talked about algorithms for optimizing parameter values and loss functions and neural networks and sophisticated fancy math. And this might give you the impression that algorithms are the most important thing when it comes to machine learning. However, that is very much not the case because at the end of the day, we are simply fitting models to our data. So if you have bad data, it doesn't matter how sophisticated your algorithm is or how much you tune your hyperparameters. If you're fitting a model to bad data, you're going to have a bad model. That's why I wanted to finish this talk to review what makes good data. And so there really two properties that make data good. The first is quantity. All things equal, more data are better than less data. When you don't have enough data, your model is prone to overfitting. So this is basically a situation where your model memorizes your training data set and doesn't generalize well to new examples. In other words, it leads to models that just aren't very helpful in practice. However, there is a situation where more data is not necessarily a good thing. And this is when your data is garbage. Because if you take bad data and train a model, even if you have a lot of it, you're going to have a bad model. And this is summarized by the old adage of garbage in, garbage out. That brings us to the second property of good data, which is quality. And the essence of quality is having data that faithfully represents reality. So there are two key aspects of this. The first is accuracy, which is the correctness of your data. For example, if your data set says someone is 35 years old when in reality the person is actually 53 years old, that is bad data. Another example is if your data says someone makes $12,000 a year when in fact they make $12,000 a month. That's another example of bad inaccurate data. The other aspect is diversity, which is basically the representativeness of your data. For example, if you have some kind of SAS product and you're trying to predict customer churn and you take data from a bunch of prousers of your SAS product and you use it to develop a customer churn model, this is completely fine if you're trying to predict churn of users of your pro plan. However, if you have an enterprise customer, this model probably won't be helpful in predicting the churn of an enterprise. In other words, it's not only important that your data are correct and accurately represent reality, but that they also contain examples of all the different scenarios you want to use your final model in. Just to summarize, good data comes down to quantity and quality. So, we want more data over less data. We want highquality data over lowquality data. And we want less highquality data over more lowquality data. All right, so I covered a ton of information in this video, so I just wanted to recap with a few key takeaways. First is that solving problems requires an accurate model of the world. And machine learning gives us a way to align models to reality using data and math. A specific type of machine learning is deep learning, which involves using neural networks to learn useful features and mappings from raw data. And a lot of times deep learning is combined with reinforcement learning which allows computers to learn by interacting with the world through trial and error. And finally, although there's a lot of math and fancy algorithms and machine learning, training good models comes down to having good data. And what this means is you need high volume, highquality training data sets. Although there were many things I couldn't cover in this short overview here, I hope it gave you more clarity around some buzzwords in machine learning and the confidence to continue your learning on your own. Toward that end, if you have any questions or suggestions for future topics you want me to cover, please let me know in the comments below. And as always, thank you so much for your time and thanks for watching."
uItWjWjH_Rs,2025-05-04T15:01:18.000000,How I (Vibe) Code with Cursor + Python,"Oh, that's slick. That is slick. We are now vibe coding. I guess we've been vibe coding for a while, but now it's absolutely official. Let's see if this works. Oh, yeah. A The thing about visualizations is that it just takes a lot of coding and trial and error to get it looking good, which makes it one of the best things to vibe code cuz no one's got time for that. Here we go. Yeah, look at that. But that looks really good. Whoa. Reinforcement Learning Plus 19. Hey everyone, I'm Shaw. In this video, I'm going to show you how I code with cursor. I'm going to do this by building an Upwork job dashboard completely from scratch using Python. My goal with this video is to show you how and when I use AI to code fast without accumulating insurmountable technical debt. Got cursor opened up here. And if you're not familiar with cursor, it's just an IDE. So, it's an integrated development environment. That's just a fancy way of saying it's an application you can use to write code. While there are many IDEs out there, Cursor is well known for being one of the first to integrate AI coding assistance directly into the user interface. Just to show you what Cursor looks like, it's actually built on top of VS Code. So, if you're familiar with that, the interface is basically exactly the same. And of course, there are many other AI powered idees out there. While I haven't used those before, I would imagine the user interface is going to be pretty similar. The key UI feature with cursor is this AI panel. We have this chat window where we can talk to an LLM and have it write us code or give us feedback or whatever it is. Interesting thing about cursor is that they have these two modes. They have this ask mode. So, this is kind of like question answer and it'll just spit back a response and give you like a code block that you can easily apply to some file that you're working on. But there's also this agent mode which is a bit more powerful because the agent is given a set of tools like rewriting files or running terminal commands. This allows the LLM to do much more of the development work than just a simple ask system. And then there's this manual which I don't use often, but basically this is if you want to limit the context that you're passing to the LLM, you can use this manual mode, but ask mode will automatically get context from your codebase as well as the agent. not going to go through all the UI components of cursor because that's not what this video is about here. I'm going to show you live me working through a real project so you can see all the nitty-gritty details of how I code with cursor, how I debug issues, and how I use LLMs to help me code much faster. I have this CSV file with a bunch of jobs from Upwork. This first one is a automation specialist for content distribution. We have the responsibilities, basically the job post here, but then we have other things like how much they're paying for it, the experience they're looking for, the estimated time. Here's just like a different view of it. This is when the job was posted. This is the title. This is the link. This is some data that was scraped. if the payment is verified of the client, the rating of the client, how much the client has spent, where they're based, how much it's paying, what they're looking for, the job description, and then some tags with the job, and some other things. All these headers are not very helpful, and there's a lot of missing data. So, this is going to be a common thing you run into in any kind of data science or data analytics project. You have a lot of information, but it's not clean. It's not in a format that's ready to start using. The first step here is going to be getting this data into a clean format so that we can create the dashboard. Typically this is a very timeconuming process but let's see how fast we can do it using claude. So we can select a bunch of different models with cursor. They have the popular ones from anthropic Google openai. So I have the pro plan with cursor. So, a lot of this stuff is included, but some of these models I guess you got to pay extra for, like these Max models or for 03 you got to pay extra for, but the default is to do the auto select. Basically, cursor has a way of deciding what will be the best model to use for whatever request. We see already the CSV file is available in the chat window. So, we can start using it as context to ask LLM's questions. The first thing we need to do is to clean this data. In order to decide what clean is, we need to figure out what data do we need to keep, what can we get rid of, how do we handle missing values and things like that. And so at this point, we really have three options. First option is that we can just sit down and manually design what we want the final table to look like and write out all these transformations that need to happen in order to take this raw CSV file and create it into the final table that we want. The second option is that we can have this collaboration with a large language model to kind of figure out how we want to design this final table and how to make the transformations. Or third, and finally, we can just have the LLM do this entire project for us. I'm going to do something in the middle cuz I already have somewhat of an idea of what I want to do, but maybe there's some gaps in my expectations because I haven't done this project before. So maybe doing this back and forth conversation is going to give me a bit more clarity and give us a nice action plan. I'm just going to give the LLM context on what problem I'm trying to solve and the solution I have in mind for it. I want to create a Upwork job dashboard based on the data in the attached CSV file. The goal is to get insight into the most high demand skills and services on Upwork to help freelancers define their services. That's kind of like the problem we're trying to solve and the solution bundled together. And then let's give it a direction. Can you help me transform the data in the CSV file for this dashboard? This is a pretty big ask. Not only are we asking the large language model to write some code for us to translate data set one into data set 2, but we're also having it figure out what transformations to make in the first place based on the context of this problem. And so let's see what happens. So, we can see that automatically the model will just start reading the CSV file and then it's going to be off to the races to start cleaning it. It looks like it's doing the entire project in one shot. So, let's see how good this did. We can see that it created a requirements file. It created this Python script. It also created a readme file for us. And then it wants us to do that and then run the script with bash. Basically, the agent just solved the entire project. I guess this is more option three than option two. But let's just humor the model and accept all these changes and then we'll create a new Python environment. Let's go here. So this is the script. So it's going to be using a few things. Map plot lib seour and wordcloud. And then let's see plain data extract skills. Interesting process upwork data. Let's just run it and see what happens. I'm going to open up a new one of these. So I'm just going to create a new environment env. And I'm going to activate this. Then I'll install the requirements. We could have also run these in the chat. So instead of like writing all this stuff out, we could have done this. But I guess there a couple problems because if we were to just run this command, it would install these requirements in my global Python environment. I don't want that. I wanted to have a special environment. That's why I didn't run these commands here. And generally, I don't like running the commands here. Even though that's one of the upsides of having an agent is that it can do these things on your behalf. I just prefer to run any kind of terminal command myself just so I know what's going on. So we have this then we can run this Python script. So I'll just copy paste that over. So this is the ultimate challenge with approach number three where we basically have the model do the entire task is that now that something has gone wrong, debugging it is going to be a major challenge. And I feel like another problem with this clean text is that it never checks for any kind of duplicate values or anything like that. This doesn't seem like it's going to be a promising direction. Okay, so let's take a step back. We can see that what the model came up with wasn't very helpful. So, let's try a different approach. We just undid everything cursor did cuz it was trying to do a lot. Let's not do it in agent mode this time. Let's do it in ask mode. And the great thing about chat is that it tends to explain what it wants to do while the agent just kind of was thinking everything. It didn't really tell us what its thought process was. I think this looks pretty good for the most part, but I don't want to go down the same rabbit hole we just did. So, let's try to build a solid foundation of this project so that the LLM has a bit more context and knows what direction to go in because when it is trying to do the whole thing all by itself, this gives us more opportunity for it to go off the rails or just do something that isn't super helpful or what we're looking for. So, I'm going to make a file called data prep and we're just going to prepare the data. We've already installed a bunch of stuff. And I guess the most important thing is we have pandas and we got a bunch of other stuff now that isn't helpful and we lost our requirements file. Let's deactivate the environment and then I'll delete this. So I think this is good because we saw what happened when the AI tried to do the entire project by itself. It generated a bunch of code and it ran really quickly, but an error came up. And while we could have spent time trying to debug that, I feel like the vibe of the whole application just didn't feel like it was going in the right direction of what I wanted. That's one of the situations where I'll just take a step back and start coding it myself. But like imagine if the AI did it and it basically transformed the data in a way that was aligned with what I had in mind. Maybe I would have gone that route of trying to debug and edit the code. But since the final result the model was going for was so different than what I had in mind. It just feels easier to start from scratch and then try again. The upside of cursor is not just this chat window here, but also this tab complete. So I just wrote this comment of remove duplicates and it wrote out some completion for us. So I'll hit tab and it's going to write all this code for us. But I don't want that and I don't want this. But I do want to drop the duplicates and I want to see how that changes the data frame. So, let's create a new environment again. We should have everything now. And then we'll do Python data prep. Okay. So, it didn't drop any duplicates. One of the problems is that the headers aren't very helpful. So, let's see if we can have an LLM help us with that specific task. This isn't going to be something it can probably figure out in tab. So, I'm going to open up a new chat and I'm going to say something like, ""Can you write me some code to change the column names in Upwork CSV extract?"" Okay, please infer the header names from the content. We've narrowed the task, so hopefully it doesn't go off the rails again. Okay, so this is struggling. So, let's not do the agent. Let's just do ask cuz this should be a pretty straightforward thing. Maybe the the CSV file is too big and it's struggling to see it. Let's do this. So, I'm just going to paste in the first 50 lines. Let's see if that helps. Okay, so it's figured out a mapping. Date posted, job title, job URL, category, payment status. That's probably not right. Rating text, rating value, rating details. Okay, so some of these are good. This is also like a hard thing for the model. So, let's try something else. Let's try to make a notebook and then we'll go to this put Jupiter lab I pi kernel and then we'll install. So I spelled that wrong and we'll try again. Seems like it worked. So we can open this markdown file in here and just say data prep for upwork job extract. So we could do this and then write the code here. But personally I don't like the interface for notebooks in cursor or VS code. I prefer it in Jupyter Lab. So, we can just launch Jupyter Lab here. And we can see that first line's there. Usually, I do all my data prep in Jupyter Lab anyway because you need that quick feedback. Since cursor wasn't able to do this for us, we'll just do it the oldfashioned way. Now, we're in a better position to see if those headers were any helpful. Let me just steal this code here and I'll paste it over here. Let's just do the top one. This saying it's the date posted. That's fine. Job title. That's good. Job URL. Job category. We might need to see some other examples. So, let me see what like 400 looks like. Okay. So, I'm going to call this search because this is what I used to search for these jobs. Payment status. Okay. Payment verified rating. So, I'll call this client rating. Oh, I see. So, these are all like different kinds of client ratings. So, I'm going to say client rating text, client rating value, client rating, I guess details. That was fine. Client total spent. That's good. Well, it's getting cut off now. Okay. So, client status. I got to do this. That might be noise. So, let's go look at another record. Spent spent. Okay. So, what does 100 say? Spent. Spent. Spent. Let's try tail. Okay. So, that isn't really a helpful thing. So we'll call it spent client location job rate. So that's hourly one. Let's see if we can find any other examples. Hourly hourly. Hourly hourly. Fixed price. Fixed price. So we'll just put hourly or fixed here. Job expertise level. Interesting. It looks like estimated time got its own thing. Oh, it's either estimated time or estimated budget. Time or budget. Job hours per week. Let's do duration or budget. Job description, skill one, skill two, skills three, skill four, skill five, proposal label. Let's go back here cuz I had already manually did one. Number of proposals is proposal label. So, let's call that numbum proposals. I feel like that's good enough. So, let's rename everything. So, we redid that. And let's make sure everything was changed. Yeah, that looks good. Now, it seems like there's a lot of things that we don't need. So, the thing we care about are going to be the job description and the skills for those jobs and then how much those jobs are paying. Yeah, maybe we can just get it from the job title. We can keep URL. We'll keep search term. Payment status isn't super important. Client rating, all this stuff isn't important for now. Maybe a future thing. Spent is not helpful cuz it was just saying spent over and over again. Hourly versus fixed is good. Job expertise, I guess that's not super important either. Estimated budget or time. So, we're going to have to write some logic here because this field, it's either going to be the estimated time or budget. And if it's estimated time, it usually has a hourly rate associated with it. And then there's also budget, job description, and then the different skills. We don't need this. I think that should be good. Let's just keep these columns. And then we'll do this. DF new new head. This is much easier to parse. Now we can do some manipulations. We'll change the column names. And then we'll drop a bunch of columns. And then I guess another thing we can do is we need to drop duplicates. So it didn't seem like anything got dropped. Let's try this. Try to do the URL. Aha. Okay. So there were duplicates. Oh, okay. Got it. So there were no duplicates based on the row because the search term could have been different. But if we do the URLs, we see that there are duplicates. So let's drop duplicates. Duplicate jobs. I guess we'll just call this DF. DF. Sure. Okay. Relax. Relax cursor. Okay, that's good. Let's say save first 10 rows. Let's try to run this. So I'm going to open a new terminal. I need to activate the environment. And then I'll run data prep. Hopefully, this is easier to work with for the LLM because we're going to need to do some things here. Let me just explain the problem to the LM and maybe we can brainstorm and let me not use the agent cuz it loves to go off the rails. So, this is one of the benefits of using LLMs is that you're forced to explain what you're trying to do clearly and in English. So, maybe this is the main value here. So, I have two kinds of jobs. They're hourly based or fixed price based. and I want to capture how much people are willing to pay for that job and represent it in the table. So how can I do that in a single table? So this is going to be a helpful like brainstorming situation. I want to use this data to represent how much people are willing to pay for each job. However, jobs can be either fixed price or hourly. How can I organize these into a table? Seems like it's coming up with something. So, it correctly identified that these are the three columns that matter. You want a table where each row is a job. Single column represents the comparable pay. Interesting. For the hourly jobs, extract that. Min, max, estimated hours per week. So, let's add data prep. So let's see how it handles this. Close this out. Close that out. Go back here. So it's making a bunch of helper functions. Let me actually make a new file and I'll call it functions cuz I don't want all these here. It's importing numpy. Numpy and reg x. Don't do that. And no need for this. Probably will need pandas. And then numpy should be installed. So I'll do pip list. Numpy is installed cuz pandas uses numpy. So we don't have to have that in the requirements file. reg x is a standard Python library. I typically like it to have doc strings new chat and then highlight all this stuff. And we can do command L and it'll automatically add these functions to the chat window which is really nice. And I'll just say like can you add doc strings? So that's doing that. We don't need to do this. So we'll go back here. We can see that it's suggesting all these changes. So we can go through one by one and like accept them one at a time. But I'll just go ahead and accept this whole file. I'm sure it's fine. Back to this chat here. And then we'll copy this code. But it doesn't have hourly rate max hourly rate min job title, job URL, pay type, estimated total pay. Let's add data. And you add these columns to the data frame. Okay. So let's see what this does. Let me just do one change and I'll say if you are writing functions write them in functions.py and we try again. Looks like all the other functions were already there. So it just added this one function and then it's updating the main script. Remove unneeded columns. That's pretty good. That's a good guess. Hourly or fixed duration or budget cuz now we have this. That looks fine. And then let's say reorder columns. That looks pretty good. Job top new search term. That looks pretty good. Okay, now let's save the whole thing. Save CSV to file and then we'll put Upwork cleaned CSV. So let's run this whole thing. What is this? Detected unusual line terminators. Remove them. I don't know. Let's see what that does. Job title, job URL, search term, pay type, hourly min. Here's it with numbers. This looks a lot better. Search term, paid type, hourly, min, hourly max, estimated hours per week, estimated total pay, job description, proposals, skill five is not helpful. It's good that we looked at the data. So, we'll just go ahead and remove that. What else? These like later skills don't seem super helpful, but I guess it's fine. We'll keep them around. I guess if it doesn't have an estimate, then we should drop it. So, let's do one last thing. Drop rows is nan or empty. We're going to have this boolean here and it's going to be is not nan. And I don't know if we want and we want the or. What is or? We can do shift command L and it'll create a new chat. I don't remember what the amperand does. So, what does amperand mean here? Yeah, it's a and. How do we do or yeah vertical bar? So this is why you should read the code. It's not always going to work as expected. So let's run this again. I changed the name. One data prep. So let's see if this updated. Didn't seem to change it. Yeah, that didn't work. So this is where we can go to the notebook. So it's always handy to have like a notebook on standby. Notebooks are really helpful for exploring data because you always end up doing something like this. Estimated total pay nan. Okay. So let's do is nan and we'll sum it 140. Okay. So maybe let's just drop this. They should all run some kind of calculation. So now none are nan. So let's just try to run this whole thing again. And then I'll just load in the data here. Upwork clean. Kill that. DF head. Upwork cleaned. Looks like they all have the estimate total pay now. Awesome. Why is search term nan? I feel like we got some more work to do cuz that shouldn't be nan. Impute missing values of search term using previous rows search term. FF fill. What is this? Let's go to the panda stock. Phil na. Wait. Deprecated. What? What does that mean? Now it's its own method. Okay, well that's good that we went to the doc. Fill name in by propagating the last observed value to the next. Okay, so let's just do that. Claude's letting us down. Impute missing value search term previous search term. Let's try to run this again. Then we'll go to our handy notebook. Let's see what that looks like. Yeah, that looks good. Hourly rate. Now we can start interrogating our data. Let's do a sample. Do the head 100. Perfect. And then we'll run this again. Can you count the number of rows that are hourly but have missing hourly rates in Upwork clean this one? You can run it. Oh, that's annoying. It doesn't have an environment set up. Let's just go to the handy dandy notebook and run this number total number of hourly jobs zero. It's cuz this needs to be capitalized. That's good enough for now. We might go further and realize there's some problems with the data and we have to backtrack and update our data pipeline. But this is like most of the work when it comes to data analytics and data science, creating the data pipeline and making sure we're getting all the information that we want. And of course, it's taken us a while to do this. Right now, we've been doing this for like an hour. Maybe it would have taken me 2 hours or more to do this from scratch in a notebook or something like that. So, I think we're in a good spot. I'll delete this. I'll keep our notebook around. No need for that. And let's just run this one more time. And I don't need this. And I'll leave that. Analyze hourly rates. We don't need that. That wasn't helpful. Thanks for not being helpful, agent. That's fine. Functions data prep. I think we're in a good spot. Now, let's create a file to create the actual dashboard. Create dashboard. And maybe it'll be helpful if we constrain the text stack. So, I'm going to have it use streamllet. Let me say something like, can you use streamllet to create a dashboard summarizing the job data in Upwork cleaned? It's too big. So, we'll just have this truncated version and we can just swap out the data as needed. What the Maybe it's trying to anticipate something. I don't know what's going on. Can you stream create dashboard summarizing the job data in this file? Let's give it some things. Here are some things to include. Include a bar plot of the top five most popular skills. Highest paying jobs. So, it's going to have to figure out how to do that. And then let's do highest paying skills. So, it's running some stuff. Okay. So, let's just accept this file. And then why are you trying to get rid of my Jupiter lab, bro? So I'll reject do that. We'll just bring in plotly and streamlet. Did all that and then we'll copy this. So let's do a pip install requirements.txt. Clear that out. We'll run this streamllet dashboard. What the Okay, here we go. Hopefully this works. Whoa. So let's see. Most popular skills. AI is a skill. Interesting. Agent development. Shouldn't be super surprised about that. Highest paying skills. How did it infer that? Average hourly rate. That's cool. I like this key statistics here. 100 jobs, hourly rate. Maybe we could do this. So, let's make some changes to this. Delete all this. Here's something to include. Key stats cuz I liked how it did that. Let's do number of jobs. Key stats at the top. Number of jobs. Average hourly rate. average fixed price average estimated total pay with dropdown to select search term. Let's start simple and just have the following component and then I will do the ask version of this continue without reverting. Now we gave it a lot more guidance. I liked that we let it try on its own first because it gave me some ideas on what the dashboard could look like. I'm just apply this over. And now I'm taking a step back and going to build it out like one piece at a time. So let's kill this and then we'll try to run the dashboard again. Okay, nice. This looks good. Upwork job dashboard number job hourly rate fixed estimated total pay. Okay, so this is not right. Let's go in the code and try to figure out where search term search term is not normalized. Let's go to our handy notebook to understand why. So df search term interesting. So let's take a step back. Standardize search term values. Yes, we'll do that. Can you further standardize the search term values to be any of the following AI, ML, statistics, data engineering? Here are the unique values. I don't think that bit is necessary. Let's see. Can you wrap this in a function and write it in functions pi? Why did you add this? You monster. Oh, wait. Did I have that? Oh, I did have that. Oh, it's just doing the whole thing. That's fine. What is this? Wait, replace the block. Did that apply? Let's see if that. Wow. Okay, nice. And then I'll just do that. And that should work. I'll kill this. Clear it out. And then we'll run this pipeline again. So that ran. Let me just keep this head in there. It's pretty helpful. Now let's run this dashboard again. Okay. So, oh gosh. Oh, that makes sense. This is just the first 100 rows. Aha. All right. So, now we have these three. We have 735 jobs after everything. So, here's the average hourly rate for all of them. Fixed price. Average total pay. Okay. So, that's cool. Maybe one thing we can do is go to a previous chat. Boom. Okay, looks great. Can you update this so that when a search term is selected, the key stats are compared to the key stats for all the jobs. See if we can do this. So, this is now the fun part. Before the building of the data pipeline is a lot of work. It's tedious and AI isn't super helpful because we can't fit the full data set in the context window of the LLM. So, it makes it less helpful. But once we got that settled, playing around with the UI is much easier, much more fun. Okay, so do this. Then I'll do this restart. Oh, that's slick. That is slick. Okay, ML data engineering. Oh, come on. And I guess statistics is the worst. But I guess there's very small number of jobs in statistics. Mel AI. That looks great. Let's do something else below this. We need to have like some kind of bar chart. Yeah. So let's do the estimated total pay distribution of each. Perfect. Now below this, can you add a distribution of the estimated total pay which is also filtered by the search term selected? All right, it's writing code. It's doing stuff. Apply. Okay. Oh no, I need to import my plot lib. Matt plot lib. Why is plotly being imported? Yeah, maybe we'll we'll have to get rid of plotly. I don't know why it's being installed. So, we're going to install that plot l and then we'll try again. A doesn't seem like the plot changed much. Oh, yeah. It's changing. I'd love to hover over and see the key metrics for each of those cohorts. Is it possible to make the bar chart interactive so that when you hover over a bar, it shows key stats like total number of jobs in group average. So we'll use plotly instead of mplot lib interactive histogram. What code? I don't see any code. Got a little wonky there for a second, but let's see if this works. Kill this. Run dashboard again. Okay, range. Interesting. Looks nice. AI. So, high hourly rate. So, I guess you want to do the AI jobs. Perfect. Let's do all. And then below, I want to do a ranking of the top skills. Below that histogram, I want a horizontal bar chart showing the top skills for the jobs. Additionally, I want this to get filtered based on the selected search term. Finally, I'd like an option to sort by count. i.e. most popular and highest paying based on the estimated total pay. So, this will probably be the last thing. I think this will be like a pretty cool thing to see. Accept file. Let's see if this works. Oh, yeah. A sort option not defined. Let's see. I got an error. Okay. Yeah. So, don't put it there. We're supposed to put it here. So, correct. Yeah, I think that's good. So, we are now vibe coding. We are officially vibe coding. Well, I guess we've been vibe coding for a while. But now it's absolutely official because I have no idea what's going on here. Cuz the thing about visualizations is that it just takes a lot of coding and trial and error to get it looking good, which makes it one of the best things to vibe code cuz no one's got time for that. Here we go. Yeah, look at that. But that looks really good. Ice ping. Whoa. reinforcement learning plus 19. What is + 19? Where's he getting these skills from? Looks like we got to fix the data. We're going to take this and we're going to go to our handy notebook to see what the hell is going on here. So, we'll replace this with this. This is just empty series. Yeah. Okay. So, now we're going to go through this process of actually adding skills to that. So, we're going to gather all the skills into a single series. you monster. Replace all. Run it. So now let's see what skills looks like. Plus three. What is What is plus three? What does that mean? Well, first let's do this. I think it's just adding each unique skill into its own thing. Yeah, there shitload of skills. Okay, so skills. And let's do this now. We'll go to our handy AI and say, ""Can you write me some code to standardize these skills and remove ones that are not skills at all?"" EG plus one. It's just noise from the scraping. So, it's thinking it through first, which is great. How can I integrate this code into and functions? code looked good, but it was all over the place. So, we'll apply this to the functions except file and then data prep. Apply you to data prep. Looks good. Do this. Clear that out. Oh, clear. Python one data prep. Clear. And then this should be much better now. So, we're going to streaml it. Run. Oh, nice. Okay, this is helpful. Average estimated total pay. image processing, reinforcement learning, media analytics, sign language, organic traffic growth, competitive intelligence, automate speech recognition, business consulting, AI generated code, technical SEO, psychology, healthcare, mental health. Okay, let's add a couple more things. I know I need to stop because I'm just going to like keep adding things, but this is like the fun part. Since I don't have to go through the arduous work of coding all this, I can just let my imagination run and have the AI make all these fun little features. Looks great. Can you update the tool tips on the skill horizontal bar plot to include average estimated total pay and total number of jobs listing the skill? Okay, interesting. That's cool. For the full plotting now looks like this. Okay, we don't need that. Camera's about to die, so I gotta really finish this. So, tool tip. Yeah, look at that. Total number of jobs, average pay. Nice. Highest paying image processing. See, that's the thing. This is why it's important to know this because this is just one job. One job is just paying really well. And that's the thing about fat tails. Organic traffic growth. Yeah, these are all oneoffs. That's why it's helpful to have both of these views. This is what people are asking for. or they will ask for artificial intelligence and data science, but also Python, data entry, machine learning, and then you can see like the typical pay. So, easy way to increase your revenue if you're a data scientist, don't say data science, say AI. Look at that $2,000 pay increase right there. Machine learning, Excel, data analysis, AI agent development. Okay, that's on the rise. Super high leverage. Look at that. $24,000 average estimated pay. So, that's another thing you can do. So you don't want it to just be popular. You want it to be in demand. So machine learning, don't even say AI, say machine learning because the people who know are willing to pay for it. Data analytics, that's good. AI agent development. Just put agent at the end of AI and you'll make significantly more money. JavaScript, data scraping, automation. Beautiful. Let's test this. So let's do AI. And then these are the jobs under AI. And we got Yeah, this would have taken me like a weekend if I was not using AI to help me code it. And I probably wouldn't be able to make something look this snazzy. This took me 2 hours to code completely from scratch. Probably would have taken me 2 days to code if I wasn't using LLMs to help me along the way. A lot of the work was just getting the data together and figuring out what I actually wanted to code. I feel like part of the problem is that it's hard to fully know what you want when it comes to like data projects like this from the outside because you need to look at the data, interact with the data, see what's available before you can start planning out what you want. So, I do find that you want to do a little bit of planning, but you can't do too much planning cuz you just have to dive in and look at the data, all that kind of stuff. Just to recap, there are like three major buckets when it comes to coding with AI assistants. One is just have the AI do the entire project for you. That's what we tried initially for this project and we saw that it threw out an error. The code was pretty complicated and it didn't even feel like it was something that was salvageable kind of align with the end product that I had in mind that looks something like this. The other extreme is you don't use AI assistance at all. You just code it from scratch. for this project. Since it was kind of complicated and it was kind of unclear what I wanted, it was helpful that I started just writing the code from scratch and then bringing in the LLM slowly to code up specific pieces of it. And then that kind of led to this middle ground where a lot of the code was already written and we could just use this AI in chat mode to write code and apply it to the existing codebase. So, I hope this walkthrough was helpful. It just showed you the nitty-gritty details and the reality of what it actually looks like for me to code a project from scratch. I'll share this code on GitHub. And if you have any questions, please feel free to let me know in the comment section below. And as always, thank you so much for your time and thanks for watching."
N3vHJcHBS-w,2025-04-27T15:00:28.000000,Model Context Protocol (MCP) Explained in 20 Minutes,"Hey everyone, I'm Shaw. This is the fifth video in a larger series on AI agents. Here I'll explain the model context protocol or MCP for short. I'll start by discussing its key concepts and then show you how to build a custom MCP server with Python. At this point, you've probably heard the term MCP before, but maybe you aren't super clear on what it actually means and why you should care about it. But all MCP is is a standard way to connect tools and context to AI applications. So this is a standard developed by anthropic. And the analogy they use to describe it is that MCP is like the USBC port of AI applications. Just like how USBC gives us a universal way to connect any device to say a computer, MCP gives us a universal way to connect external tools and contacts to AI applications. There are many benefits to having this open standard for connecting different AI components together. But just to list a few concrete examples, one, it allows users to port in custom integrations into their AI applications. For example, I might want to connect Slack and Google Drive to Claude so it can get context from these external apps or even write messages to a particular Slack channel. Another benefit of MCP is that it enables you to have these portable tool sets. So let's say I've locked in my AI coding assistant development stack. And so this includes things like boilerplate code, different prompt templates that are really helpful for debugging or maybe some custom tools of creating automated documentation or something like that. MCP allows you to create this tool set just one time and then dynamically port it to whatever IDE you like. So let's say you were on VS Code for a while, but then Windinsurf came out and you wanted to give that a try. You don't have to start over from scratch. you can just take your tool set and go from 0ero to one very quickly. The final example I'll talk about is let's say you ship your own custom AI app. You can benefit from the rich ecosystem of open-source MCP servers to increase the value of your application to your end user. All you have to do as an app developer is to create an MCP client and then immediately it allows your application to connect with any number of MCP servers developed by third parties. So this might all sound well and good but you might be wondering how does MCP work under the hood. So the architecture of MCP is actually very intuitive. It follows this client server architecture which is widely used in the web space. But the basic idea is that you have two modules. you have a client and you have a server where a client sends requests to a server and then the server generates responses to these requests. This is just like if you were to go to a coffee shop. You or anyone going into a coffee shop are kind of like the client in a client server architecture. And the barista is like the server. So the client will send a request to the server saying something like, ""Can I get a pumpkin spice latte?"" Okay. And then the server will respond, ""Yeah, you got it."" So, mapping this analogy to MCP, we'll have a MCP client, which is kind of like the customer at the coffee shop, and an MCP server, which is kind of like the barista at the coffee shop. And the client will send requests to the server like, ""What tools do you have available?"" or ""What resources do you have available? What prompt templates do you have available?"" And the MCP server will respond based on its capability. So if it just has one tool, write email draft, it'll respond with that. This is just like a highle overview of how MCP works. Let's now dive into each of these two key components. First, we'll start with the MCP client. These components are built directly into AI applications and they're the ones that send requests to MCP servers. For example, suppose we're working with the Claude Desktop application. Claude Desktop actually has a MCP client built into it. So when you're using the application, you'll see the various tools and integrations that are available to Claude in the user interface. So these are facilitated by the MCP client, but the client is getting access to these tools and integrations from external MCP servers. This isn't just limited to one MCP server. You can have any number of MCP servers connected to a single client. Some of the things that the client is responsible for is that it'll discover the server capabilities. Kind of like what we saw in the previous slide, the MCP client will ask the server, ""What tools do you have available?"" Another key responsibility is receiving data from servers. The MCP server may make available to the client a database or a file system. So it's responsible for grabbing that data and passing it to the LLM application. Another critical responsibility is managing the tool execution because of course LLMs are simply text generators. They're not capable of executing tools themselves. This needs to be done by another piece of software. So the MCP client can handle that. And an important note here is that, you know, if you're using an AI application, you typically won't need to implement your own MCP client because these libraries and applications will already have an implementation of it. However, the thing that you might need to develop more often is the MCP server. So diving into that as we saw in the previous slide the server is a independent system from the AI application and it's responsible for listening for requests from MCP clients and responding appropriately. Returning to our claw desktop example, we can have our independent MCP server living over here and it's talking to the client. And some of the key primitives in the MCP server that gets passed to a client are prompt templates. Often you have some boilerplate prompts. For example, if you have a prompt that works really well for you for rewriting your resume and you just want to quickly swap in and out a version of your resume and the job that you're applying to, you can frame this as a prompt template and easily swap out those two inputs. Another key service are resources. These can be like static data files. They can be entire file systems or they can even be a traditional database. Resources are very similar to get requests. Basically, you'll send requests to a external service and the service will send you back some specific data. And the key thing about resources is that they shouldn't be computationally expensive. It should just basically be a lookup of some data that gets returned to the MCP client. And then finally we have tools. And these handle basically everything else. Tools are arbitrary functions that allow the MCP client to perform actions. So this can be like any Python function, accessing an API, or even doing things like image processing. And the last thing I'll say about MCP servers is that there are two default ways we can transport information between the client and the server. The first is the standard IO. So this is great for local development. Basically, you can have a local application like claw desktop and then you can spin up a server locally and the server and the client can talk to each other via the standard IO. However, if you don't want to have everything running locally, maybe you want your application on the cloud, then you can spin up the MCP server via HTTP with servercent events, so SSE for short. And so this allows you to basically have the client and the server talk to each other over the internet. The discussion so far has been pretty high level and just talking about conceptual things. Let's make this concrete by implementing a custom MCP server with Python. Here I'm going to use Anthropics official Python SDK and the link to that is available in the description below. The server I'm going to build ties directly to a project I built in a previous video of a series where I made an artificial virtual assistant called Ava, who basically was capable of taking requests from me and performing various administrative tasks, whether that's drafting an email, reviewing contact information, and other things like we saw there. Ava was able to do these things because it had access to a set of tools. It had a tool that allowed it to write drafts in my Gmail mailbox. It had access to a directory of common contacts and it had access to some common email templates that I would write. So the power of MCP is that if I take this tool set and wrap it in an MCP server, then I'm not just limited to that single implementation of AVA. I can just as easily take these tools and port them to claw desktop or to cursor or to any application that has an MCP client built into it. So let's see how we can implement this with Python. The first step is to install UV. And if you're not familiar, UV is an extremely fast Python package and project manager written in Rust. However, the reason we want to use UV isn't because it's super fast. It's because it gives us a way to manage the entire Python environment necessary for running some code. We'll see why this is helpful a little later, but the short version of it is that since UV kind of handles all the environment stuff, we can spin up the MCP server with a single line command. So, if you don't have UV installed already, this is from their documentation. So you can install it on Mac or Linux using curl or you can install it on Windows using PowerShell. Next, we're going to create the server. And so this is super easy using Anthropic Python SDK. We can just import this fast MCP class. And then we can create a new MCP server called AVA with this line of code. Before we saw the three key primitives in an MCP server are prompts, resources, and tools. So let's add each of these to our server. But we can add a prompt in the following way. We'll basically create a function that takes in text inputs and returns a string output. And all we do is add this mcp.prompt decorator to the function to make it a prompt. Here I'm creating the global instructions for Ava. So we can see it says Ava artificial virtual assistant. You are Ava a virtual assistant to insert username. Insert user title. you help them with administrative tasks and then there are some additional preferences. Basically, this template will take in the user's name and title and dynamically plug it into this prompt so it can be passed to any LLM. Next, we're going to add resources to the server. We'll do this in a very similar way we created the prompt, but instead of doing mcp.prompt, we'll use this mcp.resource decorator and then we'll create this URI or unique resource identifier. So to implement this, we'll just create another Python function that takes no inputs and outputs a string. We have this dock string of example of a three-way intro email. And then we'll just open this file. So I have this folder called email examples and there's a file in it called three-way intro.md, which is the email template. We'll just open that file and then return its contents. Now, anytime the LLM tries to access this resource, it'll get the contents of this file. And then in a similar way, we can give it access to another email template called call followup. So this is the same exact code just swapping out the URI and the file path that we're accessing. And then doing the same thing for this directory all URI. So this will open up a CSV file which is filled with contact information. Finally, we'll add tools. So here I'll just add one tool and it will be one that'll allow any AI application to write email drafts directly to my Gmail account. So I'll start by importing a bunch of handy libraries. And I also have this custom function called get Gmail service. This will set up the connection between the Python program and Gmail. And I won't spend too much time on this code because I talked about it in a previous video and it's not central to MCP servers. to kind of show you the code. Again, we'll create a tool similarly to how we created the prompt and the resources. We'll just use mcp.tool. And then we have this function. And you can see that we have this pretty detailed dock string. So dock strings are important in all of these different definitions because this is what the large language model has access to and is what the large language model is reading when deciding what tool, resource, or prompt to use. So having a clear dock string is very helpful. And then here's what the code looks like. So first it's going to try to connect to the Gmail service using this function we imported. Then it's going to write the body of the email. It's going to initialize it first. And then body was an input to this function. And then we'll craft who the email is going to, who it's from, and what the subject line is. So both of these were inputs to the function. I'm importing user email from aenv file. Then this is encoding the message so we can send it over the internet to Gmail. And then we're going to create the draft using this code here. So this code was copy pasted from the previous example. So that's why we have a print statement here. This won't be necessary when using an MCP server cuz there's no user interface from the MCP client. And then if an error occurs, we'll just say that an error occurred and then we won't return any draft. Now that we've created the prompts, resources, and tools, we'll need to set up the transport mechanism. So here, I'll just do the standard IO. And then we can add this at the end of our Python script so that when we run it, it'll spin up the MCP server. So we're using this.run method. And then we're specifying transport as standard IO. Before trying to connect this to any application, we can just test it to see if it's working. So if you've clone the repo, that'll include all of the dependencies and requirements for this UV project. Then we can just simply do UV run and then we'll run this MCPD dev command and then we'll run MCP server example.py. So this is the name of the Python script in the GitHub repo which contains all the code that we've seen this far. MCP is available in the UV environment. And dev allows us to run this server in development mode. And in order for MCP to work, you'll need to have Node installed. With Mac, you can just install it via Homebrew. I'm not sure how to do it in Windows, but it should be pretty straightforward. Once we do that, this guey will run in your default browser. So it'll have all the resources available in the server, all the prompts available, and all the tools. So we can see that everything we just created is indeed available. And we can click around and make sure everything looks good. We'll see this history of the requests that we're sending to the server. We're just testing and checking if the server actually works and everything we created is available. Once we've confirmed that works, we can integrate it into an AI application. So here, since I'm running it locally, I'll use Claude Desktop. To do that, you need to first have Claude Desktop installed. Next, you can go to the developer settings. On Mac, you just click on Claude, go to settings, click developer. This is what the settings screen looks like. And then you'll click this edit config button. And then you'll paste this JSON schema into the file that appears. And then finally, you'll quit out of cloud and relaunch it. And then you should see the integrations appear. Okay, so let's just walk through these steps real quick to see what it looks like. So here's my Claude desktop opened up. We can go up here to Claude and hit settings and then we see developer settings here. So I'll click on that and then I'll click on this edit config button. This will open up this JSON file. This is like a default JSON file. And simply we paste in all our MCP servers into this JSON file. I'll paste in the code from earlier. And for me, just putting UV by itself didn't work. So I had to put the full file path to my UV installation. So I got that by typing where UV into the terminal. So we're running this UV command. And then these are the input arguments. I'm going to specify the directory where the code for my MCP server is. So that's in this folder here. And then we'll just do run MCP server example.py. The reason UV is helpful is because UV run will basically handle all the environment variables and dependencies that are needed to run this Python script. This is necessary because claude every time you launch it, it's going to spin up an instance of this server so that its client can talk to it for all the requests. So, we'll just save this JSON file and then I'll actually quit out of Claude and then I'll launch it again. And then once we relaunch it, we see that these icons have appeared. So we see MCB tools available and we have this write email draft from server AVA. And then here is the doc string. This is what is going to get passed to the LLM. Additionally, we have these integrations. We have this prompt from AA. We have this URI for the directory. And then we have these email examples from the server. I'm going to try to type this prompt. I'll put my name. I'll say I'm an AI educator. Add prompt. And then we can pass this to Claude. Okay, sweet. Write a followup email to Sarah Chen saying her code looks great. We'll see if Claude can figure out that it needs to use the directory URI from this. So, it figured out it needs to use the ride email draft tool. So, we'll say allow for this draft. And then it created that. Then it said, ""I've created a draft email to Sarah Chen praising her code. The draft is now saved in your Gmail account."" So it seemed like it didn't use the email template, but I guess for Claude, we need to explicitly tell it what resources and prompt templates to use. So let's try again. Let me just say, can you try again? And then we'll add integration call followup clause thinking. So it's again using the right email draft. So now it is using call followup in the subject line which is what it should do based on the email template but it's still adding your code looks great and all that kind of stuff. That's a good caveat. The tools are accessed automatically but you'll need to specify the prompts and the resources that you want the model to use. If we look at my Gmail, we see that these drafts were indeed created. Here's the feedback on your code. And then here's the second try. So, this one's a lot better because this is actually following the email template. So, I have like a first line and then like key bullet points from the call and then a closing thing. So, I hope this video was helpful to you. If you have any questions, please let me know in the comment section below. And as always, thank you so much for your time and thanks for watching."
6VcXukJuGA0,2025-04-21T12:00:15.000000,LLM in a Loop: Automate feedback with evals,"Hey everyone, I'm Shaw. This is the fourth video in a larger series on AI agents. Here I'm going to talk about running LLMs in a loop. I'll discuss why we might want to do this and then walk through a concrete example with Python code. So if you've ever used an LLM system like ChatBT or Claude, then you've probably experienced the power of trial and error. For example, if I wanted ChatGBT to rewrite my resume for me, I might simply just get my resume, write this prompt, can you rewrite my resume? And then pass it into Chat GBT. But usually what happens is that the first rewrite that Chat GBT does isn't always so good. It might be a little bit better, but it's probably got something wrong with it. For example, like maybe it doesn't use very strong action verbs for each of the bullet points under my work experience. So, what I might do is just respond back to Chad GBT and say, ""Can you use stronger action verbs for each bullet point and then it'll return a slightly better version of the resume."" Still, this isn't like 100% there. Maybe it's a little bit too long. So, I'll say something like, ""Please compress to one page. Limit bullet points to two to three max."" So, I pass this back to Chad GBT. And then it comes back with an even better version of the resume. And then this might be pretty close to the final output I'm looking for, but maybe I just want it in markdown. So I'll just say, ""Hey, can you format this in markdown?"" Then we'll have the final version of the resume that can help get me hired. So this simple example is representative of a larger pattern that comes up when working with LLMs. Namely, a lot of times it doesn't get it perfectly right on the first try. It takes some trial and error and refinement to get the LLM's response to be exactly what you're looking for. This basic idea of improving an LLM's response through this iterative back and forth process is the key motivation for running an LLM in a loop. But rather than having a human in the loop who is manually interacting with the model, the idea of LLM in a loop, as I'll discuss it here, is automatically providing the feedback to an LLM to improve its outputs. And so there are countless ways that this can look in practice, but just a few examples are maybe you have a coding agent that is tasked with completing GitHub issues. So maybe the way this will look is we'll have the agent write some code. Then we'll see if the code produced by the agent passes unit tests and actually runs. And if the code doesn't run or it fails some of the unit tests, we can gather that feedback and feed it back to the model and have it try again. And then we can just continue doing this process until the code runs successfully and all the unit tests pass. Or maybe we have an argument writer. So, we have an LLM that is making some case in a debate and then we have a different LLM which is taking the opposite position that responds to the original one and then they just go back and forth until one of them wins the argument. So, this could be a way of generating compelling arguments. Another version of this is maybe you want a LLM to come up with video ideas for you, but a lot of times the video ideas that directly come from an LLM aren't so good. So maybe you'll have an LLM judge that is aligned on user preferences. So let's say this idea generator comes up with five different ideas. And then this judge LLM picks out the best three ideas and passes that back to the generator who generates a few more ideas and that gets passed to the judge again who again picks the top three. And then this gets passed back to the idea generator. And this loop continues until you have a refined list of ideas. Or maybe you have a LLM that can write X posts or Twitter posts for you. So basically, you'll have it write a post and then you'll pass it feedback of different engagement metrics, maybe the like to impression ratio or the comment to impression ratio, some real world signal that gives the LLM feedback on its responses. And so across all these cases, just like how giving Chad GPT feedback on rewriting my resume led to improved responses, giving an LLM feedback across all these different use cases will lead to better and better outputs. This is important for building AI agents because this allows them to one do more and more sophisticated tasks without a human in the loop and two it allows the agent to get feedback from its environment which is a central feature of agency. A critical ingredient of getting an LLM in a loop to work well are the evaluations that we bake into the system. Here I'm going to focus on three types of evals we can use when generating this automated feedback in an LLM loop. So the first type are rulebased ones, the second are LLM based ones and the third are real world data. A critical point in picking your evaluations for an LLM loop is ensuring that the eval are highly correlated with your desired outcome. While this is still a bit more art than science, in the upcoming slides, I'll give a handful of examples to hopefully give you a better idea of what might be the best choice for your specific use case. So starting with type one, rule-based evals are tests and metrics that can be built using simple code. This includes binary tests. For example, if the code generated by your coding agent raised an error, that can be a pass fail metric. Or maybe you can check the formatting of the LLM's output using some kind of linting tool to ensure that it follows a desired format. You can check if a particular string is present in the model's output. You can check if the correct answer exists in the model's output and countless others. Another flavor of rule-based evals are continuous or at least non-binary metrics. So this can be things like the length of the output, the faithfulness. So this could be for like a summarization task. You can see how faithful the model's output is to the source documents. You can use traditional NLP metrics like the rogue score or blue score to compute the similarity between the output and a reference text. You can compute things like a readability score to see what reading level the model outputs. All these eval are going to be very use case specific, but these are just a handful to hopefully get the gears turning. Another important point here is that even if you're using continuous metrics like length, faithfulness, so basically anything that's not binary, you'll still need to translate it into a binary eval so you can define your stopping criteria for your LLM loop. And so this is just going to come down to defining a threshold for each of your continuous metrics. So, while the great thing about rule-based deval is that they're simple to implement and interpret, it is fundamentally limited in that there are just some things that can't be built with code. You can't assess the empathy of a response with code or accurately classify the sentiment of a response. For these more sophisticated tests that you might want to run, you can turn to type two, which are LLM based eval. These give us a more flexible way to assess a model's output using judge LLMs. So the great thing about using an LLM as a judge is that one, it can give you a pass fail metric or evaluation. So you can have like pass fail, does the model's output meet some criteria that you define. Another version of this is A or B. So rather than having this global assessment of is the output good in this global sense, you can also have it evaluate outputs in a relative sense. Basically taking an output and then the previous output and asking the LLM judge to assess whether it improved. The other thing we can get from an LLM judge is not just this past fail score, but also additional guidance on how the original LLM can improve its response. While this might be the first thing you might want to reach for because it gives you a simple way to automate the feedback loop, there is a big challenge with getting LLM judges to work well, which is aligning this judge LLM with the desired result you're looking for. And so in my experience, getting an LLM judge to work well can be a whole undertaking in of itself, but sometimes it might be the only viable way to do certain types of evaluations. The final type of eval I'll talk about are real world data. This consists of giving an LLM realorld feedback. Some examples of this are the click-through rate on a website. So let's say you're having an LLM write you copy for a landing page every single day. you can gather the click-through rate for that day and then you can take that click-through rate, pass it back to the LLM, have it try to rewrite the copy to improve the click-through rate, and continue this loop every single day. Another version of this is maybe you're having an LLM write you sales outreach script or maybe some ad copy, then you can use the number of people who actually bought your product or service as the feedback signal to the model. Finally, something like user satisfaction. So basically, if you have a customer support bot, you can have thumbs up, thumbs down as the signal that you pass back to the LLM. The power of using real world data in this feedback loop is that a lot of times these measures are ultimately the thing that we care about. When it comes to sales, we care about conversions. When it comes to customer support, we care about user satisfaction. These are the metrics that we're trying to optimize for or they're at least highly correlated to the metric we're trying to optimize for. But of course, the limitations here is that, you know, maybe the data integrations may not be available or maybe you have low data volume. Maybe you're not getting enough impressions on your landing page to pass CTR back to the model. Or maybe your sales cycle isn't on the order of days, but on the order of months. So it would take a really long time to know if a outreach script is actually effective. Now that we have a basic idea of what an LLM in a loop is, why we might want to do it, and different evaluations we can use in these feedback loops, let's look at a concrete example here. I'm going to build a system to refine my Upwork profile using this LLM in a loop idea. For those who don't know, Upwork is a freelancing website. So you can go on there as a freelancer, apply to jobs. You can also have a profile and clients can reach out to you about potential gigs. So it's just a marketplace for people to get freelance work. While I don't freelance anymore, I did this while I was in grad school. And looking back at my profile, it's actually pretty funny that anyone hired me at how bad the text is on my profile. So let's see if GBT can rewrite this profile for me to make it more compelling and converting. The process will be like this. I'll take my profile, give it to GPT40, and then I'll run a suite of different tests, which we'll talk about later. And then based on these test results, we'll give GBT feedback and ask it to rewrite the profile. And then we'll just continue in this loop until all the tests are passed or we hit a maximum number of iterations. We'll start with some imports. So really not much to import here. Basically just OpenAI's API. I do create a set of custom functions which we can talk about later. And then is just how I'm importing my OpenAI API key from av file. The first thing I'm going to do is write the instructions for GPD40. This was historically called the system message or the developer message. But now OpenAI has changed their API and now this is called instructions. To define the instructions, I have these two text files. One is called instructions.txt. The other is called example.txt. So I'll show these text files in a second here, but first I'll talk about how I generated them. to come up with instructions. I went on Upwork and then I searched for freelancers that have made over a million dollars freelancing AI or data science services. So I found five profiles that met this criteria and then I pasted those into chatbt and I asked it to find the different commonalities and best practices from those profiles and then synthesize a set of instructions for writing a profile based on those best practices. That's how I generated these instructions. And then additionally, I had ChadBT use the instructions and those five examples to write a synthetic example. These are both available on the GitHub repository linked down here. And just to show it, here's the repo. And then if we go to this context subdirectory and then we can click on instructions, we'll see this markdown file with the best practices. This was all generated by Chad GBT. So it's saying Upwork profile generator instructions. Follow these instructions to create high-converting Upwork profile that positions you as a premium expert in your field. Define your niche and ideal client. Draft your headline, hook with pain points, show your value, break down your services, add credibility and social proof, background and tools, pre-qualify, call to action, and then final tips. Then we can go look at the example. This is an example based on those top five freelancer profiles I found and the synthesized instructions that I just showed. Here we have unlock AI powered automation without the overhead of a full dev team. You know AI has potential but between jargon hype and tools that only half deliver, it's hard to know where to start. Then it has all these pain points. It's positioning the freelancer and then it's saying who I help, what I build, how I work, why clients work with me, so on and so forth. So this is just going to be a helpful example in guiding the model. If you want to use any of these, it's freely available on the GitHub. And then I'll just call out this read context is a custom function. It's really simple. It just reads a text file from that context directory. And so we imported instructions and we imported examples. Then I just used this prompt template to create the final instructions. So I'm overwriting the instructions variable here. Start with the instructions. I give an example. And then I add this important guideline. The next step is crafting the initial prompt to pass to GPT. For that, I'm going to read in this background.txt file, which is basically my current Upwork profile. Looking at that now, we can see that it doesn't follow any of those best practices we saw in the instructions. The worst thing I did is that it's all about me me. hates about Shaw and how qualified he is, how eccentric he is, how educated he is, how experienced he is, and the things that he does, but it has nothing to do with the customer and their problems, which is ultimately what matters. So, I'm importing the context. I'm also going to define the customer. This will also help GPG write the copy in a compelling way. So here I put founders and CXOs of small to mediumsiz businesses seeking guidance with AI use cases. And so I imported these two and then I will construct this prompt. Here I have Upwork profile rewrite task below a freelancer's background. Your task is to rewrite it based on high converting Upwork profile best practices. Their ideal customer avatar is so I'll insert the customer here and then I'll paste in my current profile here. This is going to be the initial input to GPT to kick off this whole rewriting loop. The next step is to define the eval. This is going to be the test suite that we're going to evaluate model outputs on to see if it's any good or not. Here I define four different evals. And again, I used chatbt to analyze those top five freelancer profiles to brainstorm different evaluations. And so these are the ones I came up with. They're all rule-based evals. They don't involve an LLM and they don't require any real world data. These are just things we can implement with code. The first is checking the word count. All the profiles were between 300 and 800 words. Another one is a heruristic for checking if the profile is client focused. So the way I do this is basically count the number of times you or your is mentioned in the profile. The threshold here is five or more. Another evaluation is to check for social proof. And so the heristic I use here is whether a dollar sign appears in the text. This would indicate you're talking about some past impact where you generated some dollar figure in value for a client or the profile includes a quotation from a past client. And then finally, I compute the readability of the profile and ensure that it's no greater than a 12th grade reading level. Originally, I tried eighth grade, but this was never passing. And I think part of the reason is that AI is a technical field, and they're just keywords that will never pass a middle school reading level. So, I just gave up and went for 12th grade. So, I've implemented all these functions in Python, and we can take a quick look at that. So, the first eval is the word count. It's really simple. It just counts the number of words, and then it will be true if it's within a lower bound and upper bound. And then it'll be false otherwise. This is checking if it's client focus. So basically it's counting the number of times you or yours mentioned and then it's checking if it's above or equal to the threshold. Then we have the social proof eval. So this doing a regx search to see if you have a dollar amount with some number and then million or thousand at the end of it or it's checking if there are quotations in the text. And looking at this I feel like this is redundant. It's doing the same thing. That's what you get for having AI write your code for you. And then passing if either of these conditions are true. Finally, doing the readability eval. First, I'm going to clean up the text to remove any emojis or markdown. And then I'll run this flesh concaid grade from this text stat library. This will just return the grade level. And so here I put nine. Oh, I guess I went with a nth grade reading level instead of 12th grade. Finally, I'll combine all the evals into this run all eval function. And this will check for everything and return the results in a dictionary. Now that we've defined all the evals, we can run this whole thing in a loop. The code is pretty straightforward for running these LLM loops. Here I'm just going to initialize this all past flag. Initialize it as false. I'll initialize the counter as zero and then I'll define max number of iterations. Basically, as long as all the tests are not passed and the counter has not exceeded the max number of iterations, we're just going to continue going in this loop. So, the first thing that happens is that the profile is rewritten. This takes in the instructions that we defined earlier and the prompt we defined earlier. And then client is just the connection to the OpenAI API. This will rewrite the profile and then I have this function to save the profile text as a TXT file. Once we have that, we can run all the evaluations. So this will return a dictionary with the test results. Then once we have the results, we can generate a new prompt. This is an entirely rule-based function that'll generate a new prompt based on the test results, which we can take a peek at. And then at the end, we'll update the all passed. If all the tests are passed, we'll return true. Otherwise, we'll return false. And then we'll update the counter. And then we'll just keep going in this loop. These are what the results look like after the first iteration. and it actually does a pretty good job. So, word count, client focus, and social proof. All these tests pass, but readability does not pass. So, it tries again. This time, the word count actually gets worse. Presumably, what happened is that it was trying to shorten it for readability, but then the profile got too short. It went under 300 words, so that's why this false. And then it kind of figured out, okay, that didn't work. Let's try something else. It got the word count back, but still readability failed. And then again, readability failed. But then on the very last iteration, right before it got killed, all four evaluations passed. If you're curious, you can check out all five iterations in the GitHub repo. There's this folder called profiles. So you can see how the profile text evolved over the iterations. But we can just look at the final result. So this is what it looked like before. This is the original text. And then this is the final text. And already this is easier to skim. So, a client who's rushing through and looking at a dozen different profiles is going to be able to easily skim this. And then maybe their eyes are going to see not sure how to use AI, overwhelmed with data, no clear insight, stuck with manual tasks that waste time. And they're going to say yes, that's exactly me. Then it says I can help. So that's perfect. And then what I offer, how I work. Honestly, that's pretty good. I think I might, even though I don't freelance anymore, I might just update the profiles to not embarrass myself with this profile. But I do need to make a few changes because this what clients say and proven success is completely made up. It's hallucinated. And the reason is that I didn't give it enough context to be able to write these things. But if I paste in some testimonials or different impact metrics into my original text, it'll probably be able to do it for me. And then the last thing I want to call out is this generate eval report. This is the function that rewrites the prompt and gives feedback to GPT on how to update the profile text. So this function is entirely rule-based. No LLM or machine learning is used under the hood. It's just preceded responses that pop up if certain tests pass or fail. So it's kind of hard to see what's going on here. So I had Chad GBT write it out. So basically it's this pre-anned thing where it has this preamble upwork profile evaluation and rewrite task. Here it'll paste the most recent version of the profiles text and then it'll summarize the evaluation results here. There's this little table for each of the evals and then finally it'll have rewrite instructions since just word count and social proof failed. It'll have these pre-anned instructions on how to improve those. And then I'll have this final text here. Although running an LLM in a loop gives us a new path to getting better and better responses and performance from AI agents, it does come with limitations. And the main one is so-called reward hacking. What this means is we have our LLM, it generates some kind of output which is then evaluated against some tests whether it's rulebased, LLM based or just real world data. those results are passed back to the LLM and this loop continues which hopefully makes the response better and better. So running an LLM in a loop in this way will incentivize the model to maximize its performance according to whatever evals you define and basically ignoring everything else. While this can be a very powerful force, it also opens up the door to reward hacking, which is when your model maximizes the positive feedback it's getting from your eval suite without necessarily improving the desired outcome that you're looking for. A concrete example of this could be a Twitter agent. Let's say you have a model that writes posts for you on Twitter or X. It'll write the post. It'll get fedback engagement metrics. It'll write more posts and get more engagement metrics. And then it'll continue in this loop. And its goal is to get greater and greater engagement metrics. What'll happen in this loop is that the tweets will become more and more engaging, but also they become more and more toxic. This is just the reality of any optimization process. Your system is just going to optimize for the metrics you're optimizing for. it's not going to consider anything else. So unless you explicitly have a penalty for toxicity in this feedback loop, the Twitter agent is going to ignore toxicity. It'll just be dead focused on improving engagement. This is an example from reference number one where they talk more about this reward hacking phenomenon specifically at test time. So we're not training any model parameters here. We're just feeding the output of the LM back to itself to improve it. The moral of the story is you need to be careful what you optimize for because computer programs, they're going to maximize the incentives presented to them. While this can be a superpower, if the incentives aren't properly set up, this can lead to very bad outcomes. So, I hope this video was helpful to you. If you have any questions, please let me know in the comments section below. And as always, thank you so much for your time and thanks for watching."
Nm_mmRTpWLg,2025-04-14T12:00:42.000000,LLM Workflows: From Automation to AI Agents (with Python),"Hey everyone, I'm Shaw. This is the third video in a larger series on AI agents. Here I'll talk about how we can build more powerful automations using LLM workflows. I'll start by reviewing a handful of common design patterns and then walk through a concrete example of implementing an AI virtual assistant using OpenAI's agents SDK. Before talking about LLM workflows, let's first define what a workflow actually is. Here, I'll define it as a set of steps that generate a desired result. So, let's say my desired result is to have an empty inbox. My workflow might look something like this. I'll have an email appear in my inbox. I'll read the email and then if it's something that is important, I'll reply to it. However, if it's not important, I'll delete it. While this is a very simple example, the point is that we all use workflows every single day. And the value of explicitly defining workflows, no matter how simple they may seem, is that this is what allows us to create automations. For example, if I wanted to automate my email workflow, it might look something like this. So instead of just manually reading the emails, I can take the emails, pass them through some kind of email categorizer. This can determine if the emails are junk or not junk. The junks can go to the trash. Then I can have the notjunk emails go through another email categorizer to determine if it's something that I can easily automate with an email writer. But if not, I can just leave it for myself to finish. Ever since we've had computers, there has been a interest and desire to define automations. And a key part of this process is to explicitly define every single step in detail of a workflow. So then you can translate key steps into computer code. However, there is a key limitation in building workflows using just traditional software, which is that building some of these components with rule-based code is hard and limited. For example, to implement a rule-based email classifier, we might look for certain key words that are red flags, or we might analyze the sender's email address. We might see if we've received an email from this person in the past, and so on and so forth. If we were doing this from scratch, it would probably take us some time and some critical thinking. But it's going to be really hard to write a computer program that can be as good as, let's say, a human at doing this classification because there's just a long tale of possible emails that we could receive that's hard to anticipate and account for when writing a computer program like this. However, more recently, getting computers to do things that we want has actually become much easier than before. And this is thanks to large language models. Now, instead of restricting our workflows to only traditional software, we can now build workflows that involve large language models. Looking at that same workflow from before, instead of trying to implement some of these key components with code, we can just plug in a large language model into the workflow. These robots here represent large language models. And so these are things we could get pretty decent performance very quickly by simply writing a well ststructured prompt for each of these tasks. Ever since we've had large language models, people have been trying to figure out ways to incorporate them into software systems like this. And this has kind of given rise to some controversy because some people might see the system and call it an AI agent while some other people might see the system and say this is not an AI agent. However, most people I talked to try to avoid the controversy altogether. So instead of debating whether this is an AI agent or not an AI agent, they adopt this language coined by Andrew Ying of so-called agentic workflows. And this is basically saying that instead of thinking about agents as being this binary thing, thinking of agency on a spectrum. At one end of the spectrum, we have completely rule-based workflows like the one we saw in the previous slide. And then on the other end of the spectrum, we have workflows that have the same level of agency as a human. For example, this system has agency via these LLM components because we're not explicitly instructing the LLM how to classify each and every email or how to respond to each and every email. We just give it guidelines through a prompt. However, this slightly different workflow would have a bit less agency because here we explicitly define the rules for this first categorization and then maybe we have predefined scripts for responding to particular emails. But of course, we could have a little more agency by letting an LLM draft the emails in whatever way it likes. Taking this to the extreme, the most agency would basically be having an LLM as a drop in replacement for the human and it internally and implicitly decides the best way to do this entire workflow in its head. While there are countless ways to create these agentic workflows, there are a handful of common design patterns that have emerged. This is summarized nicely in reference number two, which is a blog post from Anthropic. And so, I actually talked through each of these design patterns in the first video of this series, so I won't spend too much time here. Some basic designs are simply chaining multiple LLMs together or multiple components together or using LLMs to create these bifurcations in the workflow. And so we actually saw examples of both of these in the previous slides. However, we can make things a bit more sophisticated by incorporating parallelization. This can involve taking a task, splitting it into subtasks, and then performing them at the same time to improve latency. or you can perform the same task by multiple LLMs and then have some voting mechanism to have higher quality outputs. However, I want to draw a clear line between these design patterns and these on the right. The key difference is that in these first four patterns, the connections between steps are explicitly defined by the developer. A goes to B, B goes to C. A goes to B which decides whether B can go to C or to D and so on and so forth. The passage of information through the system is predefined. However, with these two design patterns, the flow of information is not necessarily predefined. Rather, it is driven by the outputs of an LLM. For orchestrator workers, you can have an initial LLM take a task, break it down into subtasks, and then decide which steps in the workflow are necessary. This actually comes in two flavors. One is basically the LLM can pass information to other LLMs which are later synthesized. So basically you have the LLM send out information and the information doesn't come back to it. Or you can have the LLM use other LLMs as a tool. Basically, you have this initial model send information to another LLM, which then sends information right back to that initial model. Another way LLMs can drive the flow of information is the so-called evaluator optimizer pattern. Here, an LLM will generate an output. it'll get judged or evaluated by another LLM which will give feedback to the original LLM and then this will basically continuously run until some criteria are satisfied by the output. So the number of times this loop will run is going to be input specific. So there's no way to know at the outset what that number is going to be. So I'll talk more about this evaluator optimizer design in the next video. While we are talking about LLM workflows and LLMs give us this really powerful and flexible way of getting computers to do things that we want, it's important to remember that LLMs aren't a cure all. They have their strengths and they also have their weaknesses. It's important as developers that we understand these strengths and weaknesses and make good design decisions when it comes to building out practical workflows. So a framework that I find helpful when making these decisions is thinking of software as three distinct types. This is based on a famous blog post by Andre Karpathi which is reference number four where he talks about these two kinds of software software 1.0 and software 2.0. More recently people have expanded this discussion to include software 3.0 which is basically large language models. But the story goes like this. Software 1.0 is just code. So the key activity that developers do in order to build systems with code is writing explicit instructions via computer code. So Python, C++, you know, whatever language you like. Software 2.0 also called machine learning is a different way of programming computers. Here instead of explicitly telling the computer what to do with precise instructions, the central activity you do as a developer is you curate high quality training examples. So you can basically teach the computer how to do a particular task. And now today with large language models, we have yet another type of software. Instead of writing code or curating examples, now the central activity is crafting the context to adapt the model's behavior. All three of these things have their strengths and weaknesses. One axis we can look at is predictability. By far traditional code is the best when predictability is important because you know exactly what will happen and why. You can define some input and you can explicitly see the chain of logic that will happen to that input and what is going to get spit out the other side. With machine learning on the other hand you don't really have this predictability because these are stochcastic models. You put in an input and you don't always know what you're going to get on the other side. Of course, machine learning is pretty broad. So, you have very interpretable and predictable models like linear regression and logistic regression. You have things that are a bit more interpretable like a decision trees. And then you have neural networks which are the least interpretable. However, with LLMs, this gets even worse because not only are you using a model to predict some output, like with software 2.0, you know, but with LLMs, you continuously feed the output of the model back into itself to generate text, and that's ultimately the thing that you're going to use. So, there's yet another layer of stochasticity on top of the model prediction itself. The flip side of this is flexibility. Code is the least flexible of the three. The system can only do what you explicitly programmed it to do. With machine learning, this gets a bit more flexible. types of inputs you can pass into it and the types of tasks you can perform with machine learning is much more flexible than what you can typically program a computer to do. And then finally, large language models are the most flexible. They continuously surprise us and they turn out to do things that we didn't realize they can do. So no longer do we have to explicitly tell the computer what to do or explicitly show it examples of what to do. It can do arbitrary tasks by simple prompting. And another important dimension here I would say is computational complexity. So this basically comes down to the cost of running one of these programs. So this is going to be cheapest for code. This is going to involve the least number of operations. This goes up by maybe a couple orders of magnitude with machine learning. And then this goes up even more when working with large language models which are not only massive machine learning models but you run inference for one of these models many different times. So the compute goes up even more when it comes to large language models. And of course there are other axes we can look at but these are the first three that jump to mind for me. You want to understand can I afford a high cost for this component? Is it okay for this component to cost two cents to run or do I really need it to basically be free in order to be practical? Because maybe I need to run it many different times with flexibility. Is it just a handful of different types of inputs that will get fed into this component or is it really unpredictable what types of inputs the component will receive? And then finally with predictability, is it critical that you fully understand and can interpret the behavior of your system in all different situations? or is it okay if there is some ambiguity if that ambiguity allows you to have specific performance? These are all things to consider when trying to decide which type of software should I use for a particular component in my agentic workflow. So I talked a lot about conceptual stuff and abstract stuff. So let's try to make it a bit more concrete with a specific example here. I'm going to build an artificial virtual assistant which I call Ava. As a solo entrepreneur, there are administrative tasks that I need to do, whether that's like sending emails to people or gathering information, creating documents, or whatever it is. Typically, entrepreneurs will hire VAS to help them out, but I am very poor right now. So, wouldn't it be great if I didn't have to pay someone and could just have an AI perform this function for me? This is kind of like the vision. I'm really fascinated by this idea of having email be the interface for an AI application. The idea is I will interact with Ava the same way I might interact with a VA that lives in another state or another country or something like that. So I'll just send her an email and then she'll do things like draft emails for me, update contact information, maybe gather information from a shared knowledge base or something like that. So basically a dropin replacement for a human virtual assistant. The way I'm implementing the initial version of the system is as follows. So I'll take user request which will come from me and then pass this request to a planner agent. The goal of this agent is to break down the task and understand what it is the user's asking for. and it has access to various readonly tools that allow it to gather information and to craft good instructions. A critical output of the planner agent is to decide whether some action is required. For example, if I'm just asking the planner, who are all the AI consultants in my contact list? That doesn't really require an action. It has these readonly tools. So it can gather the information and just send me an email with the information I need. So in that case, the workflow could stop and just send me an email with the information I requested. However, instead, if I asked Ava to write a three-way intro email to these two specific contacts, then it would determine that an action is needed, and it would pass the information along to an executor agent, which has the ability to perform writing actions. So, this could be like drafting an email, creating a new file, or updating someone's contact information. And so this is a very simple workflow and is essentially breaking down the process to perform any administrative task into two key components. A planning component which does like read only actions and a execution component which is actually performing actions in the real world. There are two key benefits in splitting this function into a planning and execution component. First is that the system can do more complex tasks because it isn't going to rely on a single LLM to do it. The second benefit is that the interface between the user and the systems ability to perform actions is buffered by the planner agent. This kind of reduces the risk of the system accidentally sending out random emails or overwriting important information in the knowledge base or something like that. Let's create this agentic workflow. So, I'll be using OpenAI's agents SDK. And then I wrote a handful of custom functions and tools for the agents, which I won't show here because it's just going to be a lot of code to show on the slides, but if you're curious, you can check it out at the GitHub repository. Finally, I'm just importing my OpenAI secret key and some other environment variables through av file, and I'm using thev Python library to do that. Next, we can actually create the agents. As we saw in the previous video of this series, OpenAI defines an agent as an LLM equipped with instructions and tools. These are kind of the two central components here. So, I'll start by writing out the instructions for the planner agent. Read instructions is just a function I wrote to read markdown files from a particular folder. And so, I'm concatenating two markdown files together. One is called ava.md. The other is called planner.md. And this is what the final instructions look like. So I'm just giving some context to the system. You are Ava, a virtual assistant to Sha Lebby, an AI educator and entrepreneur. You help them with administrative tasks. And then I have these additional instructions specifically for the planner. You write instructions for an executor agent to perform user requests. Unlike the executor agent, you only have access to readonly tools. For example, read directory structure and read file contents. Here we're just giving the agent context to its purpose and things that it has access to. One handy functionality of the agents SDK is that we can force agents to have structured outputs. We can do this using pyante. So I'm going to create this custom class which has three attributes to it. First is a boolean flag that says whether execution is required. This is what we're going to use to decide whether to stop the workflow or to pass information on to the executor agent. The next attribute are the instructions for the executor. So if execution is required, this will be a string and these will be the instructions to pass along. And regardless of whether execution is required or not, we'll still want to have a note for the user that'll just be a string. Structured outputs helps give a lot more controllability and predictability to the multi- aent system. We can use this when we create our agent. So this is just this agent class. We'll give it a name. We'll give it those same instructions we saw earlier. And then we can set the output type to that class we just created called planner output. And then here we have two tools that we're giving the agent. One is called red directory structure. Basically, it takes in a folder path and it spits out all the subfolders and the structure of the directory. Another tool is to read the contents of a specific file. If you're curious, this is available at the GitHub. So, you just go to tools/4agents.py and you can see the source code. And then finally, I set the temperature at 0.5. Temperature just controls the randomness of the model's outputs. So, we don't want it to be super random. that is just like gibberish, but we also don't want it to be like super predictable because we want the planner to come up with creative solutions to user requests. Then in a similar way, we can create the executor agent. So we'll define the executor instructions. In a similar way, we'll create the executor. We'll give it its name, give it its instructions, and then we'll give it tools. So ideally the executor won't need to use these read tools because ideally the planner agent gives it all the information it needs to perform the task. But maybe that doesn't happen every single time. So I just included these two tools just in case. The critical tools here involve the ability for the model to write email drafts and save it directly to my Gmail account. Another important tool is giving the model the ability to overwrite existing files. The code is living in this repository which includes various instructions for the agents, different guides and email templates that it can use in crafting drafts and a directory of people that I communicate with frequently which includes things like their name, their email, their URL and a short bio about them. So by giving the executor this tool, it can update the directory, it can update email templates, it can create new email templates. So it just gives the system a lot of capabilities. The key difference with the executor is I set the temperature equal to zero because I don't really need the executor to come up with creative solutions. I just need it to execute exactly what the planning agent tells it to do. So this is not really important to LLM workflows, but it's essential for this specific project, which is setting up the OOTH for Gmail. In order to allow the system to access my inbox and save drafts, there were a few steps I needed to do. So, I need to go to the Google Cloud Console, create a new project, add Gmail's API as a service, and set up my JSON credentials so I can authorize this application to access my Gmail account. And then in Python, I wrote this function called get Gmail service, which would set up the OOTH process. And then with this get Gmail service function, I can just set up the authorization using this script in the main file. So, I'm not going to go in depth on this whole OOTH process because it's not really central to the discussion here, but I do write out all the steps at the GitHub repository in the readme file. So, if you just go to the Google Oath setup, it'll give you step-by-step instructions. It's pretty straightforward once you've done it a couple times, but like I remember the first time setting this up, it was so confusing and I was so lost. If a lot of people are struggling to get this thing set up, happy to do another video on that. But assuming we've got the credentials set up, this is how we can run the workflow. I'll define this asynchronous function called main. The first step will be to grab the user request from this request.txt. So this is just for this initial version. The final vision here is that I'll just send an email to a mailbox called availebi.com or something like that. And that will kick off this whole process for me. But for now, I just put my request in this text file. And then I can run the planner agent using this code here. This is going to give us this result object that we can run in an asynchronous loop, which will basically allow us to stream outputs from the LLM and we'll pass in our request to it. Next, we can process the request. I'll just print some things to the command line to let the user know what's going on. And then a key function here is this handle stream events. This just a custom function I wrote to print various things like when the agent was changed or when a tool was called, what inputs were passed to a tool and to print model outputs. Since it's not like super important to workflows, I won't print it here, but the code is available in the GitHub repository. But basically the planner agent will run and then once the planner agent is done we'll print the response. This final output will have three attributes corresponding to that custom class we created. So we'll print the user note to the user and then we'll see if execution is required. If execution is required this will be true and then we'll run the executor agent. Now we'll pass in the executor agent instructions to the executor agent and then we'll do a similar thing. We'll print executing to the command line and then we'll print the final output of the executor agent. And then again if you're curious about this handle stream events function. You can check out the functions.py file in the GitHub repository. Okay, so let's just see this thing in action. Here's my terminal. I'm going to do Vim so we can see the instructions. So it's kind of like an email. Send call follow-up. Can you send a follow-up email to iffy? And then I just had some meeting notes. Then we can do python ava.py. And then this will run the whole workflow. We can see the planner agent is going. And then we see like it's going to call some tools. So I can pause it real quick. Read directory structure. Read file contents. Read file contents. So it's going to read the directory structure. So this is the directory that it lives in. And then it realized that it needs to use an email template called call follow-up.mmd. And then it also realized it needs to read this directory.csv file to get the contact information for iffy. And then this is what the planner agent said. I will send a follow-up email to iffy using the provided meeting notes. Then it's going to pass it over to the executor agent. Then the executor agent is using this write email draft tool. Here are the arguments. So, it's giving the recipient email, the subject line, and then the body of the email. Finally, the executor agent spits out a message to the user saying that the email draft to Iffy Auna has been created successfully. Seems like everything worked well. So, we'll slide over to my Gmail account, hit refresh, and then we can see indeed the draft was created. Then I can click on that, and I'll zoom in so it's not so small. And then we can actually edit the email before sending it because a lot of times you don't want to just send communications to people straight from LLM. You want to review it and make sure it's something that you want to have your name on. I added my Gmail signature and then I'll add like a couple of small changes cuz these are just raw notes before sending it off. So I can add emojis and you know do whatever I want before sending it. I'll call out a couple of things. So like one is this email guides folder. So, here's where I have some email templates cuz there are a lot of emails that I send that are just like copy paste, change a few things. A lot of those I don't really need to send myself. Ava can send those for me. And then another key thing is this directory.csv. So, this has like contact information and bios for people that I communicate with frequently. One thing I'll end up doing a lot is these like three-way emails between businesses and a handful of AI consultants that I know. This directory is great because I can just tell Ava, this guy's looking for this help. He runs this company. He's looking for people to do this. Can you just send three introductions between him and people in the directory? And it surprisingly did a pretty good job of figuring out how to do that task. And so here we did a pretty simple example, but hopefully it gave you a flavor of what types of things we can do with LLM workflows. The great thing about workflows is that they give us a great balance of capability and predictability and controllability because we can incorporate these guard rails and design the system in such a way so that it does the things that we want it to do in a reliable way. The example here we had this closedended flow. So we had this planning agent which passed information to an executor agent which actually executed the task. Even though there is some magic happening within each of these agents, the overall system is relatively predictable. It's this closedended flow. However, there can be a lot of value in having the so-called open-ended flows. This goes back to the evaluator optimizer paradigm. Instead of just running a step a fixed number of times or in a predetermined way, we can have LLMs receive feedback on their outputs so that it can perform increasingly complex tasks. And so this open-ended flow is going to be the focus of the next video in the series. So, I hope this video was helpful. If you have any questions, please let me know in the comment section below. And as always, thank you so much for your time and thanks for watching."
-BUs1CPHKfU,2025-04-07T12:26:44.000000,How to Improve LLMs with Tools (ft. OpenAI Agents SDK),"Hey everyone, I'm Shaw. This is the second video in a larger series on AI agents. Here I'm going to talk about an essential feature of agentic systems, which is giving LLM's access to tools. I'll start by discussing how this works under the hood, then share a concrete example of building a simple YouTube video agent using OpenAI's new agents SDK. AI agents give us a new way of thinking about software. For example, if we were building a traditional software system to do say customer support, it might look something like this. We'll have a customer query come in, then we might devise some clever rules or business logic to categorize this message. This could be categories like it's a password inquiry, a billing inquiry, account inquiry, and then so on with the long tale of things that customers ask about. And then within each of these categories again, we might have another set of categorizations that need to happen to get even more fine grained on what the question is actually getting at. Then for each of these fine grained examples, let's say we have like a pre-anned solution that we can generate and pass back to the user interface. Well, this can account for the vast majority of questions because typically there's usually just a handful of very common issues that come up like 70 or 80% of the time. There are a few limitations with building a software system like this. For one, it's very inflexible. So, it's going to work really well on the common issues, but again, there's going to be this long tale of things that customers ask about which may not work so well with this software system. And if you want to account for this long tale of inquiries, you're going to have to come up with some clever rules once again, which takes time and energy and money. However, if we were to try to solve this problem using an AI agent, it would look somewhat different. Instead of meticulously defining all the common questions and prewriting solutions to all of these things, we might just take a users query, pass it to an LLM, and rather than predefining responses for this system, we might do the following. So, we may give the LLM context through company onboarding documents for customer support specialists. We might give it access to a document search tool that will allow the LLM to generate queries and search over known FAQs and support documentation. And then maybe we'll also give the LLM access to a tool to escalate issues to human support. Two, the key benefits of this AI agent are one, it's a very flexible system. It's not going to be so rigid in that it can only process a handful of popular issues. It can go pretty far into this long tale of inquiries that customers might come up with. But perhaps most strikingly is how simple this solution design is. With the traditional software system, you can imagine this very complex chain of logic, if else statements, and workflows. But with the AI agent, we're just bringing together a handful of key components. So this highlights the key difference between traditional software and building agentic systems. In traditional software, you have to explicitly define every single step of the solution and implement it into code. However, for agentic systems, the mindset shifts into just curating the right tools and giving it to an LLM so it can solve the problem on its own. And so tool use plays a central role in agentic systems. But we might wonder to ourselves, how does this actually work? The trouble is that LLMs on their own cannot use tools. They are simply text generators. Given a sequence of tokens, they simply predict the most likely next tokens. So in order to get these models to use tools, we have to wrap LLMs with some code. This involves three basic steps. First, we need to monitor an LLM's outputs to see if it's trying to make a tool call. So, this can involve just some reg x check or string matching to see if a sequence of tokens or a special token is generated in the LLM's output. Next, if we find a stopping token or a sequence of tokens, we can pause the generation and then execute whatever tool call the LLM was trying to do. And then this is going to be done with some separate code. And then finally we can capture the response of that tool call and inject it back into the context of the model. So to give a concrete example of this, let's say this is the raw text that the LLM is operating on and is generating text from. So, just to break this down step by step, let's say we start with a system message where we tell the LLM, you are a helpful assistant that can use tools when needed. So, we're just letting it know that it can use tools. Next, we have a user query. So, the user might ask something like, ""What's the weather in Dallas?"" Then, we might have the assistant recognize that it needs to use a tool. So, it says, ""Oh, to answer that, I need to get the current weather in Dallas calling weather."" So, it's going to use this get weather tool with location equal to Dallas. So, this is it realizing it needs to use a tool. And then whether it was fine-tuned for tool calling or there was some other prompting involved outside of the system message, the LLM knows to use these special tokens or these special strings to demarcate its tool calling. And then the tool call has this special JSON format where you have these key value pairs. So the tool call involves a name. So we're going to use the get weather tool. And that get weather tool takes in a set of arguments. So in this case, it's just one argument called location. And for the location, we're going to input Dallas. And then that's the end of the tool call. So this is something predefined. This is something generated by the user. This is generated by the LLM. This is also generated by the LLM. But once we see this special tool called end token, we can pause generation, pass the Dallas input to this get weather function which let's say is implemented in Python. We can run that code, get the output, and then manually inject this text into this context. We can add this system message that says results from get weather. It's 72° Fahrenheit and sunny in Dallas. And then end. And then from here we can relinquish control back to the model and then have the model generate the rest of its output. While this is just a toy example, the specific special tokens used is going to depend on the model you're working with. The structure of this tool calling is going to depend on the specific model or the way you're implementing the tool usage. And still there's no single standard. Different model providers will fine-tune their models slightly differently. And people might also hack non-finetuned models to do tool use in weird and interesting ways. But hopefully this example gives you a better idea of what this looks like under the hood. But a key question here is how does the LLM know how to do this? How does it know to do tool call and then generate a JSON formatted text like this and end the tool call? Really, there are two ways to pull this off. The first way is simply prompting the model. just telling the model that it has access to these tools and give it instructions on how to use those tools. So here's one example. We can just simply describe the tools. Here's our system message. You are a helpful assistant that can use tools when needed. You have access to the following tool. Calculator evaluate simple math expressions like 5 * 4 + 2. How to use tools. So now we're giving it instructions. If you need to make a tool call, respond only in the following format. Tool call start, tool call end. So notice here we didn't put a JSON format, but this is how we're telling the model it can use the tools. So this is going to require us to continuously monitor the outputs of the model and check for this end token or special string of slashtool call. We know when the model generates this, it's trying to run this function. So we can write some extra code to implement that. And then we can inject the output into this conversation. So this is how a lot of people implemented tool use over the past 18 months, 12 months or so. They simply just took these LLMs off the shelf. They were instruction tuned. So they know how to follow instructions, but they weren't specifically fine-tuned for tool use. So the way they kind of tricked the model or got the model to use these tools is by doing a lot of prompting and writing some extra code to implement this. However, over the past 6 to 12 months or so, this has gotten much more straightforward because now we have these fine-tuned models specifically for function calling and many of which are available via APIs like OpenAI, Anthropic, Mistral, Together AI, and the list goes on. In these cases, we don't actually need to put anything into the system prompt. We don't need to give any explanations. All we need to do is provide this JSON schema for each tool that we want to provide to the model. This going to include things like the type. So let's say it's a function. It has a name. It has a description. It has some input parameters. It has other properties that have some special meaning in the particular API that you're using. And so this might be different depending on the API provider, but they all take some kind of JSON input which allows you to define how these functions work. Since these models are specifically fine-tuned to use tools, there are two key benefits and using them via these APIs. First is that the API automatically passes the tool info to the model. So there's really no need to include a description in the system message because based on however they fine-tuned the model, they devised some specific structure or format for passing this information to the model. The second handy feature of using these fine-tuned models via an API is that the API will automatically stop inference whenever it's time to execute a function call. If you're streaming from Anthropics API, it's going to stop and it's going to give you a stop reason of tool use. And then your software, your code that you're running can know that Anthropic stopped streaming tokens to me because the model wants to call a tool. Let me see what tool it wants to call. Okay, let me run that code. Okay, let me get the output and then let me pass that output back into the context in whatever way that I want to do that. So this is the most practical way of doing it and then this is what you're going to do 99% of the time. However, there is another way to use tools with LLMs. So basically there may be situations where you have a lot of tools or maybe you have very esoteric tools where models out of the box don't really understand how to use them effectively, especially if multiple tools need to be chained together. If deciding what tools to use in of itself is a complicated problem to solve, the model might need some additional training, some additional guidance on how to use them effectively. So, at a high level, what this is going to involve is to gather a bunch of examples of the complicated andor specialized tool calls and use that to fine-tune the LLM. Whether that's a model that's already been fine-tuned for tool use or maybe it's just a LLM fine-tuned for instruction following. But of course, this is an order of magnitude more sophisticated and requires more effort than just simple prompting. But if you want to get a flavor of how to do this, reference number one is really nice. It's a talk all about fine-tuning LLMs for tool use. So if you're curious, you can check out that video here. I want to make things more concrete by giving a specific example. I want to use Open AI's agents SDK to create a very simple YouTube video agent. What this is going to involve is simply getting an LLM and giving it access to a single tool which allows it to transcribe YouTube videos based on a link provided by the user. So, let's just walk through this code. First, I'm going to import a few things. I'm going to use this YouTube transcript API which allows you to transcribe YouTube videos easily. Then I'm going to import regx for extracting video IDs from YouTube links. Then I'm going to import a bunch of stuff from the agents SDK. So this agent class, this uh function tool decorator runner allows us to run agents. Item helpers allows us to keep track of all the events that come out of the agent. And then run context wrapper. I guess we'll see what that does. I don't remember off the top of my head. Then we're importing this subm module from OpenI's library. So this response delta event. So this will allow us to stream token. So we don't have to wait until the LLM generates all of its response to print it to the user. We can stream it one token at a time. And then I'm importing this env library which allows you to create av file to import environment variables. So this is just how I'm going to import my OpenAI secret key. I have av file where I have my secret key stored and then this function will import it into my Python environment. And then finally, I'll import async io which will allow us to do some asynchronous stuff which we'll talk about later. Openai in their agents SDK defines an agent as an LLM equipped with instructions and tools. So let's define those things. Our instructions will be very simple. you provide help with tasks related to YouTube videos. Nothing fancy. Then we're going to define the tool. This is going to be more involved. And one really great feature of the agents SDK is that you can easily turn any Python function into a tool by adding this function tool decorator to it. What this does is by putting this on top of our function, the agents SDK will automatically parse the function name, the inputs and its types, the output type and the doc string to automatically construct that JSON schema we saw earlier for doing function calling via OpenAI's API. So, we don't have to write that complicated and confusing JSON schema. we can just write Python code that we're comfortable with and automatically turn any function into a tool. So my tool here is actually very simple. It'll extract the transcript with timestamps from a YouTube video. It takes in a YouTube video URL and then it returns the transcript with the timestamps in the minute seconds format. Here we're going to do like a reg x search to extract the video ID from a input URL. And then if this matching process fails, we can raise an error. Then we can just extract the video ID like this. And then once we have the video ID, we can extract the transcript using this YouTube transcript API. It has this get transcript method. We just pass in the video ID. And then what we're going to do is extract the text from the transcript. And so this transcript object is going to be a list of dictionaries. So we have entry which is a dictionary. Transcript is a list. And then we're just going to extract the start time and we're going to format it in minutes and seconds. And then we're going to extract the text. We're going to format it as this string with the timestamp and then the text. So the transcript from that specific timestamp. And then we're just going to put all this into a list. And then we'll just do join to format this list as a single string. And then if anything goes wrong, we can throw an error. Okay. So we now have our instructions and we have our tool. So we have all the ingredients we need to make an AI agent. To make an agent with this agents SDK is very simple. We just need to give it a name, give it instructions, and give it tools. And then OpenAI has a default LLM it uses for the agents class. It's going to be one fine-tuned for function calling and following instructions. That's what I'm using here, but you can additionally specify whatever model you want to use as well. Everything we've done up until this point was actually very straightforward. Writing the instructions were trivial, the function was very straightforward, and then making the agent was trivial. However, now this is where the code gets a bit trickier. And a big part of that is these agent systems are going to be asynchronous, meaning that they're not going to run in a predefined sequence of steps. This is important first and foremost for streaming tokens from an LLM because for example if you were to interact with an LLM synchronously that would mean you make an API call and then you wait you get a response from the LLM and then you do the next step in your process. However LLMs can take quite a long time to generate tokens. So it's much better to stream the tokens in real time. But this is going to require you to write your script in an asynchronous way. So basically what's going to happen is you're going to make your API call and instead of waiting for all the tokens, you'll just grab some of the tokens. Then you'll display those tokens to the user. Then you'll go back, grab some more tokens, and then you'll display those. So you can kind of task switch between grabbing tokens and displaying tokens to the user. Additionally, this is helpful with agents doing tool calls themselves where they might be calling an API and now you're waiting for like a weather API to get back to you. So instead of just holding up your entire application waiting for these other processes to run, you can write it in an asynchronous way and allow the user to interrupt the agent. Like you can allow your code to do other things while it's waiting for these tasks to happen. So that's just like a long-winded way of saying that running the agent is going to be the most sophisticated part of this example. So here I just implemented a very simple command line interface. So we're going to write a function called main that will allow us to interact with this agent in the terminal. I'm initializing this list to have like input items. So these are the messages we'll pass to the LLM. And the first thing we'll do is we'll print a few lines. So we'll introduce the application. We'll introduce the agent as a YouTube transcript agent. We'll tell the user that it can type exit to end the conversation. And then we'll give it some instructions. Ask me anything about YouTube videos. And then we'll have this wall loop going where we'll allow the user to type in an input. So we'll display you and then the user can type something in. Then we'll capture that input as user input and then we'll append it to this input items. So, this list of dictionaries with all the messages we're going to give to the LLM. Then we'll check if the user typed in exit, quit, or buy. And then if it does, we'll break out of the while loop. If not, we'll just keep going. Also, if the user doesn't type anything, we'll just kind of go back and see if the user wants to type something else. But if the user types something, we can go to the agent. We can print the agent and then we can have this runner.runstreamed method running. So here we're actually running the agent. So we have this agent running and we're passing it these inputs. The inputs are going to be so far whatever the user typed in. And the agent is what we defined on the previous slide. Here there are a bunch of events we can check for and do different things based on them. And the code's getting cut off here, but it's all on the GitHub repository. But basically in this first one, we're just seeing if we're getting tokens as the next event in this stream. If we are getting tokens, we're going to print those. But then other important events are a tool call. So if the LLM uses its one tool, which is fetching the transcript, we'll just print that to the user interface. Then if the event is the output of a tool call, we can append this transcript to the context. So we can add this as a system message. It's getting cut off, but we're adding it as a system message. Then we're saying transcript fetched. And then this last one, this is the entirety of the output from the LLM. And then we can just add this to our input items as the assistant. So this role that is getting cut off is assistant. And then this is just the response from the model. So a few things are happening here. We're handling not just like the back and forth conversation between the assistant and the user, but we're also incorporating these tool calls. So we're doing things when the tool gets called and we're adding the transcript to the context via the system message. Additionally, we're going to be streaming tokens using this like response delta which is also getting cut off. Then we're going to print a new line after the assistant response. And so this is all pretty complicated and you know maybe you don't even have to implement this yourself. Maybe you're just building a UI with gradio or streamullet or something like that and this doesn't really matter. But once we have this function, we can just simply have this async io.run main. This will spin up the terminal if we're running this as a pi file. However, if you're running this in Jupyter lab, it's already running this for the Jupyter notebook UI. So you can just simply write await main and then this is going to add it to the event loop for the async functionality. But just to give you guys a demo, this is the display text. So YouTube transcript agent type exit to end the conversation. ask me anything about YouTube videos. Here I'm asking it if it can summarize this video and then that's the link from my previous video in the series and then it just goes off and summarizes it. It does a really good job. It talks about the three levels of agency I talked about and it summarizes the thing. So it says deeper discussions, practical implementations and following videos. That's good. And now I'm asking okay so we got a summary. Now let's get chapters. So this is really cool. automatically it's figuring out the chapters and just skimming this. This is doing a pretty good job. Usually takes me like a few minutes or it could actually be pretty long if I'm trying to write timestamps for a podcast that I've done. But using a tool like this would make it very fast. And then what's cool is that it automatically links to each of these timestamps. So you can actually click on these in the terminal to jump to that point in the video. And then finally, I have it write me a LinkedIn post summarizing all the key design patterns I talked about in the previous video. It's a simple agent, but this was me just like cracking open the agents SDK. It's quite a practical use case for me as I make a lot of YouTube videos and like repurpose content from it. But I guess there are things already from like Google with Gemini that you can probably do stuff like this out of the box. But as a technical person and builder, I always love building stuff myself. Okay, so what's next? Here we did a pretty simple example. This is probably like the most basic system that someone might still call an agent. But there is a key limitation to basically just let's curate all the components and rely on the LLM to solve the problem for us. And the limitation is that these systems may behave in unpredictable ways. So they're very capable because they can potentially solve problems on their own and you don't really have to be involved and use your own brain power to do that. But this comes at the cost of predictability. However, we can kind of balance these two things of predictability and capability through so-called agentic workflows. So, I discussed this briefly in the previous video, but these days most practitioners I talk to when they talk about building AI agents, they're really thinking about agentic workflows. So, not just having a single LLM doing everything, but rather you have multiple LLMs perhaps chained together. These LLMs can have access to tools, basically what we built here, but integrated into a larger software system. And so the upside of Agentic Workflows is that you can have these guard rails and eval built into the system to make sure that it doesn't go off the rails. So this is going to be the main focus of the next video in this series. So I hope this video was helpful. If you have any questions, please let me know in the comment section below. And as always, thank you so much for your time and thanks for watching."
ZaY5_ScmiFE,2025-03-31T12:38:50.000000,An Introduction to AI Agents (for 2025),"Hey everyone, I'm Shaw. This is the first video in a larger series on AI agents. Here I'll start by discussing what an AI agent actually is and why we should care about them. Then I'll review three common types of agents at various levels of agency. 2025 is said to be the year of AI agents. Yet for many people that I've talked to, it's not entirely clear what this actually means. Part of the problem is that no one can seem to agree on a single definition for AI agents. To demonstrate this, here are some definitions from a few leading organizations. Open AAI in their agents SDK documentation defines an agent as a large language model configured with instructions and tools. This is different from say hugging faces definition from their transformers documentation which describes it as a system where a large language model can execute more complex tasks through planning and using tools. So here there's a subtle difference. Open AI's definition is focused on tools. That's like the main thing. While hugging face they talk about tools but they also mention planning which is basically taking a user's query or an input breaking the task down or the problem down into specific steps and then executing it. And then if we look at anthropics definition and so this is from a blog post called building effective agents where they define it as systems where LLMs dynamically direct their own processes and tool usage maintaining control over how they accomplish tasks. So again we see this mention of tool usage. However, anthropics definition seems to be primarily focused on autonomy. Basically LLM systems that have complete control over how they do things. So, I won't try to make things even worse by proposing yet another definition, but instead I want to talk about three key features that come up regardless of the specific definition that people might give to agents. So, first and foremost, there's always an LLM involved. So, in those three definitions we saw in the previous slide and all the other definitions I've seen, LLMs play a central role in what people are calling AI agents today. So, it's going to be an LLM represented by this robot emoji and then some other stuff, some software you write around it. Maybe you use a framework or you code it from scratch. But there's some other stuff that kind of augments the LLM. The next thing is tool use. So, this can be something like a web search, a Python interpreter, memory, sending an email, or even access to a different large language model that is specialized for a particular task. And I'll talk about why tool usage is so important for agents on the next slide. And then finally there is usually some mention of autonomy. So we saw in hugging faces definition there was this notion of planning and anthropics definition which we'll talk more about. They really stress the importance of these like open loops. This ability for the model to reflect on its outputs and get real world feedback on them. Another thing this might include is the so-called reasoning capabilities of LLMs like DeepSeek R1 or OpenAI's 01 or 03 models. This ability for LLMs to quote unquote think before providing an answer allows it to accomplish more and more complex tasks without explicit instructions. And so everyone is very excited about agents these days. People are saying 2025 is the year of AI agents. YC is saying that vertical AI agents could be 10 times bigger than SAS. And this might be very reminiscent to a couple years ago when chatbt came out and there was a lot of excitement around LLMs and specifically building chatbots. Back then I had people reaching out to me all the time saying how they wanted to build some kind of chatbot for their company for some specific use case. And then fast forward one year later, a sentiment I heard from a lot of business leaders or business operators is that they weren't able to get ROI from these chatbots. And so they might be thinking it's just a repeat of this is just like another hype thing that isn't going to generate return on investment. While there's a lot to unpack there, the key difference between agents and chat bots is that LLMs alone aren't enough to solve most real world problems. And so there are two key things that I would say make agents much more valuable than just vanilla chat bots like the original version of chat GBT. So the first is that agents can interact with reality through tool use. LLMs just out of the box they don't live in reality. They live in an imagined reality. They live in some kind of imagination land. This is why hallucinations are a big problem with LLMs. And then if you asked one of these LLMs to write you some Python code and execute it, it would just make up the outputs. It wouldn't actually run the code because LLMs can only generate text. They can't run code. They can't do web search. They can't do a lot of useful things to get feedback from the real world to make them more useful. So tools take LLMs from their imagination land based on their pre-training data and put them in the real world. kind of give them real world feedback, real world context. Another reason why agents are inherently more valuable than the vanilla chat bots we've seen in the past is this discovery of so-called testime compute scaling. And this is just a fancy way of saying that the more tokens an LLM generates, the better its responses. So this is the whole idea of giving models time to think about problems, allowing them to kind of reason about problems, come up with an action plan before giving their final answer. And so today with models like 01 and Deepseek R1 which are trained using reinforcement learning, now we have models which are much more capable of solving arbitrary tasks without explicit instructions than before. And this ability to just oneshot novel problems is actually very valuable for agents because now you don't have to write very sophisticated and detailed prompts for specific problems. These models can solve a wider range of tasks. Well, and if you're not familiar with this test time compute and you know reasoning models and all that, I talked all about that in a previous video which I'll link here in case you are interested. Since no one can seem to agree on a single definition for AI agents, most practitioners I talk to don't really worry so much if a system is an AI agent or not, but rather they just talk about agentic systems. Basically conveying this idea that agency is on a spectrum. In other words, you can have systems with no agency. So basically like a rule-based system where the logic is hard-coded all the way to a system that has human level agency. And so here I'm going to convey this idea by talking about three systems at increasing levels of agency. So level one is an LLM plus tools. Level two are LLM workflows and level three is an LLM in a loop. So tool use is a central feature of any agentic system. And the main thing here is that tools expand the capabilities of LLMs beyond text completion. Two common tools that you might give an LLM are web search and a code interpreter. And so these two tools kind of fill in two major blind spots of an LLM. For LLMs out of the box, their understanding of the world is based on a snapshot from like 12 months ago, which was captured by their pre-training data. And of course, the world has changed a lot over the past 12 months or so. So if you were to ask an LLM about anything recent, it would basically just make something up. This is where web search can help. You can give the model access to a tool like web search and then you can train it to identify when it receives requests where web search is going to be a helpful thing to do. Another really handy tool is a code interpreter. This allows LLM to do a lot of different tasks. for example, doing math, which is something that LLMs really struggle with. Instead of basically the LLM trying to do the math in its head, it can write some Python code to do some complicated arithmetic and it can send that code to its Python interpreter. The code can actually be run and the results can be sent back to the LLM. And no, without the code interpreter, the LLM would still be able to write code. It just wouldn't be able to run it. So, it would basically make up the outputs. And so these are two very fundamental tools, but there are many others that you could use. So another big one are API calls. So maybe you want to fetch information from YouTube or you might want to connect to Gmail. So maybe you want the LLM to be able to read your emails or to write emails on your behalf. Or even you might have like a notion page where you have a lot of notes and you want the LLM to be able to access those notes and do some kind of task that is helpful to you. And so notice the API calls go both ways. They can both be used to give the LLM real world context, real world information, and they can also be used to allow the model to take actions in the real world, which are two essential components of agency. Another popular one is computer use. Not every program you care about has an API, basically has a programmatic way of interacting with it, but it does have a graphical user interface. So computer use enables LLMs to interact with these graphical interfaces using mouse clicks and keyboard strokes. Another tool could be another model. So for example, ChadByt has access to a text to image model. So if you ask it to create an image for you, it can take that request, pass it to this external model, and then take that resulting image and show it to you. Another tool might be rag. So instead of searching the internet, maybe you just want to search an internal knowledge base or over a set of documents or something like that. So that's where RAD can be helpful. And then another helpful tool is memory, which allows the system to retain important information over a long time period. So this could be over multiple LLM calls or even over multiple conversations. So, while this level one agent of LLM plus tools matches OpenAI's definition, many people I've talked to wouldn't consider this a quote unquote AI agent, the core limitation of systems like this is that you're basically relying on a single model and a single thread of information to solve whatever task you're trying to do. And so, while this works fine for one-off tasks or small tasks, as the requests get more complicated, this can start to break down. Like for example, if I wanted a system to like end to end write blogs for me, so basically come up with blog ideas, pick out the best one, write a first draft or something like that. This might be hard for a single model to do well, even if it does have access to like web search to do research and a library of like past blogs and past topics or something like that. This brings us to level two, which are LLM workflows. And so most of the agentic systems that you'll see these days are this kind of system. So frameworks like langraph and llama index are really optimized for building these kinds of agentic systems. And so all a LLM workflow is is a predefined sequence of steps with at least one LLM involved. To give a concrete example, let's say I wanted an agentic system, an AI agent to help me respond to emails. Instead of just taking an LLM, giving it very detailed instructions, and giving it access to my Gmail, like the level one system, I might define a workflow like this to kind of make the system a bit more reliable. So, the workflow would look something like this. An email comes into my inbox. Then, this LLM would spit up and it would categorize the email. Basically, it would see if it's junk or not junk. If it's junk, the system would move it to the trash. If it's not junk, I could pass it to another LLM to decide if it's a hard email to respond to or just like a easy email to respond to. So, if it's just like a sounds great, thanks, or an email that I've written many times before, and there's like a template for it or something like that, it can make that decision. Notice we could have just had one LLM do this categorization where we have one LLM that decides whether the email is junk, an easy response, or hard response. But this is a more complicated task. So it's going to be harder for a LLM to do in just one shot. So by splitting this complex task into two simpler tasks, we actually make it easier for each of these LLMs. And maybe you don't even need to use an LLM here. Maybe you can just use a fine-tuned text classifier to do the junk not junk. But then maybe this categorization is a bit more difficult and harder to gather training data for. So an LLM is better suited here. Okay. But then going on with the workflow, if it's a hard response, the system can just leave it for me to respond to. If it's an easy response, it can get passed to a LLM email writer that specializes in this, and then that LLM can send an email. And so the key difference between level one and level two is that we're no longer limited to just a single LLM with access to some tools, but rather we can have multiple of these LLM plus tool systems working together to solve more and more complex tasks with better performance. And so there are really countless ways you can design LLM workflows. Some are going to have more agency than others, but there's this really nice blog post by Anthropic. It's reference number three. It's called Building Effective Agents. And there they break down various common design patterns for these workflows. So I just wanted to mention those here. So the first one is chaining. So this is like a very simple design. You have module A passes output to module B which passes output to module C. So maybe you have a system that is going to write blog posts for you. So you have one module just coming up with ideas. Another module deciding what ideas are good and what ideas are bad. And then you have a third module that writes the first draft of that blog post. And again, you could probably have a single model do this, but it probably wouldn't perform as well as a system that breaks it into more specialized roles. Another common design is routing. So this is something we saw in the example I shared on the previous slide where you have like this classification step and a fork in the workflow. So let's say you have an email come in and you want to classify it as junk and not junk. And that's because you want to treat these two different types of inputs differently. Another common design is what Anthropic called parallelization. So there are two kinds of parallelization. The first one is sectioning. Sectioning is when you take a task and you split it into various subtasks that can actually be run at the same time. A good example of this is say a user types in a request into a chat interface and let's say to process this request you need two things to happen. one, you need the LLM to generate a response, and two, you need to make sure the request is appropriate and abides by your terms of use or some set of rules that you have for your application. So, you can actually run these tasks at the same time because they're not dependent on each other. And this will result in lower latency because you don't have to check if the query abides by the rules and then generate a response to it. You can just do these at the same time and then generate the response. Another version of parallelization is voting. So let's say you have some kind of lead scoring system. You want to grade leads as like lowquality, medium quality, high quality or something like that. But this is a pretty hard task where a single LLM might get it wrong like 60 or 70% of the time. But one way you can kind of overcome this is instead of just giving it to one LLM, you can give it to 10 LLMs. the same task of score this lead and then you can aggregate the responses of all 10 LLMs to give your final result. The key upside here is that you can improve the performance of your agentic system while here you can improve the speed of your agentic system. Another common design is this orchestrator workers paradigm where instead of just having a rigid workflow like we saw in the previous slide where basically everything was predefined and we have some agency within the LLM modules because they have the freedom to perform that specific task in whatever way they like. This may not work for tasks which are a bit less predictable where you don't have a clear workflow for all possible requests. In this orchestrator workers design, you first have the request go to a planning LLM. This could be a really smart LLM that can reason and think through problems. And basically its only job is to come up with the workflow to come up with the plan of solving this problem or performing this task. And then based on that plan, the tasks are delegated to separate systems. So to multiple LLMs or maybe to some predefined code components that tend to come up a lot for your specific use case. So this is kind of analogous to how you might have a single LLM think step by step through a problem and then solve the problem, but instead of everything happening within one LLM, you can spread this task among multiple LLMs to potentially get better performance. Finally, we have the evaluator optimizer paradigm. The way this works is that you have an LLM that generates some response or some output and then you have another system review that response and give feedback. And basically this loop can continue until some stopping criteria are satisfied and then it goes to the next step of the workflow. This evaluator optimizer pattern is fundamentally different than all the other patterns that we're seeing here. And the difference is that everything else has a clear beginning and end. It starts at point A and it ends at point B and you know even if you're splitting it up to operate in parallel there's a clear end point. However, because the evaluator optimizer is this loop this could you know in principle just go on indefinitely. So inherently this system is capable of doing tasks with more autonomy with more agency than the other ones because we haven't created these constraints or rails for the system to operate on. It basically figures out what it wants to do by itself. This brings us to the level three example which I call an LLM in a loop. The idea here is giving an LLM real world feedback on responses until they are satisfactory. So to give a concrete example, let's say I wanted to have an LLM system, an AENTIC system write LinkedIn posts for me. So I might have an LLM writer. It's going to write a LinkedIn post for me. And then what I'm going to do is pass this post through various sets of evaluations, you know. So, the first thing I'm going to do is see if that post is similar to my existing posts because I want it to sound like me and I want it to be relevant to the other things that I've talked about. Another evaluation I might do is pass it through a AI post detection system. That's because I don't want the post to sound like an AI wrote it. I want it to sound humanlike. And then finally, I can evaluate if the LinkedIn post is actually similar to the initial idea that I fed into this LLM system. And so then each of these evaluations, we'll have like a pass fail. And then we can have some logic where if any of these evaluations failed, we'll gather all the feedback and we'll pass it back to the LLM writer. We might say, okay, the post was relevant to the idea, but it sounded AI generated. you know, like 80% was AI generated and it didn't really match the voice of my existing posts. So, we kind of gather all this feedback, give back to the LLM, and then the LLM can go, ""Oh, okay. So, I got this feedback. Here's the post I just wrote. Let me update it and write another one."" And then that'll spit out another LinkedIn post. It'll go through the same evaluations. And then again, if any of them fail, we'll gather the feedback, give it to the LLM, and continue in this loop like this. And so the value of these systems is that they can perform very open-ended tasks. Let's say if I wanted an LLM to write a LinkedIn post for me, I might have to optimize the prompt for a very specific idea or very specific set of ideas for it to perform well. While it might work well on those examples, it's going to be fundamentally limited because, you know, maybe it has to follow a certain template or maybe it needs like a reference document to write the post from and all these other constraints. But doing it in this way where we're just giving the model feedback, evaluating it based on these outcomes that I care about, it can potentially figure out the best post to write through this closed loop. And then if all the tests pass, we can return the post. And so there's actually another version of this. As I'm showing the example here, we are just using the LLM out of the box. And then we're just getting very clever on how we define these evaluations to design this system. However, we could also do something similar where we actually update the internal parameters of the LLM. In other words, we fine-tune the LLM to be optimized in solving this task. So, the model can get better and better the more posts that it writes. This is the idea of using end-to-end reinforcement learning on realworld feedback in optimizing an LLM system. The only difference here is that we make the parameters trainable and then we use an algorithm like group relative policy optimization to update the internal parameters. And if you don't know what I'm talking about with reinforcement learning or group relative policy optimization, I talked about that in a previous video on reasoning models like 01 and R1. While this might sound like some kind of research project or something like that, this is actually the approach behind models like deep research which is currently available in chatbt and also openai's operator model which is only available to prousers but basically that model is capable of surfing the web interacting with guies and doing things on your behalf. Okay, so this was actually the first video in a larger series on AI agents. So here the discussion was relatively high level. We didn't go into the technical details or write any code. That's because I just wanted to lay the foundation for future videos where I'm going to talk through each of these one by one and build concrete examples of each of them. And so in the next video I'll be building a level one agentic system. I haven't decided what I'm going to build yet. So if you have any suggestions let me know in the comment section below. Also if there's anything else you want me to cover in this series let me know in the comments. The AI agent space is massive and it's only growing and I'm sure there's so many tools and ideas out there that I'm completely ignorant of. And so if there's anything you keep seeing over and over again and you think I should cover, let me know. And with that, as always, thank you so much for your time and thanks for watching."
Dke017sUIkE,2025-03-24T12:00:19.000000,"My First $10,000 Month (as an entrepreneur)","18 months ago, I quit my corporate data science job to become an entrepreneur. Despite my hilarious expectations of matching my full-time salary within 6 months, it took me 15 months of failure and volatility to finally do this. In this video, I'll share my realistic journey to my first $10,000 month as an entrepreneur. In no way is this meant to be a flex. Rather, it's meant to be a helpful baseline for those who are considering a similar path. What makes entrepreneurship hard is not coming up with ideas or learning new skills. It's that the road ahead is uncertain. So, by sharing my story, I hope to make the path a little more clear and a little less painful for those interested in something similar. So, let's get into it. In July 2023, I quit my dream job. It checked all the right boxes. six-f figureure salary, prestigious company, fun co-workers and so on. However, in that role, I realized that the security and comfort of corporate comes at the cost of freedom and the growth that comes with discomfort. So, with about one year of runway in savings, I quit my job to pursue entrepreneurship full-time. My master plan when leaving corporate was to sell data science services and make content about it. To my surprise, it seemed to work. Within 3 months, I got my first client. My content channels were growing exponentially, and it seemed like everything was going to be all right until I hit the six-month mark. Despite my early success, my first client was short-lived, and my content growth began to wayne. Looking back, my biggest mistake in those early days was lack of focus. I was creating content, taking discovery calls, doing client work, and entertaining any revenue generating opportunity that came in my inbox. The problem with this is that I was trying to move in three different directions simultaneously and ultimately didn't make enough progress in any of them. If I could go back now, I would put all of my focus into content creation at day one. That's because it was giving me the most return on my time investment. So that's what I tried next. I realized the importance of focus at the start of 2024. That's when I decided to stop taking on clients and focus on content creation. From April 2024 to June 2024, I doubled down on content. My thinking was since most of my revenue came from content, if I doubled my output, I would also double my revenue. And this kind of worked. My revenue went up from about $2,000 a month to $3,000 a month. However, much of this increase wasn't directly from content. Rather, it was from an increase in paid calls because I had gotten rid of my free discovery call option. A major lesson during this period was learning how content creators actually make money. This is either by one selling other people's stuff or two selling your own stuff. I didn't have a good understanding of this, so I basically made content and generated revenue by accident. I have no doubt that if I would have grinded content for one year, I would have made enough money to survive from it, even by accident. However, at this point, I didn't have the luxury of time. I only had 4 months of runway left in my savings. So, this is when I realized I needed something to sell. In July 2024, I led a private AI workshop at a large enterprise. Not only was this fun for me, but it was the most amount of money I made on a perime basis. I charged $1,500 an hour for the session, which was five times as much as my consulting rate. That's when something clicked. People seemed to like how I explained things. Not just enough to consume my free content, but also enough to pay money for it. Therefore, AI education seemed like the way forward. So, over the next three months, I explored several AI education offers. This included private AI workshops for companies, 90-minute BTOC workshops, 2-day BTOC workshops, white labeled B2B workshops, and a 6-week AI mentorship program. Despite reaching out to hundreds of people, promoting these on LinkedIn, and making landing pages, I still had no paying customers. In October 2024, I was on my last 2 months of savings. Although I hadn't found a promising offer yet, I had learned a ton from the failures of my past iterations. One of my key learnings was that given all the free AI content that's out there, any offer needed to be significantly differentiated. To me, that meant the offer needed to one, be a high ticket one, and two, include live sessions. This is when I decided to try out Maven, an e-learning platform that specializes in cohort-based courses. In just one day, I threw together a landing page for a six-w weekek live course called the AI Builders Boot Camp, priced at $550. Around this time, I was reflecting on Apple's old mantra of real artists steal, conveying the idea that you don't need to reinvent the wheel. See what's already working out there and copy it. This was one of the reasons why I thought this offer could work. Many others already had high ticket AI courses on Maven and elsewhere. This also led me to write a blog called five AI projects you can build this weekend inspired by various popular YouTube videos sharing AI project ideas. This blog turned out to be my most read article of all time. And lucky for me, I had linked my landing page at the bottom of this post where people could sign up for my weight list. The Maven landing page and the viral blog post coalesed into over 200 weightless signups in the first week. However, a lesson that I had learned many times over the past 15 months was that it's not real until the money's in the bank. So, it was now time for the moment of truth. Asking people to buy. In the first week, 16 people enrolled in the course. In the second week, another 15 people enrolled. By the end of this, 45 people signed up for this first cohort, earning me $11,760. The reason this number wasn't much bigger is that many people enrolled with a 50% founders discount and Maven then took 10%. The significance of making $10,000 in one month is that was what I made at my full-time job. To me, this was proof that I could make enough money to survive while still having complete freedom over my time and my attention. Here's a breakdown of my revenue in that first 10K month. My AI builder boot camp generated $9,26548. My medium blog earned me $89452. My YouTube revenue was $542.73. Finally, I made $36321 from paid calls, coming to a grand total of $10,826.72. It took me 15 months of failure, hard lessons, and volatility to finally match my full-time income. Looking back, there were four key lessons I learned over this period, and I wanted to share them here in case they can save you some time and pain. Number one, it's not real until the money's in the bank. I can't tell you the number of times I got really excited about a deal going through or an idea working out just for it to fall flat. This emotional roller coaster can really drain you. So, it's best to avoid it and only get excited, if at all, when the money hits your bank account. Second was shifting my mindset from that of an employee to an entrepreneur. What I mean by that is that when I quit my job, I was asking the wrong question. I was asking myself what work can I do or what services can I provide to make a living. However, what I should have been asking was what problems can I solve? By not focusing on the value that I was generating for other people, I ended up just working really, really hard to just make a couple thousand every month. In other words, when you're just getting started, don't focus on your inputs. Start from the outcomes you want to generate for your customers and then work backwards on what inputs you need to do in order to make that happen. Number three was to focus more. Every time I got more specific on the work that I was doing, the problem that I was solving, and who I was solving it for, my life got easier, and I ended up making two to three times as much. In fact, this is still something I'm actively trying to improve upon. Fourth, and finally, real artists steal. Don't try and reinvent the wheel. See what's working for others and adapt it to your situation at every opportunity. So, I hope these lessons help shortcut your journey. Of course, there were many details I couldn't fit into this short video. So, if you have any specific questions, let me know in the comments. And as always, thank you so much for your time and thanks for watching."
-sL7QzDFW-4,2025-03-17T12:00:01.000000,How to Evaluate (and Improve) Your LLM Apps,hey everyone I'm Shaw in this video I'm going to talk about how we can evaluate large language model applications I'll start by discussing the limitations of the typical Vibe check approach that most people use when building with llms then I'll review how we can do better than this through three types of evales and finally I'll walk through a concrete example of how I used EV vales to systematically improve a YouTube to blog conversion tool so jumping right into it typically when building any kind of application that involves a large language model the first and most common way to check if the prompt that you're passing to your llm is any good is by doing a so-called Vibe check essentially this is where you evaluate the performance of a prompt by skimming its corresponding outputs one example at a time for instance if I wanted GPT to write a YouTube title for me I might start by asking it this prompt here write me a YouTube title for a video on LL M EV vals then basically I would see if I can improve this prompt by skimming the output so this is what GPT 40 mini said mastering llm evaluations key metrics and best practices this is kind of like a typical response you'll get from a large language model for whatever reason they love to start titles with like mastering or unlocking or something like that on seeing this I might go back to my prompt and add something like don't start with vague words like unlocking or mastering then we see that it gets a little better so it says how to effectively evaluate large language models key metrics and methods so I would say this is a bit better but it's a little long typically you want your titles on YouTube to be short so people can actually read it when they're scrolling on the homepage or something like that so upon seeing this I might do yet another iteration where I add make sure it's no longer than 65 characters shorter is better and then we get this title here how to evaluate llms for real world applications so this is a pretty decent title and only took us like three prompts to get here you can get pretty decent results through the vibe check just through a few simple iterations some of the pros of this is that it helps you move fast it took me about 2 minutes to just iterate through a few examples and then finally arrive at a title that is decent and this is a great way to go from zero to one so a lot of times when you have an idea there's a lot of benefit in just building an initial prototype of your system just like as a Baseline and you'll typically just learn so much going through a single full iteration of the build so it's kind of ironic you actually will end up building your final product faster if you basically just build two versions of it like an initial prototype that's just fast and dirty and it's supposed to be Scrappy and then the final version of it as opposed to trying to get that final version in one shot but of course there are downsides to the vibe check so one it's kind of a pain if you're trying to compare multiple designs like for example trying to compare multiple models you're trying to compare different temperature choices you're trying to compare comp different structures and formats for your prompt maybe you have a handful of different F shot examples that you want to incorporate in the prompt easily all these different combinations is going to be overwhelming to Just Vibe check your way through and ultimately this leaves performance on the table since you're not able to explore so many different options and you're not systematically evaluating them with a clear metric it's hard to really optimize the system so this is the motivation of evals which are simply numbers that correlate with the outcome you care about for your system for example let's say we have these three different prompt options and they all have a corresponding output from a particular model some metrics that might help me distinguish which prompts are better than others are things like the length so here I personally prefer shorter titles specifically titles shorter than 65 characters and maybe like 50 to 55 is the sweet spot so I'm going to try to look for titles that are within that range this is something you can just easily check you just count the number of characters in each title another thing we might want to optimize for is the CTR so the click-through rate of each of these titles you could definitely do these various different live experiments from real human feedback alternatively you could develop some kind of model that is trained on YouTube titles and a corresponding ctrs then you can try to have like an estimated CTR for each of these titles But ultimately CTR is something that we care about because you know on YouTube you want people to click on your videos and you want people to watch them and they can only watch them if they click on them so you want CTR to be high additionally maybe there's an expert grade as you spend more time on YouTube you start to learn what titles work and what titles don't work so maybe I can have some kind of YouTube title expert review these and tell me which titles are good which titles are bad based on some guidelines or framework that they've defined but the key thing here is that each of these gives us a different view in the performance of these prompts and it makes what was initially just like a Vibe check into something that's quantifiable and thus something that's easy to improve clearly here higher CTR is better we want it to pass the expert grade and we want the titles to be within that 50 to 55 sweet spot so this is objectively the best prompt option of these three so here I'm going to focus on three types of evals of course this is not going to be a comprehensive overview there's so much breadth to EV vales and even within each of these types there's so much depth so there's realistically way too much for me to cover in a single video but my goal here is to just give you like a flavor and a survey of some different ways that this can look to give you a basic idea and then we'll make this a bit more concrete with a real world example near the end the three types I'm going to talk about are code-based EV vals human-based evals and llm based evals so starting with the code-based ones these these are simply rule-based assessments of a model's output often these are the easiest evals to both Implement and interpret for example let's say you are working with some kind of closed ended task a task where there is a clear right answer this might be something like having the llm do math the input question is what is 2 plus 2 and there's a clear right answer and the llm says four we can simply just check if the right answer is in the llm response so this is just something you can write with code you don't need any kind of more sophisticated process for evaluating it but of course it was like a more difficult problem like this LM might see something like this and this is not the right answer it's just off by one digit so beyond math other types of tasks that you can do these kinds of evals on are like coding challenges or logic puzzles these types of tasks were famously used to train these modern advanced reasoning models like 01 and deep seek R1 one so having this explicit clear right answer makes it relatively easy to evaluate the model's output additionally this is applicable to multiple choice questions like something where there a fixed number of options and the model has to pick one so we can just compare the model's answer to the ground truth but a lot of times the tests that we want to do with large language models don't have a clear right answer but we can still do this idea of doing these string checks or these regex checks of the output to help us distinguish bad responses from potentially good ones so we already talked about checking the number of characters and YouTube titles another thing we could do is we could check if the first word in the title is like unlocking or mastering or understanding some suffix ending with in this is something we can simply check with a regular expression and then checking the number of characters in a string is super simple you just check the length of the Python string even though these are loosely correlated to good titles they can still give us some insight on what system designs are better than others so applying these to the three responses we saw earlier we see that the first and the third one pass this first check of being less than 65 characters and then we see that the last two ones passed this check of not starting with x so this agrees with our Vibe check that this is the best title out of these three another thing we can use here are traditional NLP metrics so these are things like the blue score and the Rogue score which have been around for a while and they're widely used so the blue score is typically used for translation Rogue scores typically used for summarization these are just comparisons between two pieces of text to see how similar they are so they're relatively easy to calculate however the obvious downside here is that you know maybe we're not doing a translation task or we're not doing a summarization task so these are quite limited and even Within These domains they may not entirely correlate with with the specific task that we're doing so for example we'll see later that I used Rogue score in evaluating the performance of this video transcript to blog post conversion tool and we'll see that the Rogue score doesn't fully correlate with the thing we're trying to optimize for even though these code-based evals are going to be very use case specific and a lot of times you're just going to write a custom function to do a specific check based on business logic or domain expertise for your specific product but hopefully these different examples gave you some ideas of what this might look like for you the next type is human-based evals so this involves using human judgment or behavior to assess the quality of an output this is very much the gold standard especially for a lot of open-ended tasks that involve taste like writing YouTube titles or blog posts or maybe require empathy like some kind of AI doctor or AI therapist these things are going to be difficult to assess with code-based evals if it's even possible that's where having these human-based evals is valuable here I'm going to talk about two general types of human evals first Ty I'll call preference rankings so this is basically let's say you have prompt a prompt B you're going to pass them into an llm they're going to generate two responses and basically what you have is a human expert whether it's a domain expert or some kind of end user you just have them say which of the two responses is better this is exactly what open and AI did when they created instruct GPT which was the model behind the initial version of chat GPT what they did there is they hired hundreds or even thousands of contractors to manually review responses from an llm and grade which ones are better so using these preference rankings you can easily tell which of these two prompts is better so basically you do this head-to-head comparison hundreds or even thousands of times and then you can just see which of the two prompts or system designs is preferred most often but a downside here is that using these preference rankings you only get a relative sense of which prompt is better it's not a big deal for prompt a versus prompt B but if you had like prompt a versus B versus C versus D and so on and so forth like you had dozens of different options you'd have to do many of these head-to-head comparisons and then the analysis of that is going to get a little complicated because you can also have like a beats b b beats C but then C beats a because they're just preferred for different reasons cuz there may be multiple Dimensions that humans are evaluating these responses so these preference rankings only give us a relative sense of performance not a global or absolute sense for that we can take a different approach and use a expert grader so here instead of comparing two prompts and responses head-to-head we can just have a single prompt pass it to the llm get a single response and then have an expert evaluator sit down and give the response a past fail score based on some guidelines and then explic explicitly write out their reasoning this is a process that's described in reference number four which is a really nice blog post by haml Hussein who has a lot of experience deploying real world llm systems for his clients so the upside here is that now you have a global sense of the responses quality you don't need to do a head-to-head comparison to see if one prompt is better than another this simplifies the analysis like if you have dozens or hundreds of different prompt and system design options but perhaps most important what happens during this process inevitably is that the human expert the evaluator is forced to explicitly Define what is a good response and what is not a good response this turns out to be very much an iterative process where more responses make the evaluator realize that their guidelines need to be updated and made more clear but then eventually the guidelines become solid and then you have a pretty good system for evaluating these responses although human-based evals are super flexible and they can evaluate basically any aspect of a llm system the clear downside is that this isn't very scalable it's very timeconsuming for someone to sit down and review responses and give them grades it's a little faster to do these head-to-head comparison but still takes time and attention which is why a lot of times in practice humans are only a part of the evaluation pipeline the real goal of getting this human evaluation whether it's preference rankings or expert grader is that they're used to enable llm based evals so basically an llm based eval is using an llm instead of a human to evaluate the output quality of a model and so we can go back to the same two approaches as before we have this preference ranking approach and this expert greater approach but instead of having a human do this we just replace that with an llm judge so this approach is also called llm as a judge and this gives us the flexibility of human based evals while also making that process very scalable so for a expert human grading 50 responses might take a few hours in that same time an llm judge could evaluate a hundred or a thousand times as many responses but a key step here which is not trivial is to align the lm's judgments to human preferences to the human evaluators a lot of times this in of itself becomes like a whole project and undertaking because there are a few problems with the llm judges as we'll talk about in the case study in the next slide but one of them is that llms might prefer responses from themselves essentially so GPT 40 might prefer GPT 40 responses to Claud responses even if the human judgment preferred Claud in that instance so a lot of work goes into getting the llm to actually agree with humans this is typically just done through prompt engineering so iterative ful updating the prompt until you have decent alignment like 90% overlap or you could also train a model to do this or F tuna model to do this training a model is what open AI did when they created instruct GPT so they created this reward model which basically simulated human preferences and then they were able to use that to do reinforcement learning with human feedback and ultimately make instruct GPT and if you don't know what that means I talked about it in a previous video which I'll link here so to make this all more concrete let's walk through a example a few months ago I shipped this product web app called y to be which converts YouTube videos into blog posts and so even though this is something that I personally get value from because I built the system and I know what to expect and it's easy for me to edit blogs just cuz I've written so many at this point and I know what I want and you know what I'm looking for some feedback I got from a kind user genuinely kind because most users don't give you feedback so you don't know if what you built is any good so genuinely kind and they said that it didn't get me close enough to how I wanted to write the article for it to be worth it it would have been easier for me to just start from scratch so this is a great example of vibe checks not leading to Optimal Performance so I used Vibe checks to write the prompt for this tool here which resulted in it working most of the time for me but on the first try for this user it didn't work which means that there's probably not going to be a second try so if a user tries your product for the first time and it doesn't give them a good outcome they're probably not going to try it a second time unless you kind of do some additional messaging and say hey I updated the product would love to get your feedback second time and then maybe they're going to try it again so I saw this as a great opportunity to try to systematically improve the prompt that I used to do these conversions using evals and so my process is listed here so first I created a set of Baseline blogs and YouTube videos maybe you've noticed that all my YouTube videos have a corresponding blog post so I actually have a lot of data that I can use to evaluate this kind of system I can just take all my videos and extract their transcripts and generate blogs for them and then compare those AI generated blogs to the actual ones that I wrote myself so the first step of this process is just curating the set of Baseline blogs and YouTube videos next I extracted the transcript from each of the videos in this Baseline step three is that I wrote an initial prompt a starting point to generate blogs from the transcripts step four was the Align M process so I aligned the llm judge to my preferences which was basically preferring the human generated blogs to the AI generated ones in this head-to-head comparison and then step five was to iterate on the prompt I wrote In step three to systematically improve it based on the win rate against the human generated blogs so I'm going to walk through each of these steps and additionally all the code I used here is freely available at this GitHub repo shown here and Linked In the description step one curating the Baseline blogs what I did here is I picked out my top 30 blogs according to earnings on medium I'm just using earnings as a proxy for blog quality and the reason that might be a good metric is that the earnings are based on not just the number of views that the blog gets but how much time people spend reading the article so it's kind of a more holistic a more robust measure compared to just looking at views or reads alone so I just manually copy pasted all the titles of my top 30 blogs by earnings and then medium allows you to download all your posts on the platform they'll just email it to you in a zip file so I downloaded that zip file and then finally I had Claude write me some code to pick out the top 30 blogs from this massive extract and to convert them into markdown so the final result are these 30 blog posts these random characters here are actually the video ID corresponding to that blog post and MD is just for markdown the next step is now that we have the Baseline blogs the next thing I'm going to do is extract the YouTube transcripts for each of them so I also just manually copy pasted all 30 video IDs into a text file and then I ran this python script here and it just read each line of this text file and then used the YouTube transcript API python library to grab all the transcripts and write them each to a separate text file final result of that is now we have 30 transcripts saved in my codebase now that we have the human generated blogs and the corresponding video transcripts now we can use the transcript to create a AI generated block and so I had Claude write me a prompt for GPD 40 mini so this is the first part of it it wouldn't all fit on one page but the full prompt template is on GitHub and I also share it in the blog post for this video but basically I just take this initial prompt I didn't really spend much time optimizing it I just wanted a starting point and I used that to generate 30 blogs based on the transcript so these are now saved in a separate folder but they have the same names as the human generated blogs so now step four is probably where I spent most of the time on this project here I'm going to use an llm judge to evaluate the quality of the AI generated blogs but before I can do that I need to make sure the L judge is actually aligned with my personal preferences so to test that what I did is I took a real blog and then a corresponding AI generated blog and then I passed that into the prompt template for the llm judge so this is a different prompt than the one used to generate the blogs this prompt is used to evaluate which of two input blog posts is better here we're using that preference ranking approach so a key Point here is that you're going to need to iterate on this llm judge prompt until it aligns with human judgment so the quote unquote correct way to do this would be for me to manually read all 30 real blogs and all 30 AI generated blogs and for me to define a ranking but that would be very timec consuming and probably unnecessary cuz I can confidently say that I'm going to prefer the real blogs that I personally wrote to these initial AI generated blogs so basically the human eval data in this case is just going to always prefer the real blog to the AI generated one after like three or four iterations for this llm judge I was able to get this llm judge to prefer the real blog 92% of the time but a another caveat here which is quite interesting is that there's a phenomenon with these llm judges when you're having to do preference ranking called position bias basically when I did this the llm judge was always referring option number one which at first if I wasn't careful would make it seem like Oh The Prompt template is working great let's move on to the next step but what I did once I saw that really good performance is I switched the order so instead of putting the real blog in option one and the ai1 in option two I flipped it and then I saw the preference ranking stayed the same so basically it still preferred option one to option two even though option one were now the AI generated blocks this is something that seems to happen a lot lot and they talk about it in references 3 and six three is a review paper on evals 6 is a YouTube lecture about them so to mitigate this I actually ended up doing two things first the final version of The Prompt explicitly instructed the model to evaluate the blogs separately and to compute some kind of score for them this reduced this position bias but it didn't eliminate it so I also just ran the eval both ways I did it one way where the real blog was in position a and then for the second run the real blog was in position two and these were two different numbers so the win rate for the real blog was different depending on whether it was in position one or position two but then what I did is I just averaged these two win rates together to get the final evaluation metric but eventually I got the llm judge to a good enough point so I moved on to step five which is where I used the llm judge to guide updates to the blog generation prompt to improve these AI generated blog posts the final prompt looks like this this is a prompt for gbt 40 mini while the judge llm is gbd4 and so here I did four iterations and the results of that are shown here so this column is the iteration number for The Prompt this column here is showing the win rate against the human Baseline with each new prompt I generated 30 AI generated blogs and then I compared those AI generated blogs head-to-head to the corresponding real ones that I personally wrote and then I used the win rate of the AI generated blogs against the real ones as the eval metric so we can see with the first prompt the AI generated blogs only won 8% of the time in this head-to-head comparison but then that jumped up to 133% of the time on the second iteration and then went up even more to 23% on the third iteration so we can see that the prompts getting better and better but then it drops down in iteration number before and here I just made some like manual tweaks to The Prompt based on how at least I think I personally write blogs but that led to a significant decrease in performance which is kind of interesting but also for comparison I computed the Rogue L F1 score for each of these AI generated blogs the Rogue score basically computes the similarity between a reference text and a Target text so in this case the reference text are the YouTube video transcripts and the target texts are the AI generated blogs and the Rogue lcore is basically a measure of similarity between the two so this is the NLP metric a code based eval we can see that it actually tracks with this human win rate so it's getting better and better until prompt number four where even though the Rogue score goes up the win rate plummets this might be telling us that the Rogue score doesn't fully correlate with what we ultimately want to optimize for or of course it could be telling us that the win rate sucks and we should listen to the Rogue score so this was my first pass at this I'm sure there's a lot of opportunities for improvement so if you have any suggestions let me know in the comments okay well let's talk about what's next so this was my first time playing around with llm evals I was very much a Vibe checker for the past 2 years but now I'm becoming more systematic you may have noticed that I didn't use any Frameworks or libraries to do these evals I just wrote everything from scratch and that's generally my Approach for a lot of this stuff because it's going to be so use case specific and so custom that a lot of times the existing tools just may not be able to do specifically what you want to do when it comes to evals and then you're going to have to write custom code anyway so basically the startup cost of learning a new tool or framework isn't usually worth it for me so I tend to delay learning new Tools in libraries but a couple of them have come up a few times which has made me more Curious and made me actually consider learning about them and making videos about them so the first one is is open AI they just put out a beta for their evals platform which looks pretty nice so I can do a video on this if people are curious also lsmith seems to have a pretty nice interface for doing eval so I could also do a video on link Smith if there's interest or if you guys have any eval Frameworks or libraries that you love and they don't have this issue that I'm saying that they're too rigid and don't allow you to do custom things let me know I'm new to the stuff so I'm ignor and just let me know what you guys have found works for you and I'll be happy to take a look at it and finally there is a Blog of course for this video and even though this is a member only story you can access it completely for free using the friend Link in the description below and with that as always thank you so much for your time and thanks for watching
-AA0xw5xeQU,2025-03-06T13:00:00.000000,5 AI Projects For People in a Hurry (w/ Python),the best way to develop your AI skills is by building projects however for busy professionals and leaders finding time to actually sit down and write code is challenging in this video I'm going to share five AI projects you can build in an hour or less at three different levels of sophistication for each idea I'll discuss its key steps and then share example code in the description and if you don't already I highly encourage you to use coding as assistant like chat GPT or cursor to help you build faster and debug issues as they come up the first project is a YouTube to blog converter content creation is a big part of what I do as an entrepreneur while this is one of the best ways solo Founders can let customers know about their product or service it is still a timeconsuming process one automation that is help me write blogs faster is translating YouTube transcripts into blog posts using GPT 40 while this doesn't automate the entire content creation process it saves me hours by providing me an initial draft here are the key steps first extract the YouTube video ID from the video link using regex Second Use the video ID to extract the transcript using the YouTube transcript API python Library third write a prompt to repurpose the transcript into a blog post and then finally send the prompt to to GPT using open ai's python API while you could simply just ask chat GPT to implement each of these four steps for you I also share some example code for this in the description although chat gbt is a great tool for quick document question answering this is not suitable for documents containing sensitive information this is because data uploaded to chbt can be used in Future model training one solution to this is to set up an llm chat system on your local machine here's a simple way to do that with ol llama first download ol llama from their website second extract the text from the PDF using the P muw PDF python Library third pull in your favored llm using olama four write a system message that includes all the text from the PDF five pass user messages to the llm about the document and then as a bonus create a simple chat UI using gradio if you're struggling with the UI I've actually implemented a visual QA chatbot using gradio and I linked that example code in the description sometimes you don't just want to find information in a single PDF but rather information dispersed across a range of PDFs this is where retrieval augmented generation I.E rag is helpful rag automatically retrieves context relevant to a user's query before passing it to an llm here's a simple way to do that with llama index first read and store documents into a vector database second create a search function to retrieve the relevant context given a user's query third construct a prompt template which will combine the user query and the relevant context before passing it to an llm fourth orchestrate these components into a simple chain I.E the query retrieval prompt then llm finally finally you can now pass a query into the system and get an answer from the llm again as a bonus you can use gradio to create a simple chat UI for this application I actually walked through this project in a previous video on rag so I'll link that up on the screen here if you want to learn more Additionally you can refer to the example code I shared in Project 2 for the gradio UI a cenal component of rag is the vector database which enables the retrieval process more specific speically these vectors are so-called text embeddings which are numerical representations of a chunks semantic meaning however rag is not the only use case for embeddings these vectors can be combined with traditional machine learning approaches to unlock countless applications I recently used this to analyze and summarize open-ended survey responses from my customers here are the key steps that I followed first load in the survey data with pandas second encode the text responses using the sentence Transformers python Library third cluster the responses based on similarity using K means fourth write a prompt to summarize the responses in a given cluster and then finally summarize the responses for a given cluster using GPT 40 mini my full implementation of this project is linked in the description below a popular AI application is using it for content marketing doing this in practice however is a bit tricky since one the content sounds AI generated and two search engines and social media platforms are actively checking and suppressing AI generated content one way we can mitigate these issues is by fine-tuning an llm on one's unique voice and style I recently used open AI fine-tuning API to train GPT 40 mini on my LinkedIn posts here's how you can do that too first create a CSV file containing 10 to 50 idea post pairs second construct a prompt template which combines the response instructions with the LinkedIn post idea third generate input output Pairs and store them as a list of dictionaries fourth split the data set into a training and validation set five save the data in the Json L4 format expected by open ai's fine-tuning API six upload the Json L files to open aai seven create the fine-tuning job using open ai's python API and then finally compare the fine-tuned models responses to GPT 40 the biggest effort in this project is curating the training data in Step One however everything else should take you less than 15 minutes especially if you steal my example code in the description I hope at least one of these projects helps jumpstart your AI development Journey again example code and additional resources are linked down in the description below and as always thank you so much for your time and thanks for watching
bbVoDXoPrPM,2025-03-03T13:00:58.000000,Fine-tuning LLMs on Human Feedback (RLHF + DPO),"hey everyone I'm Shaw here I'm going to talk about fine-tuning llms on human feedback I'll start by explaining why we might want to do this and then discuss two popular techniques reinforcement learning with human feedback and direct policy optimization finally at the end I'll walk through a concrete example of fine-tuning a model to write YouTube titles based on my personal taste and preferences the whole motivation behind fine-tuning large language models is that although base models and these Foundation models are very powerful getting them to solve problems in a practical way is not always a straightforward thing in other words using them isn't always so intuitive and just to demonstrate this we can look at a concrete example consider llama 3.1 so this model has great performance across these benchmarks it was trained on 15 trillion tokens it has a tremendous amount of knowledge and can solve a wide variety of different tasks however if if you wanted to use this base model for something like learning how to fine-tune a language model and you just type in this input tell me how to fine tuna model you might get a response that looks something like this just reading this I am not sure about how to do this I have a model for a small area and I have a model for a large area I want to have a model for a large area that is as accurate as for a small area and on and on and on so the response isn't very helpful and is not really intuitive for people to interact with in order order to get one of these base models and Foundation models to actually give us a helpful answer we really have to do a lot of work on the prompt engineering side so instead I might construct this artificial blog post or something like that where I have a title that says intro to fine tuning llms fine-tuning is where we adapt a model to a particular use case through additional training here are the basic steps and then with this more structured and additional context the model actually generates a pretty helpful response so one gather a data set that represents the types of data you want your model to be able to handle choose a base model to fine-tune this is usually a pre-trained model that has been trained on a large data set three use the data set to finetune the base model so on and so forth so really to get base models to do what we want we have to like almost trick them into doing particular tasks like helping us fine-tune a language model so this was the whole motivation behind instruct GPT which was essentially the model behind the original version of chat GPT so this instruct GPT paper reference number two came out in 2022 and the whole thing was this process that open AI used to take their base model gpt3 and align it with human preferences just to see some examples of that so if we take that same prompt that we gave llama 3 and we pass into gpt3 we might get a response that looks something like this so if we say tell me how to find tuna model it'll just start listing questions maybe kind of like a Google search or like a FAQ section on a web page or something like that however after this find tuning process to create instruct GPT this is how the model might respond to this same input so the response is fine tuning a model involves adjusting the parameters of a pre-trained model in order to make it better suited for a given task here are generally three steps for fine-tuning a model and then it starts giving us these steps fundamentally gpt3 is doing what it was trained to do it was trained to predict text on the internet so it's doing a great job at that but that isn't necessarily something that is helpful to us on the other hand unstruck GPT was built on top of gpt3 and is giving us a response that's more helpful the way open AI pulled this off was through this threep training process which at this point is quite well known but I'll just recap it here so the first step is the pre-training so this is creating gpt3 or creating llama 3.1 405b base typically you're not going to do this yourself this is things that AI Labs will do and so you can just pick your favorite AI lab and take their pre-trained model take their Foundation model as a starting point step two is this supervised fine-tuning step and so here open AI took chat conversations like these input output pairs so like a query from a human and then the ideal response from a human labeler and so they curated this data set with thousands of input output Pairs and they taught gpt3 how to have a conversation like a chatbot however just learning how to have dialogue is still not enough because the model still may not answer answer prompts in the most helpful way also since this model was trained on wild internet data there's all sorts of toxic unhelpful just wild things you'll find on the internet and that stuff can make its way into the responses of the model so there was this third step which was this reinforcement learning from Human feedback rlf and so this led to a significant Improvement in the models responses as evaluated by human preferences Not only was the model after step three preferred to gpt3 but it was also preferred to this supervised fine-tuning in the instruct GPT paper they have a nice plot showing a comparison of these three models so that's Linked In the description if you want to check that out but here I'm going to just focus on this step three this reinforcement learning with human feedback step so the key difference between steps two and three from the previous slide is that step three uses reinforcement learning instead of supervised learning so I actually talked about reinforcement learning in my previous video where I discussed how deep seek and open AI used it to create their recent Advanced reasoning models but the basic intuition behind reinforcement learning is that it's a way for models to learn by trial and error so if we wanted to train an llm on human preferences using reinforcement learning it might look something like this where we take a prompt we pass it to a large language model and we have it generate a response and then what we do is we have a human look at the response and say whether they like did or they didn't like it and then through reinforcement learning we can take this feedback and update the model parameters pass in a new prompt generate a response and just do this feedback loop over and over again so this is very much how humans learn or animals learn so they do some behavior and they get positive reinforcement or negative reinforcement and that'll update their future Behavior that's essentially the same idea that we're doing here with a machine learning model however this is not a very scalable process because if you have to do thousands of training steps and you have a human in the loop here and it takes a human like a few minutes to make the Judgment of whether it's a good response or a bad response this is just going to be a very slow process so the key trick that open AI used in R lhf is that they replaced the human with this so-called reward model which was trained itself on human preferences we can think of this reward model as a proxy for human labelers for human preferences and so the way they created that is they took a prompt they passed it to a large language model so in this case it was that checkpoint after step two in that three step training Pipeline and then they would generate multiple responses to the same prompt then what they would do is they would take these responses give them to a human labeler and these labelers had these very detailed guidelines on what was a good response and what was a bad response and they simply just ranked them from best to worst so the key reason why you would want to do this ranking approach as opposed to just humans saying if the response was good or bad is that if we just stop and think about it if we were just given a response from a language model and we were told to rate it on a scale of like 1 to 10 or like 0 to one or something like that like give it a precise score that wouldn't be a trivial task to do it would probably take us maybe 5 15 minutes to really stop and think about it so this would just be a slow process and even at the end of it there's probably going to be a lot of variance is this response a 0.7 or is it a 0.75 or 0.85 there's not really a true right answer however simply just doing relative preferences just ranking the responses is a much easier task so humans can do it much faster which allows you to have a lot more training data and so you can take these rankings and then you can create these pairwise preference data sets so you can basically say we have response 2 and response three and response 2 is preferred so we can take responses and use that to train a reward model basically to predict which of the two responses was preferred if we just take these first two will pass in like response two and response three and the reward model will learn to identify response two is better than response 3 and then response three is better than response one through this training process the reward model will learn how to quantify the quality of a given response and so just to give you some real numbers here for instruct GPT they use used about 30,000 prompts here and then for each prompt they generated four to n different responses and then they had these ranked but the question is you know how does this work mathematically so we have this reward model that is able to generate a score of the quality of a given response but how can we use this number to actually update the model parameters openi used this reinforcement learning approach called proximal policy optimization and while there are a lot of details here the basic thing to know is that PP is an efficient way to update parameters based on rewards here's the objective function for Po and if you don't fully understand what's going on here that's fine this is for people who want to see the technical details but if you saw the previous video where I talked about grpo group relative policy optimization so that's what deep seek used in their reinforcement learning approach to train deep seek R1 you'll notice that this objective is actually very similar to what we saw there minus a additional regularization term that's because grpo was actually based on PO but the two key features of PO is that it uses a so-called surate objective which is basically using this probability ratio times the advantage function and it also does this clipping process so the surrogate objective just makes training more efficient and then the clipping makes training more stable since I spent like 10 minutes talking through the intuition of why this works in my previous video I won't talk about it here but I'll link that video for for anyone who's interested although rhf worked really well and led to a significant Improvement in the models performance as evaluated by human preferences there's still a core limitation of rhf that distinguishes it from other reinforcement learning approaches generally the promise of reinforcement learning is that you're no longer bound by existing data so in supervised learning the model is learning by example so it can't really get better than than the available data that's out there but reinforcement learning since models are just exploring the space of responses on its own it's not necessarily bounded by existing data but rhf is a little different because the feedback that we give to the language model is only going to be as good as this reward model and then the reward model itself is only as good as the preference data that we give to it this kind of brings up a fundamental limitation in rlh Chef which is that there's going to be this upper bound based on the quality of the preference data but if we kind of stop and reflect about this for a second we might have the following question which is couldn't we just use this preference data directly to train the reward model like why do we have to go through these extra steps of framing it as a reinforcement learning problem and all this kind of stuff so it turns out we can do that so about a year after the instruct GPT paper came out this paper on Direct policy optimization came out from a group at Stanford basically all it does is it reformulates rlf as a supervised learning problem this greatly simplifies the entire training process while still obtaining the great results of rlf the way DPO works is that we'll get together a preference data set here we'll have a set of prompts we'll have a winning response and then we'll have a losing response so these are just like these relative rankings so we have two responses to the same prompt where one is preferred over the other and then to do DPO what we do is we take a given prompt pass it to an llm and then instead of generating a response and giving feedback on that response instead we compute the probability that our language model will generate the winning response I.E the preferred response and the probability that the language model generates the dispreferred response or the losing response with these probabilities we can simply update the model's parameters to increase the probability of generating a preferred response and decreasing the probability of generating a dispreferred response so fundamentally that's all we're doing but more technically this is the loss function used in DPO so the key thing here is that at each step we compute the probability of generating the winning response we compute the probability of generating the losing response and we compare that to the original version of the model so basically the supervised fine-tuned checkpoint point or whatever model you started with and then basically all we want to happen is we want this to get bigger and this to get smaller we want the probability of generating a winning response to go up relative to the original version of the model and then we want the probability of the dis preferred response to go down compared to the original version of the model so this needs to go up this needs to go down that'll make this a positive number and then we have a minus sign here so this is something we want to minimize I won't walk through the derivation of this equation but for those who are interested you can check out reference number five which is the DPO paper and it's just like a few lines of math where they show that you can write in a closed form the optimal policy of rlf and then you can just minimize the difference between the predictions of whatever model you're trying to train and this optimal policy this optimal model okay so this is a bit abstract and Technical let's make this a bit more concrete through a specific example here I'm going to fine-tune this model quen 2.5 0.5b on my personal preferences when it comes to YouTube titles and so the motivation here is that llms notoriously lack taste this is why anytime someone writes a blog post or writes a LinkedIn post using AI it's really obvious and this is because it's really hard to kind of say why we liked one piece of writing versus another piece of writing and it's hard to give instructions to a language model to tell it how to have my unique Voice or your unique voice these are things to kind of convey through prompt engineering and even through supervised fine tuning it's hard to kind of give all the examples of what I might think is good writing versus bad writing because this is also like context dependent and it can get quite complicated and so the upside of using one of these preference tuning approaches is that instead of having to manually write these examples for supervised fine-tuning we can just give these relative rankings these winning and losing text pairs to train a model which is much easier for us to curate and gives a better training signal to the model to demonstrate the lack of taste of llms I typed this request into chat gbt write me a title for a YouTube video on Independent component analysis these are some of the titles it came up with I and machine learning unmixing signals like a pro exclamation mark independent component analysis the secret behind signal processing the magic of IA separating mixed signals with AI blind Source separation with AA unmixing audio images and data just reading these titles like these are all bad and I don't fully know why they're bad but some things that come to mind is like I don't like this unmixing signal like a pro that's like very vague it sounds like it's trying too hard it's trying to be engaging and sticky but it's just not it just sounds AI generated another one is the secret behind signal processing so this is kind of like a similar thing you're trying to make it a little click baity but does that really make sense for a educational video on Independent component analysis I do like this format though so you have like the keyword the thing that people are actually searching if they're curious about this technique independent component analysis but like this here is just unnecessary this doesn't really do anything and again like people might have different preferences and different opinions and maybe you love this title for whatever reason but me personally I don't like it the magic of I like that gives me no value separating mixed signals with AI I feel like this last one is probably the best because it's like the most accurate it's the most informational but it's just a bit too long if it was just independent component analysis unmixing audio images and data like maybe that wouldn't be too bad but still kind of long let's see how we can use DPO to fine-tune this model on my preferences here's an overview of the process the first step is to curate the preference data next we'll fine-tune the model using direct policy optimization and then finally we'll evaluate the fine tuned model and then I'll just call out that all the code that I used for this example are freely available on GitHub linked here and in the description Additionally the preference data set that I use as well as the finetune model are freely available on hugging face I think I made this all Apache 2.0 license so you can freely use any of this code or data or this model however you like okay so the first step is curating the preference data so this is the most important and most timeconsuming part of the process this is essentially the whole project curating the preference data because this is essentially how we're programming the computer how we're telling the model what's a good title what's a bad title my process for doing this is I wrote out manually 114 different video ideas and then for each idea I generated five title ideas using quen 2.57 V instruct and this was via together AI API and I just used the API because it's much faster than running all this on my laptop and then for each idea I created 10 different title pairs if we have five titles for each idea we can do five choose two and that comes out to 10 unique Pairs and then the fun part of manually reviewing all 1,140 title Pairs and labeling which one is better which one I personally prefer and so 3 hours later I have a data set that looks like this so I have the idea I have title a title B and then I have this column which indicates which one is preferred so one means that title B is preferred and z means title A is preferred reading these like top three text embedding models every data scientist should know I feel like that's quite sticky but doesn't make sense there aren't three embedding models that every data scientist should know I just don't think you can really fulfill on that promise in a video however title be text edting models from Theory to practice that makes sense that seems like a video that I would actually make and probably actually watch so I prefer title B the next one master multimodo rag in just 10 minutes okay that's pretty good and then why multimotor rag is the next big thing in AI this next big thing in AI is just kind of like vague and I'm not sure like what that means or even if it's true I can't think of a narrative around that so I went with title A in this case and then so on and so forth this is definitely very timec consuming I thought it would originally take me 30 minutes but turns out it took me longer cuz some pairs are obvious but other pairs are not obvious and then there are some pairs where they're both bad so like this one the beauty of fractals exploring infinite crinkle and then the math behind fractals crinkle liness and Beyond I wouldn't use either of these but I still just picked which one of the two was less bad in these cases and so I won't share the code here but it's on GitHub to do this whole process to take the video ideas and generate the synthetic titles from them and then of course the labeling was done just manually and I used Apple numbers to do that but then once I have this preference data set I reformatted it like this so we have this triplet of prompt Chosen and rejected so this is like prompt winner loser that we saw before and then I formatted everything as a list of dictionaries because that is the expected format of the TRL library from hugging face which is what I used for the fine-tuning this data set is again available on the hugging face Hub it's called YouTube titles DPO and then the notebook I used to translate the preference data set into this format is also on GitHub now that we have our preference data we can use it to fine-tune the model first thing we'll do is just do some imports this is data set that's from hugging face so I'm using the TRL library to do the training TRL has a lot of different reinforcement learning approaches so theyve implementation of Po which was used in rhf they have an implementation of grpo which was used for deep seek R1 so there a wide range of reinforcement learning approaches available in this library but I just kept it as DPO to keep it simple then I import the Transformers Library so this has some handy things for importing the model that we're going to find tun it's tokenizer and generating resp responses from it and then I also import High torch then I load in the data set and the model so the data set is what we saw in the previous Slide the base model is this one so I'm using this instruction tuned version of quen 2.5 the model that you find tune here doesn't need to be a base model and it probably shouldn't be a base model in fact it would probably be good to take this instruct model train it on title examples through supervised fine tuning and then do the preference tuning but I skipped that step for this project then I'm importing the model importing its tokenizer and then I just added a pad token here next we can just get a sense of how this base model performs this title generation task so I set up this pipeline this text generation pipeline pass in the model the tokenizer and then I pass in a prompt the data set actually has a train validation split I just grabbed the first example from this validation data set and then I have this custom helper function which just puts it into the correct format I'm not going to to show it here cuz it's just kind of long and not really Central to what we're talking about but if you're curious It's on GitHub so that creates this prompt and then I just pass the prompt into the text generation Pipeline and it'll generate an output and so this is what quen 2.5 0.5b comes up with the video idea was independent component analysis intro or something like that and so the title it came up with was unlocking independent component analysis the key to understanding your data so again this kind of has like that hacky voice that hacky vibe that we saw from the Chachi BT ideas for whatever reason it loves to put some word like unlocking or revealing or something like that at the beginning of the title for whatever reason and then it likes to have this vague pseudo engaging part of the title as well with some exclamation mark so let's see how these responses improve through fine-tuning first I'll to find some training arguments here I'm just creating a name for the fine-tuned model so all I'm doing is taking the original model name and replacing instruct with the DPO so that'll call the model quen 2.55 B- DPO and then I have various training arguments here so key ones here I use a batch size of eight I train for three Epoch I load the best model at the end and then the metric used to evaluate what model is best is the evaluation loss so this will be the loss from the validation data set and then I'll save a checkpoint of the model at every Epoch and then I'll print evaluation metrics every Epoch and then with that we can just simply train the model using this DPO trainer so I pass in the model the training arguments the tokenizer and then I have my train validation split for the train and eval data sets and so here's what training looks like so just trained it for three Epoch took about 8 minutes on my MacBook but we can see that the training loss is going down which is good but then we see the validation law starts going up on Epoch 3 so a little bit of overfitting but we have a whole lot of other metric we can use to evaluate the performance I guess the key ones here are like reward margins so this is basically like the difference in rewards between the preferred and dispreferred responses and then reward accuracies is basically the percentage of times that the correct responses preferred and then these two are just the average reward to the preferred responses and then the average reward for the dispreferred responses all these give us a slightly different perspective on how the model is performing so even though the reward margin keeps increasing which is a good thing we have the accuracy increasing and then dipping again which kind of tracks with the validation loss at the end of it we'll load the checkpoint from Epoch 2 and then we'll pass it through the same exact code to generate a title and now we have this title independent component analysis for beginners and so this is much more aligned with something that I would actually write maybe I would add like with example code or with python code in like parenthesis after this but this is much more in line than the previous title while just kind of looking at one title gives us a nice Vibe check of how the model is performing this is not a very robust evaluation strategy so here let's do a proper evaluation of this fine-tuned model one of the key downsides here is that we're trying to evaluate how the model performs based on my personal preferences so there's really not a great way to automate this there's not a metric we can take off the shelf and this wasn't something I could get working with an llm judge so I tried several attempts to get GPT 40 to align with my personal preferences but I couldn't get it to consistently agree with my preferences ultimately I just went and manually evaluated the responses so the process looked like this I picked out 50 random video title ideas from that initial list of 114 for each of the ideas I generated a title using the base model and the ftuned model and then I created these 50 pairs so I created this pH and fine-tuned model pair and then I assigned a preference label to each of them and then I computed how often the fine tune model titles were preferred and so this is what that evaluation data set looks like so we have the base title the fine tune title and then a flag that says the fine tune title was preferred and so just reading the first couple ones this one from the base model wrote from physics to data scientist my journey through mathematics I didn't study math but let's see the fine tuned model said how I became a data scientist my journey from physics to science analytics so science sence analytics isn't really a thing but if you just drop the last few words like this is a good title how I became a data scientist my journey from physics like I feel like I would use that exploring topology secrets and data analysis topological data analysis techniques this topology Secrets is not doing it for me let's see this one exploring topological data analysis a comprehensive understanding okay this isn't great but I feel like this is closer to something that I would actually write than this one a comprehensive understanding that's not good maybe a comprehensive guide or something like that so I prefer the fine-tuned ones in these first several rows but let's look at this one understanding entropy in everyday life a simple explainer it's not great but there could be something there but then this one is quite wonky so entropy of a system the measure of disorder in a system the less disorder there is the so it seemed like it wasn't even trying to write a title it was like trying to write like a paragraph or something so the base title was preferred here and then mechanistic interpretability unlocking the secrets of ai's hidden algorithms or mechanistic interpretability in video exploring the limits of machine learning models I feel like these are both good but like this mechanistic interpretability in video like that doesn't make sense so one thing that was happening is quen was getting confused cuz the prompt said write me a YouTube title idea based on this and for whatever reason it wanted to put video or YouTube video in the title itself which is just not necessary so did this for like 50 different pairs and then the fine tune titles were preferred 68% of the time and then if you're curious about how I created this data set that's available in this notebook here available on the GitHub that brings us to the end if you have any questions or suggestions for future content please let me know in the comments below and then if you want to dive deeper into any of these topics check out the references and the resources Linked In the description below and as always thank you so much for your time and thanks for watching"
RveLjcNl0ds,2025-02-17T15:15:03.000000,How to Train LLMs to &quot;Think&quot; (o1 & DeepSeek-R1),"hey everyone I'm Shaw in this video I'm going to talk about the recent wave of reasoning models which have been trained through reinforcement learning while there has been a lot of hype recently around deep seek R1 my goal with this video is to give you a sense of how models like 01 and R1 work under the hood and through that discuss why this is a promising direction for AI development without any kind of hype this will be a somewhat technical talk but I have attempted to make this very accessible so you don't need a PhD to understand what's going on here so the whole story with these reasoning models started about 6 months ago when in September 2024 open AI put out this 01 model which had this Advanced reasoning capability which it learned through large scale reinforcement learning from a practical perspective what this means is if you have interacted with 01 you've probably seen this you'll type in your prompt like what is the airspeed velocity of an unladen swallow and instead of just immediately generating text the model will start thinking through the Chach PT UI we'll see something like reasoned for 6 seconds and then after that we'll have the actual response from the model unlike the typical response we see from chat which just immediately starts streaming tokens these responses from this 01 model have two parts they have this thinking part and then they have the final response so seeing this you might wonder what's the point of this thinking process other than just just taking longer to generate a response that turns out to be the main Insight from the 01 model which is this so-called test time compute scaling which is just a fancy way of saying that when trying to solve reasoning tasks like math problems logic problems coding problems things like that they saw this pattern in the models responses where basically the more tokens it generated the better responses it had this is summarized graphically through this plot which I reillustrated from open ai's blog post on it on the left here we see this so-called train time compute and so this is summarizing the well-known pattern that if we take language models and we just make them bigger give them more data and train them for longer they have better and better performance so that's why gpt2 was 1 billion parameters but then gpt3 was 100 billion parameters and gbt 4 may be like 500 billion parameters or even a trillion parameters we don't know exactly but what we do know is that the bigger you make these models the more data they're trained on the longer they're trained for the better their performance so that's what this left plot is showing aim is just a data set of math problems so the model gets better and better at math the longer it's trained on the right we have this so-called test time compute which basically means the number of tokens the model generated so we see the more tokens that the model generates the better its performance becomes on these math problems so this is the key Insight from 01 which is basically training bigger and bigger models is not the only path to better performance we have this other path on the right which is generating more tokens although this is something that people have known through prompt engineering so if you've been playing around with llms yourself and you've been trying to get them to do what you want and you find that sometimes it takes you a very long prompt in order to do that another version of this is with so-called Chain of Thought where you basically prompt the model to think step by step and then to work through each step of the problem one at a time both of these things have been observed empirically to lead to Predator performance so this is just yet another piece of evidence to support that idea so in contrast to before where maybe we pack all this into the prompt or we just prompt the model in a clever way to get it to generate a lot of tokens 01 Works a little differently so it has the so-called thinking tokens they're special tokens that open AI introduced in the post-training phase meaning they introduced them after they created the base model and basically this delimits 0 one's Chain of Thought So it'll be something like this I don't know exactly what the special tokens look like but you can imagine it's just something like this where you have a start thinking token and an end thinking token and then between these two tags 01 is going to Think Through the problem this is what we saw in the first slide where the model was thinking through the problem and then it was printing out a summary of its thought process so it first recognized that it was a cheeky question referencing Monty Python and then it was actually thinking through the physics of how fast swallows can fly so this is important for two reason one from a user interface perspective once you have these clear tags of when the thinking starts and stops you can know when to suppress the stream to the user once your UI sees this starting token you know not to print it to the user interace but once you see the end think tag you can know to start streaming tokens to the user interface but another cool thing about these thinking tokens is that you have an interpretable readout of how the model is thinking through the problem so this might be very much like the internal dialogue a person has when thinking through a problem and it's great cuz it's in English so we can read it and try to understand how the model arrived at its solution through this we've realized that doing this Chain of Thought letting the think before answering leads to better and better performance on these reasoning tasks but the central question is how did open AI implement this Unfortunately they didn't publish the technical details of how they pulled this off they did share that they used large scale reinforcement learning to do this but they didn't really go into much detail beyond that so for much of the past 6 months many people have been using o1 but no one really knew how open AI did this until deep seek which is a research lab out of China published this paper which revealed the Mysteries behind 01 so the paper was called Deep seek R1 incentivizing reasoning capability in llms Via reinforcement learning in this paper they shared the technical details behind how they developed two models one model was called Deep seek r10 which was trained strictly on large scale reinforcement learning just like what open AI said they used for 01 and then in the second model they used a combination of super supervised fine tuning and reinforcement learning of these two models deep seek R1 has definitely gotten more public attention mainly because it has better performance than deep seek r10 but this first model is very important and helpful for two reasons one it generated training data to create R1 but the second which I find more interesting is that it gives us a lot of insight on the power of reinforcement learning and to give the punchline the amazing thing about deep seek r10 is that it learns all by itself through reinforcement learning to do this Chain of Thought reasoning and to do things like verifying and reflecting on its past responses and thinking through problems I'll talk through the details of each of these models one at a time so we'll start with deep seek r10 again the key feature of this model is that it was trained using only reinforcement learning just to compare this to supervised learning learning which is significantly more popular this is where the model learns by example so to use an analogy supervised learning is kind of like if I was looking for a new job and so I wrote up my resume but I wasn't sure if my resume is any good so instead of applying to a job I would give it to a recruiter who would read through it and then give me specific feedback and would send me an example resume which I could compare to my original resume and make some changes and updates to make it more like the one the recruiter provided on the other hand reinforcement learning works differently so instead of teaching models by example in reinforcement learning models learn on their own through trial and error this is very similar to how we might actually learn in the real world where I might not send my resume to a recruiter or a resume expert but instead I'll just take my resume and apply to a job with it and then from there it's very unlikely that the company is going to give me very specific feedback on my resume or give me examples resumés of past candidates who got hired what's more likely going to happen is they'll just send me an automated message that says you've not been chosen at this time so this is fundamentally different than supervised learning because now I have to figure out what I did wrong in my resume and then figure out the improvements that I can make to it so we can essentially follow the same process to train a large language model so in supervised learning we can have the model generate an output and then we can compare that generation to a human annotated a human labeled output and train the model on this data on the other hand with reinforcement learning we don't generate examples that the model can learn from instead we just take the model's output and then do some kind of evaluation on it so if this is some kind of like mathematical proof we can just tell the model it got it right or it got it wrong basically the model has to figure out on its own how to write good mathematical proofs while there are a lot of technical details behind r10 I just want to highlight three key features the first is the prompt template so this is basically how example questions are passed into the model these reasoning models are trained on question answer pairs which require reasoning in other words they're trained on things like math problems coding problems logic puzzles and things like that for training r10 deep seek used this prompt template basically they're just describing that there's this conversation between a user and an assistant the user is going to ask a question and the assistant's going to solve it prompting the assistant to think through their reasoning process and then enclose their reasoning process in these special think tokens and then enclose their answer with these Special answer tokens and then it has a place where the prompt will get injected and then everything after will be generated by the model and so this prompt will be a question that requires reasoning with a clear correct answer so like math coding and and Logic the amazing thing here is how simple this prompt template is it's nothing sophisticated they're not telling the model explicitly to reflect or verify or do any other kind of reasoning strategies this was actually something intentional they didn't want to include any kind of bias in the prompt template and just observe the Natural Evolution of the Chain of Thought reasoning the model generates the second key component here is the reward signal that is passed to the model this is just like if I apply to a job the reward signal I get is either they reject my application or they call me in for an interview so we need to do a similar thing for training r10 here deep seek had two components to their reward one was accuracy all the questions in the training data set had concrete correct answers maybe they're math problems like this where there is a correct value and you can just use a rule-based checker to see if the answer the model generated was right or wrong the other component to the reward was formatting so basically ensuring that the reasoning process that the model goes through is indeed within the think tokens again this is something that can be rule-based you can use some kind of logic to ensure all the reasoning occurs within the thinking tokens the Striking thing here is how simple the reward is and again this was intentional by the researchers because they suggest these modelbased rewards in other words Rewards generated by a neural network are prone to reward hacking which is basically this situation where the model we're trying to train learns how to exploit and trick the reward model to maximize its rewards without actually getting better at the task that we want to train it on and this is very much how humans operate in any kind of incentive structure you try to set incentives so people's activities are aligned with the company's goals but inevitably what happens is that people start to find tricks and hacks and learn how to exploit the incentive structure to maximize their personal gains while forsaking whatever goals the managers had in mind for the company so that's just to tell you that whether you're trying to come up with incentives for humans or for computers it's hard either way so going with very simple ones seem to work well in this case the final component of this reinforcement learning process is we need to figure out a way to use these rule-based rewards that were generating to actually update the model parameters deep seek uses this technique called group relative policy optimization or grpo this is quite technical if you don't really care about the technical details of how this works feel free to skip ahead but for those who are interested I'm going to explain it step by step the central question here is how can we incentivize the model to maximize rewards and so I'll just claim that we can achieve this through the following optimization which is shown here where we're we're trying to maximize this term here with respect to this parameter Theta let's just unpack what this term actually means the numerator is the current model's probability of generating an output in response to a question and so you can imagine in this training process we ask the model questions it generates answers and then we compute the rewards and using those we're able to update the parameters what that means is that at each step of this optimization the model parameters are being updated and being tweaked so the model is evolving let's say the 10th version of the model's probability is represented by pi Theta while the probability from the ninth version of the model is represented by pi Theta old and so a key detail here is that Q is just some question so you can imagine some kind of math problem or math question and then o is some output generated by the old version of the model by the version of the model using parameters Theta old so we can take that out and compute the probability that the model generates that output using the model parameters then we can do the same exact thing by passing in the same question and the same output from the old version of the model pass it through the new version of the model and compute the probability that the new version of the model generates this output okay this is all very technical but what does this mean what does this ratio actually mean so the key thing to know here is that this ratio is quantifying how much the current model is deviating from the pre previous one with respect to this question Q in other words it's just telling us how much the model has changed through this single step of the optimization the other key part of this term is R which is simply that rule-based reward for the output o so this is just something we compute using some python code no model training no neural network involved whatsoever so we have these two components we have the deviation of the current model from the previous one and then we have this rule based reward but how does maximizing this incentivize the model to maximize the reward and so just going one level deeper if this ratio is greater than one that means that the output is more probable in the new model compared to the old model conversely if this ratio is less than one that means this output is less probable in the new model than in the old one then if the ratio is about one there's little change in the model on the other hand for the rewards let's say if the reward is greater than zero that's a good thing and then if the reward is less than zero that's a bad thing bringing this all together in order to maximize this whole term what that comes down to is whenever we have a good reward so R is positive we want to make this deviation as big as possible so we have a positive reward multiplied by a big positive number as big as we can make it on the other hand if we get a bad reward so we have a negative reward here then what we want is to make this this ratio as small as possible and keep in mind that these are probabilities so they exist between 0o and one so this ratio is always positive so basically we want to make this ratio as close to zero as possible just to summarize the goal of maximizing this term with respect to Theta is to update the probability of generating this output such that the deviations are much greater than one for positive rewards for good rewards and the deviations are much smaller than one for negative Rewards that's the key intuition so we can incentivize the model to do what we want to basically make outputs with high rewards more probable and make outputs with bad rewards less probable by doing this optimization but it turns out that reinforcement learning can be really unstable and so there are some measures that need to be put in place to ensure that training is stable over time so one thing the authors do is Implement clipping this first term is the same exact objective as the same exact term we saw on the previous slide but now we have this clipping term and basically all this is saying is that if the deviations are too far away from one to clip it Epsilon is just some parameter we choose so if Epsilon is let's say like 0.3 or something if this ratio is less than 0.7 we're just going to round up to 0.7 and then if this ratio is greater than 1.3 we're just going to round it down to 1.3 that's all the clipping does it's just ensure that this ratio doesn't get too small and it doesn't get too close to Infinity Min is just saying that we're going to take whichever of these two values is smaller so we'll take this original objective or we'll take this clipped version whichever one is smaller but still that's not enough to stabilize training so they also add this regularization term here we have the original objective here we have that clipping term and then we have another term which is this K Divergence term expressing this out mathematically We compare the probability of the output of the current version of the model to some reference checkpoint of the model that we've deemed good so they didn't say how they picked this reference checkpoint but this could be the previous version or every 10th step or something like that just some reference version of the model to again ensure that training is stable but wait there's more so another part of this is that this is called group relative policy optimization that kind of comes into play here so we're not just going to generate one output for a given question but rather we'll generate a group of outputs and then we'll aggregate this value over multiple outputs for a given question that's what this group and group relative policy optimization means there is yet another component to this which is that the rewards are standardized at the group level so we've swapped out the RS with this a term where a is essentially a standardized version of R which means we subtract the group mean from each value and divide by the standard deviation after all these steps we arrive at the final objective function that they used for training deep seek r10 which is this monstrosity here but now we know what each component means this first term is the key thing we want to maximize this second term is if this probability ratio is too far away from one we're going to clip it and then finally we have this KL Divergence regularization term when I first saw this equation I was very scared and confused but after going through a long process of breaking it down it now makes sense to me and hopefully makes sense to you as well okay so enough of the technical details let's see what the performance is what is the result of all this effort so there are two things I want to talk about one is the performance so this is from the paper and we can see that the more they train the model the better and better it gets until it surpasses 0 one's performance on AIM which is a standard math problem data set there are two different evaluation metrics pass one and consensus 16 so pass one is how many times the model gets a question right out of like 10 tries or something like that and then consensus is we're going to generate 16 answers from the model and then do some kind of majority vote to decide what the final answer is going to be with the second evaluation metric r10 outperformed 01 with an even larger consensus size which is remarkable because this was a base model that they just trained through this reinforcement learning process so all on its own it learned how to do this Chain of Thought reasoning and solve math problems but this is is really depicted by this plot here which is figure three from the paper and you can see the longer the model is trained and the better its performance gets its average length per response gets larger and larger so this is another piece of evidence toward this test time compute scaling which we saw in 01 on an earlier slide so this is very interesting that we're able to get better and better performing models just by generating more and more tokens but I would say the most striking thing about our 10 is actually reading through its Chain of Thought reasoning so it's going to have these long chain of thoughts with this like reflection so I asked r10 this question if it takes me an hour and a half to write a code base and my intern eight hours how long will it take us both to write three code bases it just starts going off and it starts thinking through the problem to solve for how long it will take both you and your intern to write three Cod bases together we first need to find out how many C bases each of you can write per hour individually and then it has a very long chain of thought and then it kind of has an answer here and then to make sure everything makes sense it rounds it however we need to make it more human friendly so convert it it's kind of like thinking through and doing all these like weird reasoning things which it learned all on its own which is really interesting but it also learned this verification step again if we recall the prompt Ed to train this model there was no guidance it just said think and then give an answer those were the only constraints but the model learned that it's helpful to do this verification step and so it has like a long verification process and so I truncated it cuz it's quite long and then at the end of it the model gets the answer right so 72 over 19 hours so it's approximately 3.79 hours there were a couple key limitations with r10 because it is just learning on its own by trial and error kind of how if we learn things by doing in trial and error sometimes we can learn some pretty quirky routines and habits and behaviors but one of the things was readability the model doesn't always format responses properly so it doesn't box its final answer or doesn't use markdown so it's hard to parse the final response but another big one was language mixing so if you ask a question to the model in like French or Spanish or German or basically a language that's not English or Chinese it just replies in English so an example of this is I asked it add 3 + 5 in French and it just gave the whole answer in English so there's this language mixing which isn't great for like a downstream use case to fix these things they created deep seek R1 which is the model that's more popular it has better performance and the one people are excited about these days and so deep seek R1 was trained in a more sophisticated way it did use reinforcement learning but there were these other steps in the training process so it starts with deep seek V3 base which deep seek published in December of 2024 and the first thing they did is instead of just having it go off and try to learn on its own they gave it a lot of examples of reasoning data so they gave it long chain of thoughts to show the model okay this is the format you need to use this is how you can think through problems this is how you reason so this is like learning by example just like the recruiter giving me a bunch of resumés of people who are hired at the company that I want to apply to so I can make my resume similar to those step two is doing this R1 zero style reinforcement learning so first you do the supervised fine tuning then do the reinforcement learning like we just saw in the previous slides and then there's another supervised fine-tuning step but instead of just training on reasoning data it's a mix of reasoning data and not reasoning data and then finally some more reinforcement learning but also some reinforcement learning with human feedback which is what was used to create instruct GPT which was the model behind the original version of chat GPT the output of this is deep seek R1 which is a state-of-the-art language model at least it is today okay so let's go through these steps one by one and I'll just kind of give sketches of each of these steps and if you want more details you can check out the paper but I will say as much as the paper reveals about what it takes to create one of these models they don't go into a whole lot of detail about their data set and they don't actually share their data set so that's like an important thing to keep in mind but I think reference 4 is a open-source data set from hugging face that they're starting to put together and that'll give you a good idea of the type of data used to create these reasoning models this is a question from that hugging face data set and so it has a math problem and then the data set includes the worked solution so these both will be passed to deep seek V3 base to learn how to reason the authors describe three different sources for this reasoning data one it's few shot prompting so you take a question you have a long Chain of Thought example I guess human generated like we saw in the previous slide and then the right answer and it's all formatted in the way that they want the final model to format answers another one is to prompt reasoning so instead of giving examples of long chain of thought in the prompt instead they explicitly prompt the model to think step by step to reflect verify the kinds of things that people have been doing to try to get models to generate Chain of Thought reasoning over the past couple of years and then finally they use data from r10 so they would take R1 Z's template and generate a response from it and then use that as training data so after I think they used thousands of supervised fine-tuning examples for step one then for step two they do this R1 style reinforcement learning so just like what we saw previously but then they added this language consistency reward to help mitigate that language in problem again we have the rule based accuracy rule-based formatting rewards but then they add this language consistency reward which was basically the percentage of the target language words used in The Chain of Thought if we asked the model Plato ear ALB which is can I use the bathroom in Spanish and then the model generates the Chain of Thought it's mostly in English so this would be like a language consistency score of 4% cuz only about 4% of the Chain of Thought is in the target language of Spanish so this is just a way to incentivize the model to think in the same language as the prompt step three is to use mix data so the key thing here is it will show the model when and when not to reason this is actually a pretty funny point so if you go to r10 and you just type in hello or hi there or something like that it's going to generate an output like this and just start solving math problems they talk about antisocial mathematici or insert any technical discipline there this just goes to show you that the model when you only fine-tune it on math and reasoning problems it doesn't really help you in other situations to mitigate that they have some reasoning data which is one generated by the checkpoint of the model that comes out of step two so they get long chain of thoughts from there and they also incorporate here non-rule based rewards so basically they use deep seek V3 as a judge in order to generate rewards for a subset of the reasoning data this just allows the model to learn from more flexible prom which don't have a concrete right answer so there were 600 samples of this and then there was also this non- reasoning data which a lot of it was taken from the supervised fine-tuning data set used to train deep seek V3 and they also generate synthetic data from Deep seek V3 and they also hardcoded some responses to questions like hello or who are you what are you the self-cognition kind of questions and then there were 200,000 examples of non- reasoning data the final step is to do more reinforcement learning but then additionally reinforcement learning with human feedback so this is basically to just make the model's outputs something safe for humans so something that humans can actually understand and read and is helpful to us so again it's this r0o style reinforcement learning so with accuracy formatting language consistency but also this reinforcement learning with human feedback training the model along these two dimensions of helpfulness and harmlessness and since you can't really write rule-based rewards for these two things the way they generate rewards for these two Dimensions is they'll take a bunch of responses have human annotators score or rank how helpful or how harmless the responses were and then that human annotated data is used to train a reward model which can be used in a similar kind of reinforcement learning Loop that's all the technical details behind r10 and R1 one of the key contributions here is that since these models are open weight they are freely hosted in a lot of different places so you can go to deep seeks website directly together AI has deep seek V3 deep seek R1 and many distillations hyperbolic is actually the only place that I found deep seek r10 hosted but if you know of another place hosting it let me know in the comments but also you can get this model if you have enough compute power you can install these locally or on your own Hardware so you can use amaama for that or you can use hugging face and hugging face basically all these models and many other versions of Deep seek and its distillations and fine tunes that other people have created I've actually linked these in the blog post associated with this video so you can access the blog completely for free in the description below what does this all mean I know there's a lot of hype and news around R1 and what this means for AI and for open Ai and stuff I don't read the news so I don't really know what all the buzz and commotion is all about but from like a research perspective the key takeaway is that o1 and R1 have sh shown us that reinforcement learning is a promising research Direction so the question here is why should we care about reinforcement learning and the key thing is that through reinforcement learning llms May no longer be bound by existing human knowledge so this was demonstrated about a decade ago at this point I feel by alphago which was a model trained to play the game of Go and it was trained using reinforcement learning and it actually beat the world's best go player or at least one of the best go players in the world and so the key thing with this plot here is that we can see that the version of their model that was trained only on supervised learning would get better and better and got better pretty quickly but then it never could quite reach the skill level of the top top Grand Masters in the world because this model was just trained on Grandmaster data so if it's just trying to mimic the grand Masters it's never going to surpass the grand Masters but reinforcement learning is a different story because in reinforcement learning the model doesn't learn from examples of humans playing the model learns on its own through trial and error and its reward is whether it won the game or didn't win the game and so through this process the model is actually able to surpass Human Performance and actually teach humans some new ways and new approaches for playing the game of Go and so transferring this idea to the domain of large language models like what does that mean are we going to have large language models that can reason better than humans do research better than humans that can do math better than humans do science do fundamental research and so these are all open questions and I feel like an exciting time for AI research CU it's really interesting to see where these reasoning models can take us and where reinforcement learning can take us with large language models and so the description people have made about llms for a while is that they're just stochastic parrots so they're just recalling and remixing and reaying texts that they read previously on the internet which is fundamentally what they're doing when trained strictly on supervised learning they're just kind of regurgitating and remixing information from its training data but the promise of reinforcement learning is that these models can potentially transform from parrots into proper researchers and scientists and contribute new information new findings new insights new ways of thinking to our Collective knowledge which would be very exciting indeed and so that's it I hope this video was helpful to you and gave you some idea of what is going on under the hood of these reasoning models so if you enjoyed this and you want to learn more check out the blog Linked In the description which is like a right up version of this as well as the different references and with that thank you so much for your time and thanks for watching"
bZr2vhoXSy8,2025-02-08T18:10:05.000000,I Fine-tuned FLUX.1 on My Face (and how you can too),"flux is a state-of-the-art image generation model it takes a text input and generates high quality images from it although this model can do incredible things it can't produce photos of specific people in this video I'll walk through the full process I used for fine-tuning flux to generate unlimited highquality photos of myself so here I'm going to walk through a pretty straightforward process for fine-tuning this model using Python and replicate the work flow that I use here has four key steps so the first step is to get a set of images of myself to train the model on next is to write captions for each of these photos and then we can pass these image caption pairs to flux to do the fine-tuning and then finally we can use the fine-tune model to generate new images and we can see some fun examples of that on the right here but before going through this whole process step by step there are a few key requirements one is you'll need python 3.8 or newer installed on your machine you'll need a replicate API key finally you'll need 10 to 20 highquality images of yourself greater than 1,24 by 1024 and this is actually not too hard if you just go on your phone or something like that usually the images are much higher resolution than this so these will be like all sorts of shapes and sizes so here a handful of me finally I'll call out that the example code that I'm I'm going to walk through here is freely available at the GitHub repo shown here and also linked in the description below to start we're going to import a handful of helpful python libraries we'll import pillow to do some image processing OS to load and save files to particular folders we'll use the shell utility to compress our training data the most important import here is replicate they have a python library that is basically a python front end to their API so that's going to allow us to do a lot of key steps in this process then this import isn't super necessary but this is just how I chose to import my replicate API key we can actually see that here load. EnV is just this method that will look in your python directory look for this secret. EnV file and then it's going to make any environment variables that you have stored in this secret file available to your python script so I just chose to import my replicate API key in this way you can do the same or alternatively you can just hardcode your replicate API key into your python code which is just simpler what that'll look like is we're going to create this replicate object and basically all we're doing is we're setting up a connection to the API and passing in our API token I'm just grabbing my API token from this environment file but similarly you can just hardcode your API key here so with all the Imports out of the way we can start processing our training images here I use 20 images and here are eight of them and you can see that they're all different shapes and sizes and resolutions from different angles with different haircuts with hats on different facial expressions and things like that diversity is a good thing here it's just going to make the fine-tuned model outputs more flexible many of these were for my iPhone so they had the heic file type so I needed to convert them to PNG and jpeg so I took that and then I put them all into a folder called raw but there's a problem here since these images are of all different shapes and sizes these aren't optimal for training flux on really what we want to do here is make sure that all the images are square and they are the same resolution specifically making sure that we crop all these images make them square and have them have a resolution of 1024x 1024 to do that we can use Python I'm going to define a handful of helpful variables I have all the Raw photos saved in this raw directory and then I want to pre- process them and save them in another directory called Data here I'm just checking if this data directory exists if it doesn't it's just going to create it finally I'm initializing this image counter which will basically keep track of the number of images that we have I'll use this for coming up with the final image file names to actually do the pre-processing here's the code for it it's quite long but I'll go through step by step basically we're going to look in the input directory so that raw subfolder and we're going to iterate through every single file we're going to go through each file one by one we're going to take the file we're going to check if it's a PNG or jpeg with this logic here and if it is we'll go ahead and do this chunk of code so the first thing we'll do is grab the image path this is basically just combining the input directory name which will be raw and the file name which we're getting from this Loop here then we can use the pillow library to open the image so basically we provide the full path of the image and it'll create this image object which will allow us to process it in python as a first step we want to make the image Square so that'll involve cropping it and I actually had CHT write this Logic for me but I'll explain what's happening we'll take the image and get its size so it has the size attribute it'll give us the width and the height and then we're going to pick out the smallest Dimension the reason is we want to make the imagees square so basically we're going to use the smallest Dimension as the final Dimension size for the square image so for example if the image is 2,000 by 3000 we're going to take that image and crop it so it's just 2,000 by 2000 next we're going to Define how we want to crop it so this is going to consist of picking four different coordinates basically four different numbers that are going to specify the left top right and bottom of the image we're going to take the original width and we're going to subtract it by the new size if the width was the smallest size this is going to be zero but if the width was the largest size this is going to be a positive number and then it's going to be adjusted a little bit to the right for example if the width was 3,000 and we want to change it to 2,000 this will be 3,000 - 2,000 which should give us 1,000 and then divid by 2 is 500 so basically the left coordinate is going to be shifted to the right by 500 pixels and then we'll do the similar thing from the top so if the height was the largest size we'll adjust it accordingly and then finally we'll just add the new size to the left and top coordinates that we defined earlier now that we have these four coordinates we can simply crop the image just using this crop method for the image now we successfully made the image a square but the training strategy that we're going to use they recommended to use 1024x 1024 images so basically what I'm doing is I'm downsampling the image cuz all of these are higher resolution than 1024x 1024 but basically we'll just reduce the resolution from a higher quality to this 1024x 1024 and then as a final step I'll just save the image as a PNG so what I do here is Define a new output path it'll be output directory which was data and then I'm going to define a final name I just call it image- I I was initialized to be zero so the first image is going to be called image- z.png and then we'll just save this resized image to this output path finally We'll add one to I and then we'll repeat the whole process by the end of this we'll have 20 images saved in the data directory with file names ranging from image- z.png all the way to image-1 19.png and then we can see the resulting images you can see it's the same image but now they're all squared and they have the same resolution if you have raw images where you're not the subject of the photo you may need to do some additional cropping to ensure that you're in the center also it's better to not use images where there are multiple people in it if you're trying to F tune flux on yourself just use photos where you're the only subject in that image so we have these training images the next thing we want to do is generate captions although we could do this manually for each of the 20 images just writing a caption I took a different approach and used this multimodal language model called lava 13B which is available via the replicate API basically I'll go through each image one by one pass it to this model have it generate a caption and save that caption the way we're going to do this is we're going to iterate through each file in this output directory which is called data then we're going to check if the file ends with the file type PNG which it should because in the pre-processing step we saved all our photos as pngs and then we'll do a similar kind of thing so we're going to create this image path by joining the output directory in the file name and then we're going to open up this image object then what we'll do is we'll use replicates API we're going to interact with this lava 13B model put out by yo Rick VP and this is like the model version it's getting cut off here but the full name is available ailable in the GitHub repository just like a very long character sequence and then we can pass these inputs to the model so we'll pass it the image which we just created here and then we'll prompt the image by saying please write a caption for this image this by default will create this output object which is an iterable it's optimized for streaming the text responses so what I do here is I take this output object I convert it to a list and then I join each element of this list with a space that's just going to give us a single string of the model response and then this is a key Point here so instead of just using this response as the caption itself it's important that we include a trigger token so what this is is a unique sequence of tokens which is not a real word that we include in all of our captions so that the model learns to associate this trigger token with my face so after fine-tuning if we want to generate an image with my face in it we just include the correct trigger token and it should hopefully do that I just take a really simple strategy here and prepend the response with this string of a photo of Shaw TB finally I save the caption as a text file so what I'm doing here is I'm taking the image path which for the first example will be data/ image- z.png and basically what I'm doing is I'm just dropping the PNG and then adding a txt so we're saving leaving the caption with the same exact name as the associated image but we're just replacing PNG with txt and then without we can print out the caption and then we can write the text file to the data directory at this point we have 20 images and 20 captions for that image all generated automatically the next thing we want to do is to compress this data folder as a zip file so we can upload it to replicates API to do the training so I just use the shell utility module to do that data is what I want to call the zip file so it'll be called data.zip zip is the compression strategy that I'm going to use and then data is the folder that I want to compress the result of this line of code will just be a zip file called data.zip now we have our training data that was the hard part and also the most important part now we can fine-tune the model in a pretty straightforward way in fact we don't even have to write any python code to do this this guy Jared burette he has this company called ouus and he created this training endpoint on replicate that you can freely use and there's actually a web UI for it so you can actually go to this URL replica.com ostrc flux Dev laurat trainer train and basically the whole process that we're going to run through in Python has a convenient web UI so you can just take that zip file we created and upload it to this web UI to do the same exact thing just to show you what that looks like this is the web page and then there's some instructions and so so basically there's a form here that you just upload the zip file to and you can set specific parameters if you like and it'll just create the training for you automatically but let's see how we can do this process with python the first thing we want to do is create a new model on replicate first I'm putting in my replicate username then I'm going to define the name of the model I call it flux Shaw dfine tune I'm going to make this model public but you can also make it private here you're going to specify the GPU you want to use for inference for this model but I guess replicate overwrites this so I got this code from replicates blog on this process so I guess it doesn't really matter what you put here maybe it's going to replace it with the h100 either way finally we'll put in a description so I just called it flux1 fine-tuned on photos of me then we can print the model name and the model URL then we can train the model now we're going to create this training run so here we're going to use this training endpoint from ouis we'll upload our ZIP file with all our images and captions we'll Define how many steps we want to train the model for and then we'll specify where we want the fine-tuned model to be saved these are things that we just created in the previous slide and then this training job will get kicked off so we can print the training status and then you can view the status on your replicate dashboard again the full name of this endpoint is getting cut off here but the full name is available on GitHub and I also linked the training endpoint in the description below so training took about 12 12 minutes for me and it cost $112 but after that's done we can use the model this UI will get spun up and we can see it did use a h100 so I guess it didn't matter that we specified to use a T4 but what's really cool about this is that it by default creates this playground that we can interact with a fine-tuned model with so let's actually check that out now here we see the model flux Shaw fine-tuned and then we can specify a prompt so this is not a trivial process I found that it's a bit of a learning curve to figure out how to effectively prompt flux but I slowly figured out a couple of tricks and her istics so let me try something like a cinematic portrait of shot TB so we're using our trigger token a hooded man sitting in front of a Mac Book Pro writing code the background is dark and the light from the screen illuminates his face he has a serious and focused look on his face and then I found that these weird keywords tend to work well like cinematic lighting intense just like random things like that I'm by no means a flux prompting Guru but we'll see what this spits out we can alternatively also upload an image that we want to condition the model output on this handy if you want to like take an image and basically make a version of it with you in it you can also upload a mask file if you want to just do the image generation for certain parts of an image we can set the aspect ratio so I'm going to do 16 by9 which is what YouTube videos are but there are a bunch of options if you do custom you can customize it you can set what the prompt strength is so basically one the model will only listen to The Prompt zero the model will only listen to the input image we can specify what model we want to use so there's Dev and Schnell so Schnell is like a faster version and we set number of outputs so I'll actually do four which is the maximum inference steps this is basically the number of times the images passed through the model and then we can do a guidance scale so lower I guess is more realistic but I'll just set it as default then we can have a random seed to make it repeatable and then we can select the output format I'll just keep it as a web image output quality and then there are these other things so like safety Checker go fast megapixels low r scale because this is using an efficient fine-tuning method called lowra this is adjusting how much the model is going to listen to those adapter weights and if you're not familiar with Lura I talked about it a bit in a previous video in the context of large language models but the same idea applies to this context of image generation okay so let's see what this produces so it does take some time to generate these images I'm doing four Images usually takes like 30 seconds or so to spit these out let's see these examples these are pretty good so this is me coding oh this is another we see the the MacBook oh yeah this is good this could be a thumbnail or something oh this interesting there like a little reflection off this screen I honestly like these I'm going to save these so you can just click on them to download them let me see if I can add a camera angle camera is angled directly toward man you can easily find yourself spending way too much time playing around with prompts I know I did I think eventually you kind of learned some tricks and figure out how to prompt it the first time to give you something that you like this is another this could be a good thumbnail as well not a huge fan of that one okay this is interesting oh this is cool I like this as well I look kind of like a alien in this one my eyes are too close together and it's interesting so using the playground is fun you can get you some prompting chops and produce things pretty easily however we can also do the same exact thing through python so this is helpful if you want to create like a new UI or you want to experiment with several different prompts or several different parameter choices or if you want to just generate a 100 images let it run you go do something else and then you can come back and just view all 100 images or something like that this is how you can just do it through replicates API this is very similar to what we did with the lava 13B model but now we're just going to specify the fine-tuned model and then we specify all the inputs we want to pass to it and then again the output object is this iterable so it's going to generate a bunch of URLs to web pictures which we can print out like this and then we can click on them one by one or you can download them directly to your computer so I played around with this for a while and and here are a handful of pictures I made of myself with generous appropriations of hair and physique I also really like this one cuz if you look in the background it's just like a bunch of variants of me it looks like some kind of very weird dream that you wake up in love the hair here this me as a surfer I've never surfed before but yeah these photos are like really high quality here's some more so here's me as a doctor so I can send this to my parents to let them know I've made it in life this one looks very realistic so this is like a more accurate photo but also you know this is me as a quarterback this is in the likeness of that shot of selan Murphy in Oppenheimer you can definitely have a lot of fun making images with this model and this is actually super valuable to me because this helps me create unlimited images for thumbnails which is a very timeconsuming part of making YouTube videos okay so what's next here I used replicate to do this whole process and it made it super easy thanks to Jared burette and that training endpoint that he Mak conveniently available to everyone but you know there's one downside here so I generated maybe a few dozen images and then to do the training as well it cost me about $7 which is not a lot of money in the grand scheme of things but of course the researcher in me wants to pick this thing apart and have like a deep understanding of everything that's going on so the next step I see here is to figure out how to do this whole process completely on my laptop so that's training the 12 billion parameter model and running inference for it so I don't know if it's possible I just got a new Mac book it has a M4 with 64 gigs of memory so I'm going to try it out and then I will hopefully have a video on that if everything goes well all right so I hope this video was helpful to you and you can make a bunch of entertaining photos of yourself with this and again the code I walk through here is freely available on GitHub and with that thank you so much for your time and thanks for watching"
QvxuR8uLPFs,2025-02-03T18:00:00.000000,How to Build Customer Segments with AI (Real-World Use Case),"although today's AI models are clearly powerful it's not always obvious how businesses can use them to generate value last month I hosted a workshop where I shared how I used AI to create customer segments for my business the two key benefits of this approach are one it can process unstructured data such as open-ended survey responses and two it can easily scale to hundreds of thousands of customer records in case you missed it here's the recording I'm going to be talking about a quick project that is easy and instructive but also I used this exact code for like a practical real world business problem for myself so excited to share it and hopefully gives you guys some ideas on some beginner friendly projects that you can start working on that also provide some kind of real world impact if you don't know me I'm Shaw here's a little quick about me before we get into the talk I got to AI about 6 years ago while I was getting my PhD at that time I was a researcher working a lot with machine learning and AI for scientific discovery after that I graduated and went worked at Toyota as a data scientist after a while in that role I went out on my own so I quit my job to become an entrepreneur and so that consisted of doing some independent Consulting so I helped over 100 clients my first year doing a lot of content creation but also doing some other offers so started a course on Maven also I launched this app called why Tob which converts YouTube videos into blog posts which is something that I personally use in my own content creation let's just dive right into it the business problem is that analyzing survey data to improve in this case I'm talking about a course but it could be any kind of product or service to improve the marketing and the product itself that's the problem we're trying to solve typically numerical responses are pretty easy to analyze so for my course I have a pre-course survey and I have these two questions which have numerical responses which are pretty easy to analyze so we can see how much experience people have with the a ML and how much programming experience they have and it's prettyy easy to visualize it and do some kind of analysis but these numerical metrics don't tell us the whole story especially they don't tell us why they enrolled in the course or if it's some other kind of product they don't tell you why someone bought your product or why someone bought your service numbers can only tell you so much so this is where open-ended responses are super helpful the one I had for my precourse survey was what is your dream outcome for the course and then this is where you really get the real meat of why your customers bought your product or your service the open-ended responses give you better insights but of course the downside here is that it's tedious to like read through all this and it can be difficult to analyze and read the tea leaves so the first thought might be something like chat GPT so this is like a great initial solution but it may not be ideal for the long run so let's just talk about the pros and cons here the pros is that chadt is fast and easy to use you can take a CSV file upload it to chat PT say something like hey can you please split this into different customer segments and generate personas for each segment or something like that and another upside of chpt is that it can work with both the numerical and the text Data the text Data it can understand directly can also understand the numerical data but it'll probably write some python code or something to analyze the numbers that you provide in any kind of CSV file but of course there's some downsides to using chadut so one is that this might break down if it's a complex task so if you have a lot of different variables that you're looking at or it's going to require some kind of like multi-step reasoning to generate a good breakdown of these customer segments I guess now they have this more agentic stuff with chachu but still it can kind of go off the rails if you don't give it a lot of good guidance and instruction but regardless of how well CHT can reason the ultimate problem is that this won't scale past a few hundred samples because the context length of any large language model is limited so you can't fit 500 or a th000 or 10,000 survey responses in their context window you can't just upload this to chat PT or Claud that's kind of what we're talking about with scalability which brings us to solution number two and so there are these things called text embeddings which are basically meaningful numerical representations of text the basic idea just visualized here is let's say we have these survey responses so we have people's responses to the question what's your dream outcome for this course or this product or whatever it is and we have these open-ended responses text embeddings will take these text responses and translate them into a numerical representation which we we could visualize on a plot like this and what's valuable about this is that now that we have translated these responses into numbers we have a wide range of statistical tools and analytical tools that we can apply to that to do some kind of analysis so let's say we want to do a clustering analysis to generate segments based on these responses now we've got these clusters appearing here and then if we were to look at this red cluster here we see that these responses are saying things like I want to land a program manager job working with AI product or AI program this other person is saying they want to land a new job so it's like okay so these are like the job Seekers here then we look at this green cluster and it's people trying to build stuff so build GPT to access functions and apis Hands-On fine tuning with Laura this one's saying I love to apply AI solution to existing and future backend projects so these are more like the builders the developers and then we have this blue cluster which is somewhere in the middle and it's people trying to build products or let's see train and build a model to have some practical usage or put AI into practice for Real Business Solutions so these people aren't just interested in building stuff for fun but they're interested in building stuff to make an impact so now we're starting to see patterns in this unstructured data of these free flowing text responses so that kind of brings us to the solution I'm going to talk about in this talk which is using these embeddings to do customer segmentation and this is actually a really simple four-step process so first we'll just import our survey data using Python and we'll use a library called pandas to do that we'll use open ai's API to compute embeddings so to do this translation from text into numbers then we'll use psychic learn to Cluster the responses and then finally we can visualize the results and see the Clusters the code for this example is freely available on this GitHub repo I'll copy this and I'll drop it in the chat in case people wanted to download it or follow along and we can just look at it together here so if we go to this lightning lesson folder we'll see the example notebook we'll see the requirements and then there's some instructions on how to get this running on your machine so here's the notebook and I'm just using Jupiter lab to run this and if you follow these instructions here on the GitHub repo it'll load something like this in your browser the first step in writing any kind of python code is to import some handy libraries here are the libraries we're going to use here so pandas is going to allow us to import the data and do some data manipulation map plot lib is just a standard library for data visualization we're using sklearn to do dimensionality reduction so this goes to your question Andre because these text embeddings represent text via very high dimensional vectors but the problem is we can't really visualize a thousand dimensions on a computer screen or in any way so we can use this approach called PCA to translate those 1,000 Dimensions into just two so it will fit on our Monitor and then finally we're we're going to use K means to do the clustering and then numai is just a standard library for doing some math but then this is a key one so for generating the embeddings we're going to use open ai's API this how you import their Library you're going to need a open AI secret key to do this their API is paid so unlike Chad PT which you can access for free to use their API you need API key and it has some cost associated with it but generating embeddings is actually very very cheap I don't think I spent more than a cent on this example and then we're just going to set up the connection with open AI API using this line of code here so now that we've got all our libraries imported we're just going to loan in our data so we can take a look at that real quick so if we go to this data folder and click survey. CSV see have this CSV file of job titles and then join reasons and it seems like there are some patterns just by skimming it it's like okay we see founder founder founder staff scientist data scientist data scientist DP CTO CEO CTO so just skimming it we see these patterns but of course maybe it's not a big deal for I guess you know 83 is a lot so maybe you don't want to read through 83 job titles and join reasons but you could do it if you were motivated let's say if this was 800 samples there's no way you're going to go through and read all these and be able to do some kind of meaningful analysis with it we can take this CSV file and import it into python using this line of code and then it's just going to create this data frame object this is what the data frame looks like like so we've got the job titles here and then we have the join reasons but one thing that we'll run into is that we see that originally the data frame has 83 rows and two columns but the issue is there are some missing values like for example there's no job title or join reason for that person this person doesn't have a join reason and these three people don't have join reasons so we have missing data and there isn't like an obvious way to fill in that data that won't skew the analysis so in this case I'm just going to go ahead and drop all the missing values with the assumption that basically these values are missing for some random reason it looks the same after we dropped the missing values but we can see that it went from 83 rows down to 66 rows so we lost a fair amount of data so again even though this is like a small data set here this code will work just as well if we scaled it up by a factor of 10 or a factor of 100 that's the upside of using python for a problem like this because it's just going to scale automatically you just got to change this input CSV file the next step we're going to do is to compute the embeddings using open ai's API you can actually control the embedding Dimension so out of the box I think it has 3,000 Dimensions that you can use so here we're going to just use uh 12 dimensions for the job titles and then we're going to use 24 dimensions for the join reasons basically What's Happening Here is we're going to embed all of these job titles and all of these join reasons separately so to do that is pretty straightforward we just make this API call we're we're going to pass in all the job titles from our data frame we're going to specify the embedding model we want to use so there are two options there's a big model and there's a small model so we're using the small one and then we're going to set the embedding Dimension so here I'm just going to use a 12 dimensional Vector so you can imagine we put in the job title and then we get 12 numbers out for each job title and then we'll do a similar thing for the join reason what the only difference is instead of using 12 Dimensions we're going to use 24 Dimensions which is what's defined up here then we're going to do what we're going to do is create a list with all the embeddings there's like a lot going on here so let me just explain it so we're creating this embedding list and then what we're doing is we're taking this job embedding response let me just print it out I think that's going to be easier job response is what the API spits back out to us and then we can access this data attribute of that class so if we do that we see we have this embedding object and it has these 12 numbers and then what we can do is extract the embeddings from this object object and so it's going to organize everything into a nice list we can do a similar thing for the response reason so basically you can just add lists together like this in Python so now we have 36 numbers which are the 12 numbers of the jaob response the 24 numbers of the response reason and then we're combining all that into a list and then we're going to do that for every single Row in our data frame so this embedding list at the end of it should be a list of lists so it'll be 66 lists corresponding to each Row in our data frame and then each list will have 36 elements okay so all I'm doing here is translating this from a list into a data frame so we can actually see what that looks like so now everything's just organized into a data frame we have the job embeddings as the first 12 columns and then the final 24 columns are corresponding to the join reason just taking a step back what we've done here is we've taken this representation of the survey responses which was just two columns of text we've translated it into 36 Columns of numbers and so now we can do the clustering here I'm going to just do five segments and we can play around with different segment options just to see what it looks like and then with one line of code we can do the clustering process using K Mains so we have the segments this another option for clustering it's called DB scan it's a bit more sophisticated so you can look into that if you're curious but I'm just going to stick with K means here and then we can do PCA to do that dimensionality reduction so again we've got 36 numbers here so we're talking about like a 36 dimensional Vector if we're trying to visualize it which we can't so let's reduce it to 2D we can do that with one line of code using PCA and then basically I'm just going to go through each of our segments so we have five segments here and I'm going to plot a scatter plot for each segment using a different color this is what the result looks like so it's a lot more messy than that toy example I showed on the slides but we see that there's some patterns here so we have like segment one tends to be over here segment two tends to be over here segment 3 is over here segment four and segment 5 okay we did the clustering we made a pretty plot but what does that actually tell us like what is segment one what is segment two how do we interpret this going one step further what we can do is we can take these clustering results and basically go back to our original data frame look at all the rows for each cluster so for example we're going to look at segment number one we're going to return all the rows of our original data frame that correspond to segment one and then we're going to extract the job title and join reason and then what we're going to do is we're going to take that data and pass it to GPT 40 mini I'm using this prompt template to do that so basically just having it adopt a Persona like you are a business strategist specializing in customer segmentation and on and on and on so basically I'm telling it to take these survey responses and generate a job title desired outcome and key challenge for each segment and the there's some additional instructions here I wrote this function called generate Avatar which will take in our secret key it'll take in a data frame of all the text responses of a particular segment it's going to translate it into a markdown table so it's going to take this data frame and make it a text representation so we can give it to GPT 40 we're going to pass this table into our prompt template which is what this thing is here and then finally we'll just make the API call so now instead of using that embeddings endpoint now we're using the chat completions endpoint so we're going to call GPT 40 mini and then we'll just pass it a single message and then we're going to extract the response the result of all that is for each segment we have a clear interpretable description so segment one corresponds to Founders their desired outcomes are to gain practical skills to develop and fine-tune AI applications particularly in llm and related Technologies key challenges limited time to fully engage with code examples and Implement projects in various Cloud environments got Solutions architect gain practical skills and knowledge in AI to effectively Implement and manage AI projects in their respective Fields need to bridge the gap between theoretical understanding of AI and practical application in real world settings then we got product managers Founders again data scientists and so on now we're starting to get a better idea of what these segments actually mean and the different types of customers that are enrolling in this boot camp and then we can even go one step further and look at specific segments so segment two sounded a little Broad and I don't remember seeing that many solutions architects in the CSV file so we can kind of double click into that and we can see all the different survey responses in segment two and similarly we can do that for you know segment one so Founders teacher and a CEO let's see segment five are data scientists yeah so data scientist ml engineer data scientist head of Data Solutions and so on so forth okay so that's basically it I guess I went a little crazy on time so I will do Q&A and I will stay 15 minutes after if you guys have questions but I'll just call out the AI Builder boot camp a cohort starting on February 6th Thursday session same time as the lightning lesson over Zoom six sessions over 6 weeks and it's about 12 hours total of content as we saw in the example these are the people that seem to be getting the most value from it so technical professionals Business Leaders and product managers and entrepreneurs and I'll just call out that the course is $700 but as a special promo for being part of the lightning lesson doing a 25% discount so $175 off you can just use this code lightning 25 to get that so with that we can jump to Q&A I know we're out of time but happy to answer questions if you guys got them John asks is this method specific to Cluster analysis of qualitative data versus quantitative data no so you can actually use both qualitative and quantitative data in this analysis so that's like the upside of using text embeddings is that you can take qualitative data translate it into numbers into quantities and then just do all your analysis what if we are business architect and don't know python will this course help so that's a good question 50% of the course is python examples building writing code things like that surprisingly a segment of people and we can actually see it here there are segment of people who are not technical at all so less than one year of experience in programming but they still seem to get value out of the course and specifically what I do after each lecture is I'll stay on 30 minutes after and then we'll just have these like open discussions and I've been told that the non-coders and non-technical people really enjoy that that post discussion to talk about things at a high level and I've also been told that even just like sitting through these lectures and watching me build these projects and maybe building things on their own too it helps bridge the gap between the business goals and the engineering side of things one product manager was telling me that she can finally understand her Engineers after taking the course Katherine asked to get the most out of the course would it be helpful to have some foundational experience maybe do a learning session in a language yeah I think it would be helpful to have some basic idea of python I would recommend like one year of experience in programming is helpful because if you're trying to simultaneously like crack programming and how it works and trying to understand AI simultaneously it might be a lot like a bit of a learning curve and there are some resources I can recommend for learning python like very beginner friendly resources for that yeah deep learning AI that's a really good one they got a ton of free courses they've got a python course there as well okay so we got a couple people asking for so I've got this like short course here so I'll sh this in the chat and then if we go down here there's this mini course on deep learning AI that I think is super helpful so this one's much longer it's 4 hours but you're excited to get into Python and start writing stuff I think this is a great place to start Dar asked is it only python we need yeah so all the code examples are in Python and that's the industry standard when it comes to AI development"
W4s6b2ZM6kI,2025-01-31T22:38:22.000000,Fine-tuning Multimodal Embeddings on Custom Text-Image Pairs,n/a
hOLBrIjRAj4,2025-01-22T21:25:16.000000,Fine-Tuning Text Embeddings For Domain-specific Search (w/ Python),n/a
V1BR2tb_e8g,2025-01-13T21:10:47.000000,My AI Development Setup (From Scratch),n/a
CVcpOjwWDlw,2025-01-09T15:30:11.000000,The Problem With AI (in 2025) #ai #software,the problem with AI these days is that it means a lot of different things to different people one thing that helps give me a lot of clarity in this rapidly evolving time is Andre Kathy's framework for thinking about AI here's a basic breakdown in an old blog post he talked about software 1.0 this is the original version of AI which consisted of rules-based systems such as traditional software and expert systems then later came software 2.0 rather than computers being explicitly programmed by computers they're programmed by example in other words using data this gives us things like machine learning and deep learning but today we're seeing a new type of AI that we can call software 3.0 the Paradigm now is we can build on top of general purpose Foundation models like GPT or Claude or llama and create programs for specific use cases thinking about AI in terms of these three types also begs the following question what will software 4.0 look like while there's really no way to predict what this will be there are two patterns that run through the three different types that we've seen so far the first pattern is that the models get bigger and bigger with each iteration we started with rules-based systems which might consist of a handful of parameters and now we have Foundation models with up to trillions of parameters the second pattern is that the instructions that we have to give to computers are getting less and less precise so bringing these two things together then speculating about software 4.0 we might expect even more massive models that can basically solve problems without guidance
lIuC0IVTeXI,2024-12-30T17:26:44.000000,Getting a Data Science Job Was Hard Until I Learned These 5 Things #datascience #resumetips,n/a
R5WXaxmb6m4,2024-12-26T20:39:24.000000,How to Build a Resume Optimizer with AI (Code Walkthrough),"two weeks ago I hosted a workshop where I built an AI powered resumé optimization tool from start to finish this turned out to be a very popular topic with 870 people signing up for the event given this big response I wanted to share the recording here on YouTube for those who might have missed it I don't typically post Project Specific videos like this so if this is something you want to see more of let me know in the comments and without any further Ado here's the recording so the whole is of this talk is that I posted a Blog last month and it was like five AI projects you could build this weekend and this was the first one and it was kind of like oh this would be like a fun beginner friendly project and saw it as a throwaway but for whatever reason people really gravitated toward this project I decided to turn it into a lightning lesson because so many people were asking me how to implement this project specifically and so just for a little background you know maybe you saw this lightning lesson the topic sounded interesting but you're wondering like who the heck is this guy why should I listen to him it's a really good question so I got into AI about six years ago I was a AI researcher while I was getting my physics PhD and maybe with the recent Nobel and physics going to AI researchers that connection makes a little bit more sense today than maybe it did last year afterward I got my first job in corporate I was working at Toyota Financial Services as a data scientist so that was really cool to not only get that academic view of AI but also to see what it looks like in a practical business setting and then after spending about a year there I decided to go out on my own and pursue entrepreneur ship which is mainly consisted of doing some independent AI Consulting work have helped over 100 clients in my first year of doing that it was also like content creation so maybe you've heard about me through my YouTube channel or my blog posts on medium more recently I just launched a course on Maven and also been pursuing product development so I actually launched my first product this month called why toe which converts YouTube videos into blog posts and basically the under the hood of that product is very similar to the type of approach we're going to take in this lightning lesson so let's just dive right into it the whole motivation of this project is adapting your resume to different job descriptions is quite tedious and I see one person saying people are unemployed in the chat here so maybe this is a very practical project for you indeed and another thing is that even if you have optimized your resume say for like a data scientist role or like an ml engineer role or whatever role it might be the same job title can have different definition misss across different companies so you're a couple pretty comparable data scientist roles at Netflix and Google respectively but if we kind of double click into what they're looking for they're just slight differences in the job descriptions and what the roles look like at these specific companies even if you have that data scientist resume kind of going in reading the descriptions and optimizing it specifically for that description might be a little tedious or maybe you want to Branch out maybe you work as a data scientist now and you want to go to ml engineering or now we have these AI engineer roles so you want to explore those and you have the right skills but you just don't know how to pick out your experience to align it with the job description and you might be thinking you know can I use chat GPT for something like this and the answer is yes you can definitely use chat PT for this in fact I would recommend starting with chat PT when building any kind of project like the one we're doing here however there are some key limitations with using these AI Tools in this web interface like chat or Claude or whatever your favorite AI assistant is one of which is that having these long prompts is quite tedious so if I were to write out this really well-crafted prompt and put it into chat GPT I would have to like go in every single time and paste in the job description and it could just be a little tedious to do this but maybe that's not too bad you know you don't mind going in it's just like 10 jobs you'll just go in copy paste the job description into the prompt but you know if you say you have like 20 job descriptions this can get quite tedious so like the bulk processing of this task of resume rewriting may not be so convenient the solution to this is we can automate this whole process using Python and open ai's python API this is actually a pretty broad strategy and a wide range of use cases will follow the broad steps that we're going to follow in this specific example so instead of just doing everything in chat GPT what this will look like is you'll take your resume you'll take whatever job description you like you'll pass it into your python script which will make the API call to open Ai and then out will come your new resume just to break this down into a flowchart this is the basic steps that we're going to want to follow when implementing this project so the first step is we're going to need to import the resume and the job description that we want to adapt it to next we're going to want to construct a prompt and we'll see how we can do that efficiently in Python and then we're going to take that prompt and send it to open aai via their API just like we would if we were to type it into chat GPT and then finally we'll want to display and save the results the example code for this is freely available on this GitHub repository it's AI Builders boot camp 2 I'll send you a link to this afterward or if someone wants to share it in the chat that would be much appreciated as well and then with that we can just jump into the example code so let's just walk through this we're going to implement those four basic steps that I mentioned on the slides so we're going to import a handful of python libraries the key one here is this open AI library and then I have a couple other libraries down here which will actually allow us to take the output from GPT 40 mini and we can export it as a PDF so this is actually going to give you a PDF resume that you can submit directly to a potential employer our very first step is to import the the resume and job description so we can do that pretty easily in Python so this is how you can read files into python so I have my resume saved as a MD file which stands for markdown and we can actually take a look at what that looks like so if we go in here we see that we have something that looks like a resume but it has all this weird like syntax and these hashtags and asteris and bullet points basically markdown is a language for formatting text and and one of the upsides of using markdown is that it can allow you to have a single representation of a text document but then dynamically restyle it as needed you know kind of customize The Styling Downstream if you're not familiar with markdown I would recommend learning it because it's actually something that's really handy to know when prompting large language models we can actually display this markdown text in Jupiter lab directly and that's using this display markdown function that we imported earlier and so this is what my resume looks like you can see that we have links here my education text skills different work experience so I kind of just dumped a wide range of experiences in here and then just some other things like Awards and honors and Publications and things like that the next thing we want to do is to import the job description so I already imported one from Netflix but we can actually rerun this and so what I did here is I'm using this input function from python which will actually stop running the script until that point and it will wait to get a user input what we can do is go and grab any job description we like so we see data scientist research YouTube data scientist at Google and this actually sounds kind of perfect for me I've done research I'm posting on YouTube and a data scientist so let's just go ahead and copy this job and we'll come back over here I'll paste it into here and now we can see that text is now represented as this JD string variable in in Python so now that we have both the resumé and job description imported we can now construct The Prompt that we're going to pass into gpg 40 mini here I'm defining this prompt template and it's just like a regular prompt you're going to pass into chat GPT but there's one interesting thing that we can do in Python which is actually very powerful so we have these curly braces and I'm actually inserting these two variables with in these curly braces and so what that's telling python to do is to take these string variables that we've defined earlier and to dynamically insert them into the prompt and I'm doing this as a so-called Lambda function what that means is that this prompt template will actually be a function which we can pass in any resumé string and any job description string and it will automatically craft this entire prompt for us so we can see that in action here I'm creating a prompt using this prompt template function so it looks like this and then we can actually print this prompt to see what it looks like okay so it's like a big monstrosity let me actually do the print function it'll print it out more nicely it's a very long prompt but it has my resume here and it's inputting that markdown file and then we can see the job description and it's inputting the job description here and I guess before moving on it's worth just briefly discussing The Prompt and where this came from my general strategy for writing prompts from scratch is basically to ask chat GPT or any large language model to write the prompt for me so for this one I just went to chat GPT and I asked it please write me a prompt for GPT 40 mini that takes in my resume and a job description and rewrites it for that particular job and kind of the key intuition of why that's helpful is that language models think think a little differently than we do and so it's better to have the language model write a prompt that's going to eventually be given to another language model because it will frame the problem or the task in a way that makes sense for another language model and another thing is that these prompts are so long I don't want to meticulously sit through and write it I'd rather have chat PT just quickly write me something in like 30 seconds and then I can go in and modify it as needed that's exactly what what I did here so I started with this and I generated a few outputs and then basically I would just tweak it if I didn't like how the output came out so an example of it is that originally Chad GPT didn't write these two bullet points here under the guidelines which was to limit work experience to the two or three most relevant roles and to limit bullet points to the two or three most relevant so I added that in because I don't want the resume to be too long I want it to all fit on one page but everything else basically came directly from from chat GPT another thing we'll notice is that it's added in this additional suggestions so not only is it going to write out the resume and optimize it for that job description if there are any gaps in the resume and the job description it's going to give suggestions for how we can fill in those gaps to make our application more compelling for that particular job so that's just like a practical thing for solving this specific problem and then see that it's going to Output it in a clean markdown format now that we have our prompt ready to go we can just send it to open AI just like we would if we were to typed into chat te one of the upsides of using the API other than you could write a python script to automate this for like a 100 resumés pretty easily is that you can get access to these other parameters for customizing the model's response so one of the key ones to know about is temperature which essentially controls the we can say randomness of the output response so in other words a very high temperature will result in responses that are basically like nonsense and unrelated to the input prompt and temperatures that are very low will correspond to very predictable and responses with high regularity a temperature of 0.7 is a nice Baseline to start out with because it'll generate relevant responses but it won't be so boring and predictable and then we'll just pass in our prompt like this so we can also Define a system message which basically sets the tone or the general instructions to the model that we're prompting and then we're going to send it the prompt like this so we're basically just inputting the prompt that we just made right here the last thing I'll mention is that here we're using GPT 40 mini rather than GPT 40 and the reason is that a lot of times for simple tasks you don't need to use GPT 40 and GPT 4 mini will do a well enough job and it's also like 20 times cheaper to use gbt 40 mini so I always recommend to start with a smaller cheaper model refine the use case refine your prompt refine how you're formatting the text and all that and then if you need to go up to a bigger model you can do that later on okay so just with this single function call we can get the response from GPT 40 mini we can extract the response like this so all that's Happening Here is that this response is like a broader object you can actually have it return multiple choices so like multiple outputs so you could like set the number of responses to like three and instead of just one resumé it'll spit out three résumés and you can pick whichever one you like but here I just generated one and I'm going to pick that one out and then we can extract the message which is like this messages attribute here and the content of the message like this and then we're left with this response string just briefly taking a look at that we can print that out and we can see that the output looks something like this so it looks just like the input markdown file that we saw here but there are some tweaks so one it split my technical skills into three sections rather than just two so before I just had tools and certifications now there are programming languages tools and Technology certifications work experience indeed it did limit it to three rather than the let's see how many I put here 1 2 3 4 5 6 instead of the six I put here and then it truncated the Publications from 11 down to 2 so we can see that it made it more concise and more relevant for this role let's take a closer look at it it didn't mention anything like any of my YouTube experience so maybe it didn't it didn't deem that relevant it just focused on the data science roles and the research roles that I have heeld but I feel like adding in something about YouTube might help this application as well but then we can actually SC scroll down and we can see the model also has these additional suggestions so let's see is to add experimentation design AB testing I might read this and be like oh I've actually done this um in a past project you know I did a whole blog series on causal inference so maybe I'll add that in because it's something that I've done it just wasn't something that was in my resume uh let's see strong emphasis on statistical techniques like regressional analysis hypothesis testing so this was something I did at um all my different roles like so as a research assistant and at Toyota so I can just add in some specific uh instances on that to make it more aligned and then familiarity with data visualization tools Beyond Tableau so I haven't used powerbi or d3js so I there's really nothing I can do there you know these are just like helpful suggestions for making the resumé more compelling and then what we can do next is to display the results here what I'm going to do is I'm taking this output string here and I'm going to split it into two parts basically everything before this additional suggestions bit and then everything after this additional suggestions bit and then I'm going to print them out if we print out the first bit which is the updated resume it'll look something like this it looks similar to my original rume with the appropriate tweaks and then now that we have this markdown version of the new resumé we can actually save it as a PDF the way I'm doing that is using this going all the way back up using this Wheezy print python Library as well as this marked down Library so let me just explain what's happening here so first we're defining the name of the file we want to export the new rum as so I'm just going to call it rum new.pdf and then we can take the raw markdown text that came from GPT 40 mini and we can convert it to HTML using the this markdown Library that's what this line of code is doing it'll take this markdown text and convert it into HTML and then using that Wheezy print Library we can export this HTML content as a PDF I just uh have this style sheet as well which will customize the styling of the PDF so it doesn't look so ugly and actually the the Styles here is something I asked Chach BT to generate for me because I'm not a web developer I don't know CSS but I know enough to modify and hack the code if I have a file in front of me asking Chad P to do this is a really handy trick so we can just see what that looks like it's just a regular CSS file and it's just setting the font sizes the font family the size of the headers size of the margins and all that boring stuff so I can pull up the resume actually and so this is what the output resume looks like it's a little longer than one page which should be fine I mean even we could we could drop the second page and it wouldn't be such a big deal and we can see this looks pretty nice you know we can click on these links and we can can see that they indeed do work you can click on this and yep it'll take me to my LinkedIn page so the links work and then we have everything that was laid out there okay so let's go back to the script so that's how we export it as a PDF we can also save the markdown file if for whatever reason we didn't like how this PDF was formatted and we wanted to go online and find a markdown reader or something like that and then we can paste in the new resumés markdown file into that converter and then finally we can just print out the suggestions from GPT 40 mini and that's what this looks like so we're doing pretty good good on time let me just close all these out and then I'm going to just skim the chat for questions Shi ask can the prompts generated by one model be used as an input to another model yeah you can definitely do that I haven't robustly tested this and done experiments on it but my intuition is that if you want to say write a PRP for GPT 40 mini it might be a good idea to use gpg 40 mini or gpg 40 and my intuition there is that it probably has a better understanding of itself to put it one way but you can use any model I actually used on my first iteration of this I used llama 3.2 to write this prompt because chat PT was down for whatever reason yes the recording will be shared so don't worry if you join late does the CLA API work in the same way I haven't used claw's API but I suspect it's very similar maybe someone with more experience with cla's API can correct me if I'm wrong would it be possible to save the styling of the input resume PDF and use it to generate the output one oh that's a really interesting question what you could do is take your current PDF like go to chat or whatever AI assistant you like upload the PDF and just ask it to write you a CSS file to copy the styling of that PDF and while it probably won't be perfect it'll be a good starting point okay which editor are you using this is Jupiter lab and you can install it using the requirements file great question Christian is there a way to have a simple interface to use it that is the next example so let's keep going on with that so here we walk through the code and now we understand like the guts of this but this isn't really practical this is kind of just as bad as copy pasting it chat PT maybe it's slightly easier but we still have to like go find the resume input it here run the whole notebook and it's like this whole thing so let's just create like a simple user interface to run this whole process I really like gradio for simple web apps like this and it's just a python library for making simple ml apps this one cell is everything you need to create a gradio application so let me just like show you what it looks like and we can walk through it here we have a title and a brief description we have a place where we can upload my resume and paste the job description it'll optimize the resume and then everything will be exported down here so let me find my resume so we can take the resume just paste it over here we can you can go find a job description let's do a new one oh yeah let's go to Black Rock they are quite successful and well known so let's copy all their qualifications and then we'll paste it right into here and then we can click this optimize resume button so one of the reasons it's taking so long is we've got a long resume and quite a long job description but eventually it got it so here's what the new resume looks like so Black Rock private Equity company is probably going to highlight the work at Toyota Financial cuz there's probably a lot of overlap so it used a very similar work experience you know maybe I would be better off including more bullet points in my original resume maybe like 5 to 10 bullet points under each job and then having the model pick out the ones that are most relevant but what's really nice about this is that we can go to this additional suggestions area and we can see that there's indeed some opportunities to improve it so like add skills like front and Frameworks like JavaScript and streamlet you know while I haven't used these maybe I can mention my experience with radio and link some projects as well CU stream lit and gradio are very similar knowledge of gcp or Asher so I have used gcp on a project so I can mention that project in there and then you know just kind of updating it there and actually I've created this little UI here to update the resume in a pretty simple way so like taking a look at it we can see that Ive never used R shiny or flask I don't know where I got that crazy IDE from so I can actually just delete that and then maybe I'll add like gradio and then I'll add gcp as well and then let's see we'll leave all that the same and then we can just click this export as resume button and we can see it was successfully exported and then if we go here bring the resume over and we can see that indeed it made the changes and exported the resume maybe if we don't want to have this second page here we can go back and just delete this and then reexport and we can see now it's just one page here I like radio because you can like pretty easily make these simple user interfaces and it just makes it like slightly more convenient to interact with these things and so I'll just briefly walk through it won't spend too much time on this but happy to answer specific questions if you have it basically you can imagine the way gradio works is we're building the web page from top to bottom so at the very top we have the title which is this thing here and this description which is this thing here and these are these markdown blocks so we input it in the markdown syntax one hashtag is like a title No hashtag is like body text then we can have a section where we gather all the inputs so this with rows thing will basically allow us to arrange elements horizontally so that's exactly what we're doing here so we're arranging this file upload and paste job descriptions in a single row and then I have this button here to optimize the resume the way these buttons work is that they essentially represent functions we take this run button which we defined up here and then we have this click method which allows us to connect the click of this button to a function the function here is called process resume it's basically what we defined in the first notebook but it's a userdefined function that will take in the resume job description and send it into chat gbt but I've just abstracted it into a single function call and then these are the inputs of that function and then we're saving the outputs as these variables here by clicking on this button it's going to grab rume input and JD so ré input JD input and it's going to spit out output ré MD output resume and output suggestions which we're using here here and here below this button we have these outputs so this is the markdown version of the new resume this is a markdown version of the suggestions and then we have this text cell which allows us to edit the new resume so this is the output resume in markdown format these are the suggestions in a markdown format and then we have this text box which we can edit that's what this interactive true allows us to do which we can make the changes that we like and then we can have a button to export it as PDF and then if the export is successful we just have this export results variable get created and then we just launch the app like this happy to answer specific questions about the gradio interface or anything on the notebook as well can you connect an external CSS file to gradio page that's a good question I'm sure you can because you can actually have custom HTML blocks in gradio as well I just haven't done that before but I'm pretty sure that's something you can do okay so I think we're doing pretty good on time let's keep going along here so I'll do a Q&A up until the hour and I'll even stick around for like an extra 30 minutes after the hour if people have more questions but I'll just call out the AI Builders boot camp we're in cohort 1 now but cohort 2 is coming up on January 10th and it's going to run till Fe 21st the main premise of the boot camp is to learn AI through building it through practical examples so kind of like the example we built here along the way we had to learn a lot of Concepts we had to learn about prompt engineering learn about apis learn a little bit about CSS learn about markdown learn about HTML you pick up a lot of skills through building and ultimately you know you can learn Concepts and take courses all day but really the truth is in the code as they say you know when it comes to actually taking these ideas and implementing them into reality it takes code to do that so I think it's just one of the best ways to learn here's a quick overview of what the course includes we'll build a bunch of projects and then there'll be homework where you can build your own projects and I'll give you feedback and you can get feedback from others in the cohort it seems like three types of people have taken cohort one and seem to get value out of it so first are like techical professionals whether you're coming from it or software development or maybe you work as a data scientist or data analyst but but llms and this new wave of gen Ai and these new AI models are new and unfamiliar those types of people you know Business Leaders and product managers you know maybe you want to lead AI initiatives and you know bring AI to your org or maybe you're like a engineering VP or like a head of engineering head of data but you haven't coded in like five or 10 years and you want to keep up with this rapidly evolving AI landscape and then finally entrepreneurs you know people trying to build products with the these new AI tools the cohort is $700 but as a lightning lesson promo and thank you you can use this code lightning 40 and you'll get 40% discount saving almost $300 on cohort 2 and of course this is a huge investment I you know I'm aware aware of that like $420 even is fair amount of money for a lot of people so there's still a ton of free content on my YouTube channel on my medium blog and the example codes from these cohorts are made freely available on GitHub so there's something for everyone depending on what your goals are and what you need with that let's do Q&A so we'll do 15 minutes of like official Q&A but I'll also be happy to stick around for like 30 minutes after we end here any ideas as to what types of projects we are building in the course yeah so that's a good question let me go to the GitHub AI Builders boot camp one so this will give you a flavor this is the GitHub for the current cohort and you can go through and take a look at all the different projects that we've built and these are just the class projects people have been building really interesting projects as their homework optional homework and they share the example code with everyone so it's not just these examples that you can learn from you can also learn from fellow cohort members are there any early bird offers for the course so right now the 40% discount is the U the main offer right now any suggestions for those who don't know python should I learn P python if so where there's a really good question although you can go really far with like the no code tools and like clads UI or Chach UI or something like that I still think if you're trying to build custom Solutions like you're trying to build products or you're trying to build projects learning python is still important and it also allows you to use these models in more sophisticated and advanced ways so I think you should definitely Learn Python I have a python intro on my YouTube channel maybe I can find it real quick um this is like a great entry point for people here we go mini course for getting started and so I have the same philosophy of like learning python as I do with learning AI which is like the best way to learn is by building projects so you know just learn just enough to start coding or start building specific projects because you're going to learn a lot do you have to be technical to attend this boot camp can a product manager yes so this is meant to be accessible to product managers non-technical people as well as technical people so like the first half of the course is probably review for a lot of technical folks but for the people who are new to Ai and machine learning it'll be a lot of good foundational information I think actually like 60% or 70% of people in cohort 1 have less than one year of machine learning experience how much coding experience is recommended for the AI buil boot camp I would say one year is good because if you know absolutely nothing about Python and this is relevant to punam's question as well if you know nothing about python there's going to be a bit of a learning curve to just like get it set up on your machine and start coding of projects so I'd say like one year helps a lot but really the coding is optional you know some people they get value from just coming the lectures hearing the discussion and then also we'll do these like post lecture discussions which isn't like me lecturing but it's just everyone having like free flowing discussion and then the non-technical people seem to get a lot of value out of those more free flowing broad discussions have you or anyone else seen improvements in round one calls by tailoring resumés to job descriptions that's a good question I haven't tried that but it's definitely worth trying out you know while this is going to be a helpful tool for people trying to find jobs or whatever it is it's still like a pretty nent project like if you want to turn this into like a product or something like that more work is going to need to be done examples are fine for personal use cases what is taught in the boot camp to cover Enterprise use cases that include guard rail security integrating production grade applications and scalability so that's a really good question because these projects are great entry entry point you know great to learn the concepts and build out these like simple prototypes but when it comes to like building an app that is going to have like a 100,000 users or something like that or like a million users and this is relevant for companies where they have wellestablished products but maybe they want to build out these AI features the current version of the course we don't cover guard rails or deploying things because there's such a wide range of ways to deploy apps especially in AI there's just really not practical way for me to cover it in class that's applicable to a wide range of people and a wide range of needs but I plan on incorporating like evals and guard rails and these other things in future cohorts that's another thing that's worth calling out it's still the early days of this course so I'm very much learning from the students in what it is that they're interested in learning and then I'm going in working and trying to put that content together and make it accessible for people can a generated CV be automaic Ally imported to a CD template or word doc that's not something I tried but that's a really good question there are tools like pandoc and other like converter tools so you can input markdown and convert it to like a doc x file for example so I'm sure something like that exists and is out there but it's not something that I've done personally can't we use cursor AI to do the coding after giving prompt instead of developing P script in Jupiter for sure so I use cursor a lot actually and I really enjoy it the demo here and the Demos in class I use Jupiter lap just cuz it's easier to walk through than like a dense python script but yeah you can definitely use CERN does your course cover any agentic AI Frameworks that's a good question actually someone taking the course emailed me and asked if we could cover that just today so cohort 1 we didn't cover AI agents but I plan on incorporating that into cohorts 2 and three that's on the agenda uh can you share promo code again so the promo code is lightning 40 also I'll share the slides afterward and this is a a link that you can click on this is also a link you can click on and it'll automatically send you to the course page with the discount applied let's see in the data engineering section how deep will you go using smart nic's for instance so we don't cover smart nic's the ETL and data Engineering in the course is really the basics and fundamentals it's just enough to build basic machine learning projects and AI projects so imagine if you're familiar with like a data science role or an ml engineering role it's about as much data engineering that a data scientist or a machine learning engineer needs to know not necessarily the data engineering that a data engineer will need to know because they're usually not just interested in specific projects but they are tasked with managing the data infrastructure of an entire Enterprise or an organization so we won't cover that kind of data Engineering in ETL but we will talk about it on a project by project basis no Cube flow either I haven't used that and it's usually not necessary for oneoff projects you think we could add to this function a way to pick the model let's say clot in GPT yeah for sure let me go to the that functions. py so I have this function get resume response another thing we could do is have the ability to take in different input models and then what we could do is just have some basic logic in here that says if GPT 40 do the API call like this if Claude 3.5 Sonet or whatever the smaller one is called I think it's Hau actually so if you have CLA 3.5 Hau use this other piece of code or something like that but that's something you can easily incorporate into this function here how many tokens come soon for a single resume is there a way to print the cost of a single resume generation in the UI yeah you definitely can because open AI posts their API cost input tokens and output tokens are priced slightly differently you can get the a API inputs and the outputs count the number of tokens then you can just compute the API costs by multiplying the total number of input tokens by the input token cost multiply the number of output tokens by the output token cost and then you'll get the API cost but I'll just say for context I ran this probably a couple dozen times in developing the example and it cost me like 5 cents so generally the API costs are super cheap especially if you're using GPT 40 mini how to pass resumé template and create resume exactly similar to the template that's a good question and that's one of the upsides of using markdown since my resumé string the one that I inputed here is already in markdown the model will follow that same exact output that's just like a thing that happens automatically when you pass in your resume in the format that you like when you submit out rume to AI does our data sent to public domain is it fine okay yeah so that's a good question if you use chat gpt's user interface open AI p and likely will use your inputs for improving the model so there is a chance of data that you paste into chat gbt making its way to the public indirectly through being used to train future versions of the model however that's not the case with the API the API they do not use that data for further training that's another upside of using the API as opposed to using chat's UI do we need chat PT API key for the code you providing yes so that's a good question I'm importing my API key here so I have this python script called top secret and there's a string defined in that script called my SK and then that's my open AI API key generally if you want to use these apis you're going to need to have a API key would you consider doing another follow-up lightning round where you store resumes in a vector database and use an llm to match the best candidate that's a really good question it's very possible that next time I do a lightning lesson I can cover something like that but I can point you to the example code here if you go to AI Builders boot camp 1 go to the repo and then you can go to session 4 and look at this rag chatbot over blog series it's going to be slightly different but this will give you a sense of like how you can do that kind of system and maybe you don't even need to have llm hooked up to it you could just have like a semantic search basically where you paste in the job description and then you find the resumés which are most similar to that job description but yeah I can imagine that's a really powerful use case for hiring managers and recruiters and whatnot for participates in boot camp 2 do we need to pass and read boot camp 1 while it'll probably be helpful to kind of skim through these examples in boot camp one and look through the syllabus to make sure it aligns with your goals and what you're trying to learn it's not necessary it's not like a boot camp 1 is a prerequisite for boot camp 2 we're at the hour if anyone needs to drop all good thanks for joining I will again share the recording and the GitHub repo and the slides but if anyone wants to stick around and chat and have questions I'll I'll be here for like another 30 minutes and happy to answer questions or just have like a more General discussion about AI"
RRnuZGEhneQ,2024-12-23T17:57:13.000000,How I’d Learn AI in 2025 (if I could start over) #ai #python,here's how I'd learn AI if I could start over today I'd first familiarize myself with modern AI tools like chat gbt and clae this would give me a practical understanding of what these models can do and how to use them second I'd install python on my machine while I could go far with today's no code AI tools these can't be easily used to build custom AI Solutions or process data in bulk third I'd build my first autom using Python and an llm my Approach would be to pick a task that I frequently use chat gbt for and then try to implement it using open AI python API once I had built a few simple automations I'd next build a machine learning project this would consist of either training a model myself or doing a traditional NLP project this might be a semantic Search tool a basic rag system clustering documents based on text embeddings training a text classifier or fine-tuning a large language model finally I'd seek out a real world problem I could apply my newly developed AI skills to to do that I'd try any of the following One reach out to business owners and Professionals in my network second if I was a student I'd join a research group at my University also if I was a student I'd find an internship and then fourth and finally I'd find freelance work on upwork I talk more about my five-step Approach in a recent YouTube video
e3p9-hYxwSQ,2024-12-17T14:29:21.000000,How I’d Learn AI in 2025 (if I could start over),AI has changed a lot since I started 6 years ago while there are more learning resources available than ever figuring out where to start is a challenge in this video I'll talk about how I'd approach learning AI given what I know now and the tools available today and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship if you enjoy this content please consider clicking the Subscribe button that's a great no cost way you can support me in all the videos that I make I've worked in AI for the past 6 years I started as a researcher while getting my PhD then worked as a data scientist at Toyota and now I teach people AI while Building Products on the side although I still have a lot to learn the fstep approach I share here is based on what I've personally found helpful in navigating this space if I were to boil it all down the central principle of my Approach is to learn by doing where each step outlines a clear and specific objective through which completion will naturally develop key skills in other words rather than reviewing a list of Concepts and courses each step is a task designed to force me to learn key skills by completing it if starting from zero the first thing I would do is familiarize myself with modern AI tools like chat PT Claude or the like this is important because frequently using these tools will give me a practical understanding of what what these models can do it'll also develop my ability to use them effectively through prompting alone on a more meta level these chat interfaces are amazing tools for learning Ai and really anything else I'd use chat gbt to explain confusing buzzwords and Technical Concepts like llms tokens apis rag Etc and be sure to ask followup questions until I have a clear understanding of each idea for those that don't click I'd seek alternative resources using Google search and YouTube although I could go far with today's no code AI tools they are still fundamentally limited namely these tools can easily be used to build custom Solutions or process data in bulk that's why the next thing I would do is install python on my machine python is the industry standard programming language for AI development to get it installed I'd ask chat GPT for step-by-step instructions if I got stuck I'd go back to chat gbt explain the issue and ask it for guidance while using chadt or any other AI assistant in this way can significantly streamline this process I would still take the time to understand each key step of the process and ask follow-up questions as needed this is an important Habit to develop because it'll help avoid accumulating technical debt which I'd have to pay for later when something goes wrong once I've become comfortable with chaty BT and installed python on my machine my next step would be to build a simple automation using python my approach to generating project ideas would be to think of things I consistently use chat gbt for and try to implement it in Python this would require me to become familiar with open ai's python API so I'd start reading through their documentation and reviewing example code there once I felt comfortable with the API I'd start writing python code my first step would be to Think Through each step of my automation for example if I wanted to summarize research papers the steps might be one read the paper contents into python two construct a prompt for gp40 and three make an open AI API call if I got stuck I'd go to chat GPT for assistance for instance if I didn't know how to read PDFs into python I would ask chat GPT for help if it spits out code that I don't understand I'd ask follow-up questions until I understood each line again it is important I take this approach to coding with chat GPT because blindly copy pasting code wouldn't really teach me much it would also acre unforgiving technical debt in other words I'd get short-term gains but would have to pay for them later through technical difficulties and headaches after step three becomes easy for me I'd seek out more sophisticated prod projects rather than making chat GPT like API calls I'd buil a project that required me to use an embedding model or train a model myself for this I'd use hugging faces Transformers library and find a pre-trained model that's relevant to my project on the hugging face Hub potential project ideas might be things like a semantic Search tool a basic rag chatbot clustering documents based on similarity training a text classifier using embeddings or fine-tuning a large language model for example if I went with the rag project I'd first educate myself about Rag by watching YouTube videos then I'd break down my system's basic components and the steps to implement it finally I'd start coding the project using Chachi as a co-pilot like in step three although I would have learned a lot about the technical side of AI through doing projects in steps three and four when it comes to generating value with AI this is not enough for that I need to use what I learned to solve real world problems there are two ways to do this I could one solve my own problems or two solve someone else's problems since I hopefully already did the former in steps three and four here are a few ways I'd approach the latter one I'd reach out to business owners and Professionals in my network two if I was a student I'd join a research group at my University three I'd look for an internship again if I was a student or four I'd find a freelance gig on upwork let's say I'd already graduated college and wasn't quite confident enough to start looking for freelance gigs that leaves me with option one for this I'd start by making a list of people I could reach out to Ideal contacts would be small business owners or professionals working at small to mediumsized businesses then I would craft a message template and reach out to everyone on my list through Linkedin DMS or email if I struggle to find the right wording I'd again turn to chat GPT for help although AI involves an interdisciplinary set of technical skills and knowledge with today's tools and resources it's never been more accessible that said it's important to remember that learning itself is hard you will get confused you will get frustrated and you will wonder why you're putting yourself through this struggle however if you are patient and persistent you will be rewarded with Clarity if you have questions or you want feedback on project ideas feel free to share them in the comments and as always thank you so much for your time and thanks for watching
rSHwqTD-FcY,2024-12-16T15:10:56.000000,3 Ways to Make LLMs Multimodal #ai #multimodalai #llm,are three ways you can pass images audio or any other type of data to a large language model first is to Simply use pre-built tools to process multimodal inputs or outputs for example passing input audio into a speech to text model or generating images using a text to image model based on llm responses second is to connect together pre-trained models using so-called adapters trouble with connecting pre-trained models off the shelf is that they may not play nicely with one another one way we can better integrate these models is by using adapters in other words a small number of parameters that translate one representation to another a final way is to fuse multiple modalities at pre-training while this is the most technically and financially demanding approach it unlocks improved reasoning capabilities at lower inference costs this is presumably the secret behind state-of-the-art models like GPT 40
Y7pNmocrmi8,2024-12-07T21:41:10.000000,Multimodal RAG: A Beginner-friendly Guide (with Python Code),although language models like GPT Claude and llama gain an advanced understanding of the world they don't know everything that's because information is constantly being generated or may not be widely available a popular way to overcome this ignorance is retrieval augmented generation or rag for short in this video I'll discuss how to build multimodal rag systems in other words rag systems that can process text and non-text data I'll start by reviewing three highlevel strategies for doing this then walk through an example implementation with python code and if you're new here welcome I'm Shaw I make videos about the things I'm learning about and building in Ai and if you enjoy this content please consider clicking the Subscribe button that's a great NOC cost way you can support me in all the videos that I make so here we're going to talk about multimodal rag which will combine the ideas from the previous two videos of this series namely multimodal large language models and multimodal embeddings before we talking about multimodal rag it's worth answering the question what is rag since I talked about this in a previous video I won't get into all the nitty-gritty details but just at a high level rag stands for retrieval augmented generation and it consists of improving in llms responses by automatically providing it the relevant context just to give a simple example of when this might be necessary suppose that I'm trying to remember the name of a python library that a coworker had mentioned in a meeting from yesterday if I were to take that query and pass it to say chat GPT CLA or whatever your favorite large language model is it would probably give a response like this that says I'm sorry but I don't have specific access to the details of your me meeting there's no reason we might expect chaty PT or the like to have access to our meeting notes or transcript so this is a very reasonable way to respond to this query this is a broader limitation of large language models although they have tremendous general knowledge about the world they have no knowledge of things that occurred after their pre-training and their understanding of specialized or domain specific information may be limited rag allows us to overcome these limitations by automatically providing the relevant context to a large language model so what this might look like is we'll take that same query of what was that python library that Rachel mentioned in yesterday's meeting and before passing that query to Chad gbt I'll just take the entire transcript of the meeting and add it into my prompt and then pass it to chat GPT from this the large language model will be able to extract the desired python python Library which may have been unstructured and unstructured is indeed the name of a python library that helps parse files of various different formats which is very relevant for multimodal applications so what I just showed was a specific example of what rag might look like but this is a much broader idea and a basic overview of a rag workflow may look something like this where we start with a user query we take that query to retrieve the relevant context from a knowledge base we then will take the query and the relevant context and format it into a prompt then we'll take that prompt pass it to a large language model so it can generate a response there are many technical details here and knobs and dials that we can adjust to refine and adapt such a system for a particular use case but this view here shows all the Essential Elements if you want to explore rag more deeply you can check out my previous video on the topic so now that we've defined what rag is let's see how we can make such a system multimodal so in other words how can we build a rag system that doesn't just process Text data but that can also process non-text data a simple strategy would be to take a query like we did before then take that query to perform search over a multimodal knowledge base so these blue rectangles here are representing text information while these green rectangles are representing image data then we can in the same exact way as before construct a prompt but now instead of only using context that is text based we can use context that is also image based or Audi based or includes time series data or whatever type of data modality that you might want to work with we can then pass this into a multimodal large language model so this is a large language model that can not only process text but it can also process non-textual data and then it will generate a response for us the two key elements here that make this rag system multimodal are one it's knowledge base containing not just text based data and two is the multimodal large language model which can process multiple types of data so here I'll talk about three different strategies for building multimodal rag systems and we can view each of these different levels of multimodal rag as becoming more and more sophisticated as we go to the next level and then I'll just mention that the discussion here will assume that you already have a basic understanding of multimodal large language models as well as multimodal embedding models which were discussed in the previous two videos of this series starting with level one is to just translate everything into text hearing this your first thought might be well sha that doesn't really sound like a multimodal system it sounds like you're just translating all the different data types into text and just using a regular rag system and it sounds like that because that's exactly what's happening here and I would say anytime it's possible to implement the strategy for your use case I would go with this one and that's because you don't want to over complicate your AI system for the sake of sophistication a general law of engineering is to keep your systems as simple as possible so what this might look like is you can take text documents extract the text and split them into chunks you could take figures and tables and extract their captions and maybe some of their key insights and store them as text chunks and then you can take images and grab their captions or descriptions and store them as text chunks or you can even take images pass them through a multimodal large language model and prompt that model to just generate a descriptive caption for that image and that way you can extract the essential features from an image another example might be if you have videos or audio recordings that consist of people talking you can transcribe that speech and just store that speech into text chunks but no matter what the original modality is this process will translate everything to text and then you can store all these different text chunks and text items in a knowledge base and then you can just dynamically retrieve the most relevant chunks for a given query like a traditional rag system although this is a very simple way to do multimodo rag basically you put all the effort in the pre-processing of the data this has obvious limitations so sometimes there's not an obvious way to translate the original modality to text for example let's say you have time series data of like market indicators or you have time series data of brain activity from EEG there may not be an obvious way to fully capture the information in that signal into text and even if there is a way to do that you don't have a good way of passing that full richness of information to a large language model because the prompt here is text only so one way we can overcome that is level two so here we're going to keep the retrieval process as text only however we are going to use a multimodal large language model so what that looks like is that we will do the knowledgebase step exactly as we did in the previous slide so we'll take all our data modalities and we'll extract text features from them and store them in our knowledge base here it doesn't necessarily have to be a full-blown texal description of the underlying data it could simply be meta tags including like a title or a date or user that generated that content or whatever and then these meta tags could be used in these search process however the key difference here is that once we identify which items are most relevant to our query we will directly input the original data modality into our query and then we can process this multimodal prompt using a multimodal language model and a key detail here is that you have to ensure that the model you're using to process this query can indeed take the modalities that you're passing to it these days processing text and images is quite common however if you wanted to also process audio and video you would have to make sure that the model that you're using can do that the last General approach I'll describe for multimodal rag is to do both multimodal retrieval and use a multimodal language model and so what that will look like here is that let's say you extract text from the text documents let's say you have these PDF reports which consist of text and figures so you're going to extract both those data types and then you extract images directly then what you can do is you can use multimodal embeddings which we talked about in the previous video to generate Vector representations of all these items and store them into a multimodal knowledge base the key benefit of using multimodal embeddings here is that you'll have a shared Vector space which can represent the raw data that you want to include in your knowledge base and then we can perform search over these items directly by Computing the similarity between a vector representation of the query and all the items in the knowledge base so that was probably a bit high level and Abstract so let's see a concrete example of how we can Implement such a multimodal rag system using python so here we're going to walk through the development of a multimodal Blog question answering assistant so in other words an AI assistant that will have access to all the text and images from these two blog posts and we be able to answer any questions based on this content we'll start by importing some helpful libraries here we'll use the Transformers Library again to import clip which is a multimodal embedding model that can represent text and images in a shared Vector space and then we'll import a few different functions from pytorch okay the next step is we're going to load in the text and image data although I won't walk through the data extraction process the code for doing this is freely available on the GitHub repository link down here so if you go here and then you go to the multimodal rag example there'll be a notebook that lays out exactly how I took the blog articles and extracted all the text and extracted all the images from them I'll just load in the text content and image content which are saved as Json files also available on the GitHub and then I'll load in the multimodal embeddings which I generated using clip and then saved as a pie torch tensor and so we can just load those in like this if we print the shape of these tensors we see that the first one is 86x 512 indicating that we have 86 text chunks that are represented by 512 dimensional vectors so it's a vector in a 512 dimensional space and then similarly we have 177 images which are also represented in this 512 dimensional Vector space so now that we've imported the text and image content and their embeddings the next step is to define a query and embed that query in the same space as the text and image content so I'll Define a query that is what is Clips contrast of loss function and to embed this query we'll need to follow four steps first we'll load in clip this is a special type of clip that only processes is text so we'll load that in using the Transformers Library we'll also load in this data processor which will just handle the tokenization of this query before passing it into the model so once we have those imported then we can pre-process the text with this processor we just imported and then we can pass the resulting inputs into clip which is our model here and this will give us the outputs once we have outputs we can just use this text embeds attribute to extract the text embeddings and then if we print it shape we see it's a 1x 512 High torch tensor which is compatible with the representations we saw on the previous slide for our text and image chunks at this point we have Vector representations of our query as well as every item in our knowledge base so basically all the contents of these two blog posts now we can use these Vector representations to perform Vector search so this will consist of computing the similarity between the query embedding and all the embeddings in the knowledge base so we do this in the following way first I'll Define some parameters to constrain the search so here I'll restrict it to the top five most similar items and then have this additional similarity threshold that will exclude any items that have a similarity score below 0.05 starting with just the text items the way we can compute the similarities is by doing a matrix multiplication so what we do is we'll multiply the query embeddings with the text embeddings Matrix we just do this transpose here so the Matrix is the right shape and it's important to know here that if we do this matrix multiplication the elements of this text similarities tensor will be between negative infinity and positive Infinity which makes it hard to Define any kind of Threshold at the outset so a common strategy to rescale these similarities is applying the soft Max function to these values we can do that by first defining a temperature value we'll just take these text similarities divide it by the temperature and pass it through this softmax function from the P torch library and this will give us these text scores which are essentially similarities but instead of ranging from negative Infinity to positive Infinity these range from 0 to 1 and then once we have these scaled text scores we'll sort them here I'm doing ARG sort so this will sort the values but rather than returning the sorted values themselves it'll return the indices of the sorted values you can just imagine this is a one-dimensional tensor and the zeroth element of the tensor will be the index of the largest similarity score the next element will be the index for the second largest similarity score and so on and so forth now we can use these indices to generate a tensor of the actual ual sorted scores and then what this will allow us to do is to do this somewhat sophisticated filtering process to ensure that we're only returning the top K results that are above this predefined threshold we're using Python's list comprehension capability here just to break this down we're creating a tuple which consists of the indices of the sorted scores and the sorted scores themselves then we're going to Loop through the indices and the scores in these tupal and then then we'll extract the index if its score is greater than or equal to our predefined threshold so the result of this will be a list of indices sorted from largest to smallest score and then at the end of it we can just take the top K elements from this list so I know there's a lot of steps here but this is how we can simultaneously filter for this threshold and these topk results and then what we can do is go through our text content list this is what we imported from the Json file and then just return all the content from this I topk filtered list so after all that we are returned just a single search result and the reason is even though K was set equal to five the other four search results had similarity scores below the threshold so they were not included I want to point two things out from looking at this first is that Beyond just the text that was used to do this search there's also the article title and the section title where this text was taken from in the article and so having metadata like this is helpful for both the search functionality because it gives you more flexibility in how you do search but also it allows you to give more context to the downstream model in generating responses so this is just a good practice and then you can check out the data preparation code of how I extracted this metadata on the GitHub repo but the second thing I want to point out is that this text doesn't seem super helpful in answering our original query of what is clip's contrastive loss function indeed the section is about contrastive learning which is definitely similar to our query but just because this text here is similar to the query doesn't necessarily mean that it's helpful in answering it this is one of the main limitations of vector search which is that using these embedding models out of the box even though it allows you to identify similar Concepts it doesn't necessarily help you identify items in your knowledge base that are helpful to answering your query although there are many ways we could mitigate this issue a simple solution is to just have less stringent search parameters to do that I'll first can this whole similarity search process into a userdefined function so we don't have to just keep writing it over and over again and then I will do similarity search over the text chunks and the images separately and this way I can have different search parameters for the text and images here I have more inclusive search parameters than we saw in the previous slide so instead of k equal to 5 I have k equal to 15 the threshold went from 0.05 down to 0.01 and then I kept temperature the same and then for images I just keep k at five I have the threshold as 0.25 and I have the temperature as 0.5 so the way I determined these parameters was just through trial and error on three simple queries and just eyeball in it so that's a fast and easy way to build out an initial version of a retrieval system but if you are serious about optimizing it and improving the model's results it's going to be helpful to generate a data set of example queries and their Associated relevant items then you can do some kind of grid search to find the best choice of search parameters here the results from these two searches are shown here so we have 15 different text items returned but just one image item was returned looking at these texts this one mentions contrast of learning this one mentions clip this one mentions clip this one mentions clip so we see that yes indeed that these items are similar to our input query but it doesn't necessarily mention anything about the loss function that's not a promising result but if we look over to the image this is exactly what we're looking for so this is Clips contrastive loss function laid out mathematically now that we've retrieved our context let's see how we can put this into a prompt to pass it to a multimodal language model so the text results are stored in this list of dictionaries with all the different metadata so we're going to need to translate that data structure into a string representation so we can pass it to a language model the way I'll do that is I'll iterate through all the items in this text results and then just format it according to this code here so what this looks like in text is this this we have the article title with these Asters on either side we have the section that the text was taken from and then we have the text snippet right here and the reason I'm using these asteris is because this translates to bolded text in markdown which is a text formatting language so a trick for adding more structure to your prompts is to just write them in markdown then we can do a similar thing for the image results and so what that looks like is we have the article title the section it was taken from the path of the image and then the image caption then what we can do is we'll generate this promp template and so here we're going to combine the query and the text and image context says given the query and then we'll just insert the query here and the following relevant Snippets and then we'll just include all the search results here please provide a concise and accurate answer to the query incorporating the relevant information from the provided Snippets where possible so once we've constructed this prompt we can pass it to llama 3.2 Vision here I'll run everything locally using ol llama like I did in the first video of this series and since I'm doing it in the same exact way I won't spend too much time on the code here but basically what we're doing is we're loading in llama 3.2 Vision which can understand both text and images and then we'll pass in the prompt as well as all the images from our image search results and then with that we can print out the response the monos response looks like this we can see that it has like this interesting syntax to it which is actually markdown so what we can do is take the raw output of the model and then process it with some kind of markdown reader and it's going to look something like this and then this is another upside of writing your prompts in markdown is that models will tend to write their responses in markdown when prompts use it there are some good things about this response so one it correctly understands what is in the image so it says the image depicts a contrast of loss function for aligning text and image representations in multimodo models however it seems to misunderstand the meaning of positive Pairs and negative pairs in this context it seems to think that a positive pair is any text image pair while negative pairs are text text or image image pairs which don't make any sense in the context of clip all pairs are text image however positive pairs are a text image pair that correspond to one another while negative pairs are text image pairs that don't correspond to one another so the model doesn't really quite understand that and maybe if the text results were more helpful it would have understood that it goes on to talk about text text or image image pairs which just don't come up in clip but it does kind of get this right that it calculates the difference between positive and negative pair similarities how it works text image embeddings generates embeddings for both text and images using multimode encoder so that's true and then it goes on to talk about the benefits which is interesting so lines text and images for better visual content understanding of visual content from text description enhances performance and downstream tasks like image classification retrieval and generation okay so it gets the benefits it doesn't really answer the question of what is clips contras of loss function it does describe some like General features and some important points but even though it had the answer in the image it didn't convey that information very well and I suspect one of the reasons for that is so many of those text search results were not helpful here we just kind of walk through the process and logic of implementing this multimodal rag system step by step but this isn't how you would actually implement this for some kind of real world project so for that also on the GitHub repository I have a radio user interface that kind of packages this all up if you run that Notebook 3 in the example code you should see something like this so let's try a different one so what are three paths described for making llms multimodal although this is something that llama doesn't necessarily need to read my articles to understand and this is something more article specific so let's see how it handles this one so far so good so it's using the Snippets it got the first path llm plus tools path to llm plus adapters so here it seems to have done a much better job and that's likely because the search results are higher quality for this query as opposed to previous queries and then you notice that we're streaming the text here which is better from a user interface standpoint and then you can see how that's implemented in the example code so here we implemented this multimodal rag system and one thing that came up is that the quality of a multimodal rag system is very much dependent on the quality of the search process of this retrieval process and we saw that the search returning irrelevant items can be a major problem there actually a few other ways we can improve this the first is we could use a ranker the way this works is that rather than only relying on Vector search or keyword-based search what you can do is take those top 15 text search results we saw in the example code and we can pass them through yet another language model that's trained via contrast of learning but specifically to identify query and helpful answer pairs so before we used clip embeddings to generally identify items in the knowledge base that were similar to our query the value of a ranker is that we could use a model that was specifically trained to identify positive pairs of qu iies and relevant search items so the output of this will be another set of similarity scores we can use in our rag system another strategy is that rather than using Clips multimodal embeddings out of the box we could fine-tune clip so that rather than simply evaluating the similarity between the query and items in our knowledge base we can f- tune an existing multimodal model to align queries with relevant items in the knowledge base so not just semantically similar ones but items that are helpful in answering the query but of course the downside of fine tuning is that this is going to require us to generate a lot of query item positive pairs in order to do this training process so if either of these techniques sound interesting to you and you want me to cover them in future videos let me know in the comment section below if you enjoyed this video but you want to learn more check out the blog published in towards data science although this will be a member Only Store you can as always access it completely for free using the friend Link in the description below and with that thank you so much for your time and thanks for watching
YOvxh_ma5qE,2024-11-28T18:59:44.000000,Multimodal Embeddings: Introduction & Use Cases (with Python),although AI research is traditionally split into distinct Fields like NLP and computer vision countless real world problems require solutions that integrate information across these modalities in this video I'll discuss how we can solve such problems using multimodal embeddings then show how to use them to do things like zero shot image classification and image search and if you're new here welcome I'm Shaw I make videos about the things I'm learning about and building in Ai and if you enjoyed this content please consider clicking the Subscribe button that's a great no cost way you can support me in all the videos that I make here we're going to talk about multimodal embeddings although the discussion here will focus Around clip which works with text and image data this is a much more general idea that can be extended to many other modalities before talking about multimodal embeddings it's worth answering the question what are embeddings the way I'll Define embeddings here are useful numerical representations of data learned through model training a classic example of this is Bert which is a popular language model before the era of gpt3 and all the modern large language models B used to be state-of-the-art and one of the things that it does is masked language modeling in other words you can give it a sequence of text where one of the tokens in that sequence is masked meaning that it's not visible and Bert will predict the most likely token that goes in the place of that mask so if you pass in the sequence listen to your the most likely token that goes in the sequence is instincts so it turns out that through learning how to do this predi ition Bert learns useful representations of text which can be extended to other NLP tasks the basic idea here is that you'll take Bert and you'll drop its head so the classification head which is doing this masked language modeling and you'll have this mutilated version of B which instead of doing this token prediction it'll take an input sequence of text and return a numerical representation of it where each row in this Matrix corresponds to each token in this text sequence and then each of these columns corresponds to the embedding Dimension the dimension of this internal representation of text that Bert uses in order to do masked language modeling we can go one step further and go from token level representations to sequence level representations so we could do something like take the average across all these tokens in the sequence and we're left with a 1 byd Matrix which represents the entire sequence and of course to get these embeddings to be a bit more practical people will often do additional fine-tuning on top of these embeddings but this is the basic idea of where they are coming from a key point about embeddings is that these aren't just arbitrary numerical representations they are typically semantically meaningful such that if we were to look at how text were organized in this embedding space similar Concepts would tend to be located close together while this similar Concepts will tend to be far apart for example the sequence a cute puppy might be relatively close to the sequence a good boy while that same sequence a cute puppy might be relatively far from a sequence like funny cat meme however this isn't limited to just text we can generate embeddings for any type of data another popular type of data we might work with are images so if we had some image embeddings the space might be structured like this where we tend to have cats in the top left part the dogs tend to be in the bottom right part and then we have a goat further away from these although text embeddings and image embeddings are super helpful in that they can be adapted and repurposed to solve other either NLP tasks or computer vision tasks one major limitation here is that any random text embedding space we might be working with and any random image embedding space we might be working in don't have any relationship to one another there's no way out of the box to directly map this text embedding space to this image embedding space and vice versa even if they are semantically meaningful in of themselves and that's something we can plainly see here in that the text and image embedding spaces are not aligned because for this Tex embedding space the puppies tend to be in the top right the cats tend to be at the bottom while in our image embedding space the cats tend to be up here and the dogs tend to be down here but what if there was a way we could merge these two embedding spaces together that's exactly the key idea behind multimodal embeddings which are embeddings which align representations of different data modalities and the basic intuition here is that if we had a multimodal embedding space we could represent text and images in the same Vector space so now indeed text like a c puppy will be close to images of cute puppies the text to cute cat will be close to images of a cute cat and the same thing will hold for other Concepts however this idea is not limited to just images and text we could just as easily embed audio and images together maybe this is a audio file that is a cat meowing this is a goat making goat noises we have a puppy with like a cute bark and then maybe we have like a funny shrieking sound associated with this cat meme here another another application of this is aligning representations of brain signals with images and text but this means is if we were to record someone's brain activity and then represent it in this embedding space we could in principle decode the brain information to generate images and text so in essence reading people's thoughts actually in reference number four they are aiming to do exactly this with large language models intuitively this idea of multimodal embeddings is pretty simple to understand we have this embedding space which is agnostic to modality so it doesn't matter if it's a image of a cat a text description of a cat or the brain signals of someone looking at a picture of a cat these numerical representations will be relatively similar but how do we create these aligned numerical representations in other words how does this work under the hood so the key technique behind creating these types of embeddings is contrastive learning which is an approach that seeks to represent different views of the same underlying information similarly and the way this works is that we'll train a model on two things one positive pairs of data and two negative pairs so in the case of aligning image and text representations positive pairs might be a picture of a cute cat and a textual caption for this image and then we might have the text and an image of a cute puppy and then we might have the same thing for a baby goat on the other hand negative captions might look something like this where you have the image of a cat but the caption is a cute puppy image of a puppy and the caption is a goat and you have a goat and the caption is a cat so the intuition here is that we train a model to maximize the similarity between these positive Pairs and minimize the similarity between these negative pairs that's the key intuition in the following slides we're going to go one level deeper and look at the loss function and the math behind how this is accomplished if you don't care about the math and how this is working under the hood feel free to skip ahead to the example code but if you're interested in the math we're about to jump right into it the way we can use contrast of learning to align image and text representations is we can take images generate image embeddings using an image encoder so basically we take our images and generate a single numerical representation for them and then we can take all these image embeddings and we can concatenate them into a matrix that I'll call I sub e so this will be a n by D Matrix where n is the number of images so if you have three images here it will be 1 2 3 and then D so the number of columns will be the embedding Dimension then we can do a similar thing for text so we can get a text encoder from off the shelf we can generate these text embeddings and then we can concatenate them into a matrix that I'll call T sub and that will have the same shape so we'll have end captions and then they'll have some embedding Dimension D just to point out here that the representations that we would put into these matrices won't directly come from an image encoder and text encoder instead these will be multiplied by some learnable weight Matrix and then normalized before being organ ized in this Matrix so that weight Matrix that we multiply the original embeddings by are the learnable parameters once we have these matrices I and T we can construct this lit's Matrix basically what that means is we're going to take each image in our image embedding Matrix and then each text sequence in our text embedding Matrix and we're going to compute their similarity typically this is just a cosine similarity so you do the dot product between these two matri sees and then you'll divide it by a temperature parameter that's what this tow parameter is representing and the reason we call them logits is because at some point it's going to be the argument in an exponential function and we'll see that in a little bit here so taking just those three examples from the previous Slide the similarity between the first image and the first text sequence will be in this one one position of the Matrix and then the similarity between this cat image and the sequence the cute puppy will be represented by this value here then the similarity between this cat image and the text sequence a cute baby go will be represented by this value here and so on and so forth just looking at this we can see that what we want is to make the logits on the diagonal of this Matrix as big as possible so in other words we want to maximize the similarity between the positive Pairs and then we want to minimize the off diagonal Elements which correspond to negative pairs one way we can do this is via the contrastive loss so this might take slightly different forms depending on the context or the paper that you're reading but here I'm going to follow what was done in developing clip which is reference number three here and so basically one way we can achieve this goal of maximizing the similarity of these on diagonal elements and minimizing the similarity between these off diagonal elements is via this equation here which is basically saying for the E image so let's say this cat image here we want the numerator to be as big as possible so the numerator will be the i i element so this will be either 1 1 or 22 or 3 3 and then we want the denominator to be as small as possible so if the numerator is Big the denominator is small that means this fraction becomes big and then if we take the log of that we'll still have a big number and then we want this number to be as big as possible because the goal of training is to minimize the loss and then if this number is big and we have a minus sign next to it then this will be as minimal as possible that was probably a bit abstract so let's walk through this step by step let's look at just the first image first with this notation I call the loss associated with the first image L1 this will consist of taking this one one logit and then summing over all the logit in this first row so we're basically taking this image and comparing it to every single caption then we do the same thing for the second image we have the positive pair similarity here and then we sum over all the logits in this row and then we do a similar thing for image number three so we look at the positive pair similarity and then we sum over all the logits or similarities in this row we can do this for every single image in our batch or even in our whole training data set and then we can aggregate them to get the final contrast of loss what that'll look like is we'll take the loss according to the first image the loss according to the second image and the loss corresponding to the third image and then we can just take their average and that'll give us the contrast of loss for the images but we can do the same exact thing for text this is how I'm notating contrast of loss for the text I've switched the index here from J to I and then I've changed the summation here to I I feel this notation might be a bit too too subtle but hopefully explaining it step by step it makes sense what I mean here so let's see what this looks like for the first text sequence we're going to be evaluating a cute cat so we'll look at lits one one here and then we'll sum over the logits in this First Column we'll do the same thing for this second text sequence a cute puppy we'll sum over all the logits in this column and then finally we do it for the final Tex sequence it's important to note here that generally this logits Matrix is asymmetric because the similarity between the text a cute puppy and this image of a cat is in general different than the similarity between this image of a puppy and the text sequence a cute cat that's an important thing to note here and that's the reason why we go through this whole procedure for the images and the text sequences separately and then we can aggregate the loss over all the text examples just like we did for the images like this and then we'll get a total text loss by taking the average of all the examples in our mini batch we can then combine the image loss and text loss together by taking their average then we can write it all out to have this big monstrosity all on one page but basically this first term here corresponds to the image loss the second term here corresponds to the text loss and this is how we train the weights which translate the raw image and text encodings into our multimodal embedding space this will give us a training signal which we can use to update the weights of these projection matrices and just keep doing that until we're satisfied with the loss so if that was much more math than you were hoping to get out of this video I apologize for that but let's jump to practical applications of multimodal embeddings here I'm going to use clip for two different use cases this first use case is zero shot image classification the meaning of that is we're going to do image classification without explicitly training clip to distinguish between the different image classes that we're considering the first step is to import Transformers and I'm going to bring in these two things and then I'm importing this pil Library which will allow us to work with images in Python next we'll load in the model and the data processor the image pre-processing is important because images could be any type of size and shape and all that the clip processor is an abstraction that ensures the data aren't in a suitable format to be passed through the model next we're going to load in our image so I'm going to load in this image of a cute cat so it's the same one we've seen so far then I'm going to define the text classes so this is a really interesting aspect of using clip for zero shot image classification because before if you wanted to do image classification traditionally that was something that was set at model training it was implicitly coded into the architecture of the model in that you had this classification head and each value in the output layer corresponded to the probability of class one versus Class 2 versus class three so on and so forth but now when using clip which is trained via contrastive learning we actually pass these classes as text inputs so with our text and image inputs defined we can pass these through our processor to put them in a suitable format as clip and then we can just pass it to the model then with this one line of code we'll generate these outputs then we can extract the logits per image recall the lits Matrix that we saw a few slides ago where we had an image and then we had loit values or similarity values between that image and every single piece of text that we passed into the model that's exactly what we're extracting here we're extracting the similarity score of the input image to to both the text inputs then we can convert these logits to probabilities via the softmax and then this will give us a prediction what I'm doing here is I'm just doing ARG Max of the probabilities tensor and using that to pick out the predicted class from this original list that I created and then I'm just going to print everything like this so I'll print the predicted class as well as a rounded probability corresponding to that class with that the most probable class is a photo of a cat with a Associated probability of 99.79% so it basically Nails the classification of this image but let's see what happens when we use different text classes instead of passing in a photo of a cat and a photo of a dog which are pretty easy classes to distinguish between let's try something a bit more nuanced like a ugly cat versus cute cat and again here the model basically Nails it with 97% probability of this cute cat class and then we can try something even more challenging like trying to distinguish if this is a cat meme or not a cat meme and it indeed gets that it's not a cat meme but we can see that the probability dropped significantly then as a final test of this model let's see what happens when we pass in an actual cat meme and give it the class choices of cat mem versus not cat meme and so here the model again Nails it it correctly classifies this as a cat meme with a probability of 83% and so again what we're doing here using clip is we're taking these three entities we're taking the Tex sequence of cat meme the teex sequence of not cat meme and this image of a cat encoding them in a shared embedding space and we're evaluating the similarity between this image of a cat and the text sequence cat meme and the similarity between this image of a cat and the text sequence not a cat meme and then we can convert that similarity into a probability as well as a class prediction the key unlock here is that you are not restricted or Limited in the different class labels you can use for image classification you can be as detailed or vague as you like you can adapt this to endless different use cases which is pretty amazing this second example is basically the inverse of zero shot image classification there we had an input image and we wanted to match it with one of the input text sequences here in example two we're going to do the exact opposite so instead of starting with an image we're going to start with a piece of text in other words a search query and then we're going to match it to a set of images so essentially what we're doing is we're doing a search over a set of images the way this looks is we'll first load in our images here we have a picture of a cute cat picture of a dog and a picture of a goat we store them in this image list we're using the pil library to open the images and just store them in this list then we're going to define a query and process the inputs here our query will be a cute dog and then we'll pass this query along with our image list through the processor so it's in the appropriate format for clip then we'll run these inputs through our model get these outputs extract the logits per text now before we did logits per image now we're doing logits per text so these are going to be the similarity scores between the input text and all the images that we inputed and then we'll convert these logits into probabilities so with that we can evaluate the best match so I'm doing that again in a similar way so we have these probabilities doing ARG Max which will give us an integer 0 1 or two we can use that to pick out the best matched image and then we can take the probability associated with that image and then we can just print everything so again the query here was a cute dog and this is the best matched image with a probability of about 98% but again that was a super easy example so let's try a trickier query like something cute but metal in this case the model Returns the goat which is indeed cute but also goats are associated with heavy metal music and it got a 77% match probability reading this a good boy the text itself doesn't have anything to do with animals you know maybe it's a human boy and he's well behaved but a good boy is a colloquialism for dogs that we use often and the model can pick that up quite easily so it matches it with a dog with 82% probability it would be interesting to see if we threw in a picture of a human boy to see how the model would handle that case this could be something that you do with the example code from the GitHub and then we can try an extremely controversial query like the best pet in the world for this the model returns a cat with a 56% match probability this is likely indicating that on average people on the internet love cats more than they love dogs nevertheless it's super interesting how we can use this model in order to do search like this so those were the two examples code is on the GitHub Link in the description below let's look ahead to the next video of this series in the previous video so part one we talked about multimodal large language models so basically large language models that can process or generate multiple data modalities in this video we talked about multimodal embeddings like those generated by clip which can be used to do things like image search so we pass in a query and a set of potential images and then it'll spit out the best matched image in the next video of the series we're going to bring these two ideas together to create a multimodal rag system the basic flow here will be to take a user query like what's there to do in Bali we'll pass the query into a multimodal retrieval system which involves using a multimodal embedding model to pick out the documents and images that are most relevant to this query we'll take the user query and relevant documents and images to generate a prompt and then we'll pass that prompt into a multimodal large language model which can process the user query relevant text documents and relevant images to generate a helpful response and as a final note if you enjoyed this video and you want to learn more check out the blog published in towards data science there I went into some details that I probably missed here and as always even though this is going to be a member only story you can access it completely for free using the friend Link in the description below and with that thank you so much for your time and thanks for watching
Ot2c5MKN_-w,2024-11-20T13:31:14.000000,Multimodal AI: LLMs that can see (and hear),n/a
gUJJB235DVs,2024-11-18T15:11:07.000000,5 AI Projects You Can Build This Weekend #python #ai,five AI project ideas that you can implement this weekend one resumé Optimizer use open ai's python API to adapt your resume to specific job descriptions number two YouTube lecture summarizer take those long lectures collecting dust in your watch later playlist and summarize them to see if they're worth keeping around number three PDF organizer if you're like me you have an unreasonable number of research papers on your desktop you can build the tool to generate text embeddings for each PDF and then use a simple clustering algorithm to organize them into folders number four is multimodal semantic search searching text is easy but what about images and videos now you can use multimodal embedding models to put text and images in the same search space then finally desktop question answering you can connect a multimodal database
bAe4qwQGxlI,2024-10-25T13:18:08.000000,I Built an AI App in 4 days... here's how.,I built a web app to translate YouTube videos into blog posts the crazy thing is before building this I had absolutely no web development experience yet I was still able to build and deploy this project in Just 4 days so how did I do that in this video I'm going to share my full process for building this app and all the tools I used to go from idea to deployment and if you're new here welcome I'm Shaw I make videos about D data science and Entrepreneurship and if you enjoy this content please consider clicking the Subscribe button that's a great no cost way you can support me in all the videos that I make for a bit of context my background is in data science which is basically all about using data and machine learning to solve problems while this means I've done a lot of programming coding to process data or train models is very different than building consumer software this is a common struggle for most data scientists who might be very comfortable with python but never learn how to ship SAS products with my savings on track to hit $0 this quarter I was determined not to be one of these data scientists after spending too much time on Indie hacker Twitter I figured the best way to learn this skill set was by doing it that's why this quarter my goal is to ship one product every single month until until something makes money my first such product is a tool for converting YouTube videos into blog posts it took me just 4 days to build and deploy the initial prototype for this app and here I'm going to share exactly how I did it the first day I was starting from scratch I had my goal of shipping a product in less than a month but hadn't settled on an idea my approach to picking product ideas came from advice I got from Steven Wolfram who said the product time building is a product that I want my interpretation of this advice is to solve your own problems this led me to three product ideas a YouTube thumbnail generator a YouTube clip finder and a YouTube video to blog converter after doing some basic research and playing around with chat GPT I settled on this last idea because I felt it was the one I could implement the fastest I had also seen a job post on upwork paying a few hundred to do exactly this with an idea settled my next step wasn't to start coding instead I started designing the website and user interface for me starting with the design was actually a necessity because I had no idea how to code the thing my approach to doing the design was to First create a brand blueprint in other words creating a color palette picking fonts and designing a logo for the product for the color palette I used a website called cooler to quickly find colors I liked then brought everything together in canva having this brand blueprint made it pretty easy to design a simple UI at the end of day one I had something that looked like this on day two the goal was to implement the front end based on my canva design since my goal was to ship the product by the end of the month I didn't have time to learn HTML and a front-end framework the fastest way forward would be to use the language I'm most com able with python lucky for me a few months ago a python library was released to do exactly this the library is called Fast HTML which allows developers to build modern web apps in Python I spent the morning learning fast HTML by watching a lecture from the creators and reading through their documentation then in the afternoon I started coding the front end based on my canva design my first step was to take a screenshot of the design and pasted into cursor to get me started even though fast HTML is a new library that Claude and GPT haven't been trained on I uploaded this text file from Fast html's GitHub repository to give cursor more context which actually worked surprisingly well this experience sold me on using cursor and AI assistance for coding I'd also use chat GPT from time to time for tasks such as writing CSS styling this workflow of trying AI chat in cursor with Claude first then going to chat GPT if needed became my standard approach by the end of the day I had a site that looked very similar to my initial design by day three I had coded my website but it didn't do anything The Next Step was to implement the backend I started by asking chat GPT to write me a prompt for creating blog posts based on YouTube video transcripts I tinkered with this prompt in chat GPT a bit and experimented manually with different videos when I had something that looked good I implemented the whole process in Python the two key libraries I used here were the YouTube transcript API and open ai's python API since I've used these libraries for past projects I was able to repurpose most of the code from these examples that was the extent of the AI I used for this initial version I could have done something more sophisticated like allowing users to upload PDFs or fine-tuning a model on my own YouTube videos and blog posts but that would a taken time I don't have right now if this product is something that people actually want then I'll go back and make these improvements the rest of the backend implementation was just making the website work via basic web development which I was all learning on the Fly by day four I had a working prototype of the app running locally on my machine but you can't really sell a product that only runs on your computer the goal of day four was to deploy my app to the internet to avoid abuse from spammers and Bots I set up Google oo this is basically that signin with Google option that you see on websites these days the benefits of requiring people to sign in with Google are one I don't have to manage sensitive information like user passwords and two I don't need to worry about verifying that the users are human while setting up ooth sounds simple enough it took me most of the morning to get this working after that I bought a custom domain for the app I paid $70 for the domain yb. from Squarespace and then finally I deployed the app on Railway this was the fastest option because there was example code for deploying a fast HTML app to railway in their documentation after a handful of failed deployments and switching my domain provider to Cloud flx I was finally up and running here's a demo of me using that early version of the app I used it to create a blog post for a podcast that I hosted last year typically it takes me at least an hour to write a first draft for a blog like this however using this tool I was able to do that in just 5 minutes and then after just one day of this blog being monetized on medium I've made $112 from it which is already some real value for me personally in just four days I went from idea to production this is the world now thanks to python libraries like Fast HTML coding tools like cursor and deployment services like Railway developers can Implement ideas faster and validate them in public while this project would have taken me weeks without these tools it was still a prototype so it had limitations such as ooth users had to be manually defined in the Google Cloud console there wasn't a database hooked up to the app so users could use it without any limits and most importantly there was no stripe integration so no way of making any money from it it took me another 8 days to make an MVP version for which any Google user could sign in usage metrics were stored in a SQL light database stripe integration was set up and created a landing page that included a demo for people who hadn't signed up yet you can try out the latest version of the app completely for free using the link in the description below although shipping an app in one month may have sounded crazy a few years ago this is becoming normal based on my experience with this first product here are three key tips for those trying to do something similar first build with what you know if I had to learn JavaScript or react before even starting this project I'd still probably be watching YouTube tutorials Second Use AI tools coding assistants like cursor and stbt have become the norm in programming if you aren't using them you're probably moving two times slower than you would otherwise third and finally it's not just about building I know this video focused on the development side of the product but what are just as important if not more are the idea that you build and how you Market it for ideas I'd take wolfram's advice and solve your own problems this allows you to move faster and increases the likelihood of validation because you are your own customer for marketing that's still something I'm trying to figure out but when I find something that works I'll be sure to share it here if you have any questions about my process or any of the tech I used please let me know in the comments and as always thank you so much for your time and thanks for watching
4QHg8Ix8WWQ,2024-10-17T12:50:12.000000,Fine-Tuning BERT for Text Classification (w/ Example Code),"massive Transformer models like GPT 40 llama and Claude are the current state-of-the-art in AI however many of the problems we care about do not require a 100 billion parameter model there are countless problems we can solve with smaller language models in this video I'm going to share one example of this by fine-tuning Bert to classify fishing URLs I'll start by covering some key Concepts then walk through example python code and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship if you enjoy this content please consider clicking the Subscribe button that's a great no cost way you can support me in all the videos that I make so the title this video has a few pieces of jargon fine-tuning bird and text classification so let's talk about each of these terms one by one starting with fine-tuning this is when we adapt a pre-trained model to a particular task through additional training using an analogy we can think of the pre-trained model like a slab of marble that we can refine and chisel into a statue which in this analogy would be our fine-tuned model and typically the pre-trained model in this approach to machine learning is a self-supervised model meaning that the labels used to train the pre-trained model aren't manually defined but rather are derived from the structure of the data itself for example when talking about Text data we can use the inherent structure of text in order to do word prediction we could take a sentence like adapting a pre-trained model to a particular task and simply train the model to recursively predict the next tokens so maybe adapt would be used to predict ing adapting would be used to predict a adapting a would be used to predict pre adapting a pre would be used to predict train and so on and so forth so the key upside of this is that humans do not need to annotate or label the training data on the other hand the fine-tuned model will typically be trained in a supervised way in other words the targets are manually defined by humans so the reason we might want to develop a model in this way is because since the pre-trained model doesn't require any man manual annotation of training data that means we can train on much larger training data sets we no longer have that human bottleneck for example Bert which is the model we're going to be focusing on in this video was trained on 3 billion words in other words we have billions of examples that this model can learn from this is in contrast to a typical fine-tuning task which might consist of a few thousand examples so we're talking about six orders of magnitude difference in the number of examples today this is the prevailing Paradigm for developing state-of-the-art machine learning models there is an initial step of pre-training which will typically use some kind of self-supervised learning which allows the model to learn a tremendous amount of knowledge on a massive training data set and then the pre-trained model can be refined to be a bit more helpful through fine-tuning another benefit of developing models like this is that it unlocks the democratization of AI because developing these massive pre-trained models like llama or mistel or Claude or GPT 40 requires tremendous resources that no individual or typical research group will be able to pull off however splitting the model develop ment process into these two steps of first pre-training and then fine-tuning enables the pre-training to be done by specialized research Labs such as open AI Google meta anthropic mistal so on and so forth who can then make their pre-trained Foundation models publicly available for fine-tuning another Nuance here is that we aren't restricted to just doing one iteration of fine-tuning really any fine tun tuned model can serve as the starting place for additional fine-tuning so maybe we'll take this fine-tuned model and then we'll do some additional fine tuning to make it even more specialized for a particular use case and this is exactly the story of Chad GPT which was developed through three phases it started with the unsupervised pre-training then there was a supervised fine-tuning step and then there was a final step of reinforcement learning which refined the model even further that's all I'm going to say about fine-tuning in this video but if you want to learn more I do a deep dive in a previous video of this series the second piece of jargon in the title of this video is Bert even though this Paradigm of pre-training and fine-tuning models got super popular when open AI released chat gbt and shared that they were able to do that through this three-step process of pre-training fine-tuning and then additional fine tuning this is an approach that's been around since at least 2015 and one of the early models that really exploited this idea was Google's Bert which was released in 2019 ber is a language model developed specifically with fine-tuning in mind in other words the researchers at Google created Bert so that other researchers and individuals could repurpose the model through fine tuning on additional tasks in order to do this Bert was trained on two tasks the first task is masked language modeling what that means is is if we have the sentence the cat blank on the mat masked language modeling consists of using the context before and after this masked word this hidden word in order to predict it so given this input sequence this can be passed to Bert and then Bert will predict what the masked word is this is in contrast to models like GPT which don't do masked language modeling but rather causal language modeling where the training task is next word or next token prediction as opposed to masked language modeling the benefit of doing masked language modeling versus causal language modeling is that the model can use context from both sides of the sentence it can use both the text before and after the masked word in order to make a prediction intuitively this additional context gives the model more information for making predictions but that's not the only thing that Bert was trained on the second task that it was trained to do is next sentence prediction what this looks like is given two sentences A and B so here sentence a is Bert is conceptually simple and empirically powerful and then sentence B is it obtains new state-of-the-art results on 11 NLP tasks Bert is trying to take sentence pairs like this and output a label of is next or is not next this is a sentence pair taken from the original ber paper which is reference number one and so this is indeed the next sentence alternatively we could have a pair of sentences that are not a match so instead we might have Bert is conceptually simple and empirically powerful and then the next sentence is the cat sat on the mat and so this would be not the next sentence and would receive a different label the intuition behind having this second task of next sentence prediction in addition to mask language modeling understanding the relationship between two sentences like sentence a and sentence B is important for Downstream tasks like question answering or sentence similarity so training Bert on these two different tasks allows it to be effectively fine-tuned on a wide range of tasks the last thing we're going to talk about before getting into the example code is text classification which consists of assigning a label to text sequences and actually the next sentence prediction task we saw in the previous slide is an example of text classification however there are countless other examples of where text text classification might be handy one is Spam detection so given an input sequence of text which is a incoming email we could develop a model to predict whether that email is Spam or not spam similarly if we have incoming it tickets we could develop a text classification model to take those text sequences and categorize the it tickets into the appropriate tiers finally one could use a text classification model to do sentiment analysis on customer reviews in other words to analyze the number of happy customers and the number of unhappy customers with all the jargon covered fine-tuning we covered Bert and we covered text classification let's see a concrete example of doing this so here I will fine-tune Bert to identify fishing URLs this will actually be similar to the example I shared in my fine-tuning video from last year however soon after posting that video someone had pointed out an issue in that example that I had missed and if we look at the training metrics it's actually pretty easy to spot the training loss is decreasing as expected then if we roughly look at the accuracy it improves from the early Epoch to the last epox but it's a bit shaky so actually the best performing Epoch was number three and it seems to wiggle around for the remainder of training but the most obvious red flag is if we look at the validation loss so this is actually monotonically increasing during training which is the opposite of what we want to happen so this is a clear sign of overfitting this wasn't something I had caught so I'm glad someone had pointed this out on the medium article here I'm going to do another example where we don't see this overfitting so for this example we'll be using Python and the hugging face ecosystem so we'll import the data sets library from hugging face which gives us a data structure for loading our training testing and validation data we'll import some things from the Transformers Library the auto tokenizer class the auto model for sequence classification training arguments class and the trainer we'll import the evaluate library from hugging face which has some handy model evaluation functions we'll also import numpy which will be used to compute some of these metrics and then we'll also import this data collator from the Transformers Library which we'll talk about in a little bit with our Imports we'll load our data set so this is something I've made freely available on the hugging face Hub it's a data set of 3,000 examples 70% of which are used for training 15% of which are used for testing and then the final 15% are used for independent validation with our data loaded in we can load the pre-trained model so here we're going to be using Google's Bert more specifically Bert base uncased which consists of 110 million parameters which by today's standards is Tiny But at one point this was a pretty big model we'll load in the tokenizer for this model what this will do is take in arbitrary text sequences and convert them into integer sequences based on what the Burt model is expecting finally we can load in the Bert model but slap on top of it a binary classification head so all we have to do is use this autom model for sequence classification class and use this fir pre-trained method and we'll pass into it the model path which is what we defined here the number of labels the number of classes and then a mapping for the classes so the ID is going to be an integer and then we'll have a label for each class zero will correspond to a safe URL and one will correspond to a unsafe URL with our model loaded in and our classification head slapped on top of it let's set the trainable parameters by default when we loaded in our model in this line of code it initializes all the model parameters as trainable so all 110 million parameters plus the parameters in the binary classification head are all ready to be trained however if you're just running this example on your laptop like I did that's going to be pretty computationally expensive and potentially unnecessary so to help with that here we're going to freeze most of the parameters in the model we can do that pretty easily so here I have a for Loop that's going to go through the base model and it's going to freeze all the base model parameters to just break this down a little bit we have this model object which has this base model attribute and then that base model attribute has this named parameters method which will return tupal corresponding to all the parameters in the model basically what we can do is go through all the base model parameters and set this requires grad property to false after running this all the base model parameters are frozen and only the parameters in that classification head that we slapped on top of the model are trainable another name for for training a model like this is called transfer learning where we leave the base model parameters Frozen and only train a classification head that we add on top of it however this might result in a pretty rigid model because we can only refine the parameters in that classification head so one thing we can do is we can unfreeze the base model parameters in the final two layers so in the pooling layers of this model one way we we can do this is that we can loop back through the base model parameters and then if we see the word Pooler in the name of the model we can just unfreeze those parameters the result of all this is that we freeze all the model parameters except for the last four layers this allows us to keep the computational cost down for fine tuning while also giving us a fair amount of flexibility and of course you can free free or unfreeze any of the model parameters that you like and this would be like a fun thing to experiment with on your own with our model ready to be trained next we need to pre-process our data this is actually going to be pretty simple we'll Define a function called pre-process function which will take in an arbitrary sequence of text and tokenize it so it'll translate a string of words into a sequence of integers according to to the Bur tokenizer additionally I add this truncation flag what that will do is ensure that none of the input URLs are too long so by default I think this will truncate all the input sequences to 512 tokens and then with that function defined we can just apply this pre-processing step to all the data sets in our data set dictionary that we imported earlier so this will tokenize the training testing and validation data sets and return it in this tokenized data variable and then another thing we can initialize at this point is a data collator and so we took care of input sequences that might be too long but when we are training a model it's important that every example in a batch is the same size and so even though we won't have any examples greater than 512 integers we will have examples with fewer than that so to ensure all the samples in a batch have a uniform length we can create this data collator which will automatically do that for us during training okay and then the final step before we actually train the model is to Define evaluation metrics these are the metrics that will be printed during the training process here I'll use two evaluation metrics the accuracy and the Au score these are both loaded from the evaluate library then I will Define this function called compute metrics which will comp comp the accuracy and Au score for any example so the input of this function will be a tuple it'll consist of predictions and labels predictions will be a logit it will be a number between minus1 and 1 while labels will be the ground truth so this will be either zero or one to convert the logits into probabilities so basically to map a number between -1 and 1 to a number between 0 and 1 we'll apply the softmax function which looks like this this will compute probabilities for both cases for both the URL is safe and the URL is not safe so let's only look at the probabilities for the URL being not safe and then we'll use that to compute the Au score we do that by passing in the positive class probabilities so the probability that the URL is not safe and the ground truth and we'll round it to three decimal plates that gives us our Au score and then we can predict the most probable class so predictions will consist of two numbers it'll be a loic corresponding to a safe URL and the logic corresponding to a unsafe URL we'll just do ARG Max so it'll just return which element is larger and then we can pass that into this line of code to compute the accuracy so we'll compare the predicted class with the ground truth and we'll round that number to three decimal places then we'll return a dictionary which consists of the accurate and Au score while this may have been a lot of Hoops to jump through this will be nice during training because at each Epoch our trainer will print out the accuracy and Au score for us now we're ready to start training the model we'll Define our training parameters we'll set the learning rate at 2 to Theus 4 batch size as eight and number of epoch as 10 we'll put all these into a training arguments variable so we can set our output directory which I set as Bert fishing class classifier teacher the reason this teacher is here we'll talk about near the end we'll set the learning rate defined here we'll put the per device training and evaluation batch size which is just going to be eight number of epoch is 10 and then logging evaluation and save strategy so logging strategy sets how often the trainer will print the training loss eval strategy sets how often the trainer will print the evaluation metrics that we defined earlier so Au and accuracy and then we can also set our save strategy so we can have the model get saved at every Epoch just in case something goes wrong we can refer back to that latest save and then we'll set that the trainer will load the best model at the end so if the 10th EPO isn't the best model maybe it was the eighth Epoch we'll use that one instead of the last one with all the training arguments set we are ready to find tune our model we'll pass everything into our trainer pass in our model the training arguments from before our TR training data set our evaluation data set the tokenizer the data cator and the compute metric function and then simply run trainer. Trin so this I think took about 15 minutes to run on my laptop didn't use a GPU or anything but here are the results we can see the training loss is steadily decreasing we see the validation loss is a little rocky at times but it eventually goes down and then we have our accuracy in AU C so we can see the accuracy is for the most part increasing and then the Au is steadily increasing so this is what we want to see we don't want our validation loss to be monotonically increasing we want it to be decreasing with training but we can go one step further this is evaluating our model on the training data and the testing data but let's look at it on the independent validation data set this validation data was not used for training the model parameters or to the hyperparameters so it gives us a fair evaluation of the model to apply the model to the validation data will'll generate predictions for it like this then we will extract the logits and the labels from the predictions and then we'll just pass these into the compute metric function defined earlier and then we can see that these are the results the accuracy and Au score are comparable to what we're seeing for the testing data set in the previous slide so accuracy about 0.89 Au about 0.9 5 if we go back we see accuracy 0.87 Au 0.95 pretty similar so that's a good sign that this model is probably not overfitting at the end of this we have a binary classification model that consists of a little over 110 million parameters however as a bonus we can actually shrink this model even further and that's exactly what I do in a previous video of this series where I talk about model compression for for large language models the example in that video I take the model that we just created here and reduce its memory footprint by 7x using model distillation and quantization the key upside of these compression techniques is that we can shrink the computational requirements for our models without sacrificing performance in fact for the example that I walk through in this video the performance actually goes up a little bit that brings us to the end if you enjoyed this video and you want to learn more check out the blog and towards data science there I cover some more details that I may have missed here and even though this is a member only story you can access it completely for free using the friend Link in the description below and as always thank you so much for your time and thanks for watching"
tMiQIxSX64c,2024-10-10T13:50:57.000000,5 AI Projects You Can Build This Weekend (with Python),n/a
7Oy2NmPwJXo,2024-09-26T23:24:35.000000,I Quit My Job… Here’s How Much I Made 1 Year Later,n/a
ZVVkdXHqEuM,2024-09-23T15:45:12.000000,Knowledge Distillation Explained in 60 Seconds #deeplearning,n/a
reXoKNC_Wx4,2024-09-20T18:15:44.000000,Quantization Explained in 60 Seconds #AI,here's quantization explained in 60 seconds may have heard of quantization in the context of efficient fine-tuning methods such as Q Laura it might sound like a scary and sophisticated word it's a very simple idea when you hear quantizing just think splitting a range of numbers into buckets for example there are infinite numbers between 0 and 100 there 27 55.3 83.7 823 and so on could quantize this infinite range by splitting it into buckets based on whole numbers that means 27 would get mapped to 27 55.3 would go to 55 and then 83.7 823 would go to 83 you can even go one step further split this range into even bigger buckets these numbers would go to 20 50 and 80 it's recently gained popularity in AI this is because it allows us to take massive AI models and Shrink their computational requirements by quantizing the model par
9joIFeKuf04,2024-09-16T14:01:44.000000,Python Explained in 60 Seconds #programming,n/a
pNg2DJ4spXg,2024-09-12T16:36:12.000000,Python QuickStart for People Learning AI [Mini-Course],"python is the go-to programming language for doing Ai and data science although no code Solutions exist if you want to build fully custom AI projects and products learning python is still essential in this video I'm going to give a beginner-friendly quick start guide to learning python specifically for AI projects and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship if you enjoyed this content please consider subscribing that's a great no cost way you can support me in all the videos that I make this video is going to be broken up into three main sections first I'm going to start with a highle introduction of python and who this video is made for second I'm going to walk through the fundamentals and basics of python and then finally I'm going to share an example AI project showing how we can use Python to create a research paper summarizer and keyword extractor starting with the super basic question of what is python python is a programming language which is simply a way to give computers precise instructions to do things that we can't do or just don't want to do one example of this from my world is I'll send follow-up emails after Consulting calls with clients so the current process is we'll have the call I'll get on my laptop and type up a follow-up email and then I'll send the client the email however if this was something that started to get out of hand and I ended up spending hours every week writing follow-up emails I might create a python script that would do this instead of me typing up the email after the call I could create a python script to do this automatically and this is just one random example there are very few constraints on the things that you can do with python and AI another thing I want to highlight is that coding now is easier than ever two of the major reasons for that are Google and chat GPT so for example if I wanted to figure out how I can send emails automatically with python my first thing might be I just go to Google and type that into the search bar and then I'll get results that look like this so stack Overflow if you're not familiar is a very popular developer Forum where people are constantly sharing their questions and more experienced programmers are sharing answers the power of things like Google and stack Overflow is that it Taps into the wisdom of the crowd so key thing here is that no individual knows everything about python however if you get a collection of people together their Collective knowledge far exceeds any single individual and this is something that you can tap into using forums like stack Overflow or just generally browsing the web the second thing that's really become a game changer for a lot of developers is chat GPT and other tools like it another option here is instead of typing this into Google is you might type this same prompt into chat gbt so if you type this into chat gbt it'll give you a step-by-step answer as well as example code however that's not the only benefit of using chat gbt for coding questions one of the most powerful aspects of it that I really enjoy is the ability to ask follow-up questions so if there's something that doesn't really make sense here like what is the meaning of MIM or why is it called SMTP lib just these random questions that I might have and these might be questions if you asked a regular developer or program they might get annoyed at you because you're getting way too into the weeds or getting way too pedantic but Chachi BT is eternally patient and can answer almost any beginner level question with a high degree of accuracy I'm making this tutorial with a specific audience in mind namely people who are learning Ai and have done some coding but are brand new to python some examples of this might be someone who works as a consultant or even a business intelligence analyst another demo I have in mind are students or recent graduates but also seasoned developers who might just be new to python this is not meant to be a comprehensive introduction or overview into python rather the goal here is to Simply give you just enough to get you started coding your very first AI project and doing that as quickly as possible so with that let's just jump into the basics of python the first thing is that most computers come with python pre-installed so the first thing to do is to check if you have python installed on your machine if you're on Mac or Linux you can do this through the terminal app or if you're on Windows you can do this via command prompt or even Powershell I can show you what that looks like real quick here we have terminal opened up and I'm simply just going to type in Python if I do that we can see the python version that we're using and has some basic instructions with this we are in Python right now and we can go go ahead and start coding however if you type Python and this does not come up that means you're going to need to install python on your machine that's actually pretty straightforward if you are on Mac you can go to this link here if you're on Windows you can go to this link here and of course I'll share these in the description below but just seeing what that looks like this is the official python documentation and if you go to this python website link here it'll have a set of downloads for stable releases of python so if you're on Mac you can go ahead and click this thing and it will download an installer you can use to get python on your machine and a similar thing for Windows alternatively there's a popular python package system for data science called Anaconda this is available for Windows Mac and Linux so this is another way you can install Python and this has some handy features beyond the base installation of python now that we've confirmed we have python installed let's start digging into some key Concepts and code the first thing I want to talk about are data types which are simply a way to classify data so it can be processed appropriately and efficiently one example of this are strings which are character sequences which we can use to represent arbitrary text here are some examples of strings hello and hello spelled with numbers and special characters we can also go back to our terminal here and start typing strings so hello is an example of a string we can also write strings with single quotation marks so this is another way to write strings fundamentally there's no difference between using the double quotes and single quotes for Strings but what that does allow you to do is to use quotations in your strings themselves for example if we wanted to have the string Shaw said python is awesome the double quotation marks is going to cause an error however if we use the single quotes oh oops messed it up however if we use the single quotes we can have this be a string and the last thing I want to show here is we can also use triple quotes if we want to have strings across multiple lines and then you'll see that this new line character will appear in our string some other data types that we can use to represent numbers are ins which are integers and floats which are numbers that contain decimal points so going back to our terminal an example of an in is 1 2 42 you know just like in math floats would be anything with the decimal so 1.0 3.14 and then we can add two of the same data types together so we can add two integers we can add two floats we can even add an integer and a float and then python will automatically convert the integer into a float before doing the operation and we can even add strings together so here's an example of that so here we've added a string together another words we've concatenated it however we can add a number to a string so if we try that we'll get an error although ins and floats can be added together ins floats and strings cannot be added together if we wanted to do that we would have to write one as a string another fundamental data type are lists which are ordered collections of values some examples of that are shown here we can have a list of strings like a C we can have a list of integers like 1 2 3 we can also have a list with mix types so we could have a 2 and 3.14 and we can even have a list of lists this flexibility makes lists a very handy data type when doing AI in data science projects the final data type I'm going to talk about are dictionaries which are key value pair sequences what that looks like is a little different than a list so instead of the square brackets we have these curly brackets and then we have key value pairs where each item in the dictionary consists of two parts a key which will always be a string and a value which can be any data type and so a simple dictionary might have a single item with a key called name and a value of Shaw which is my name something a bit less trivial is to have a dictionary with multiple key value pairs corresponding to the name age and interests of a individual so here have the name as Shaw have age as 29 so you notice that this is an integer and then interests are organized in a list so we have multiple data types in this dictionary but we can also combine the list in dictionary data types so we can have a list of dictionaries here instead of just having a dictionary corresponding to one person we have two dictionaries corresponding to two different people so we have sha who's 29 interested in AI music and bread and then we have iffy who's 27 who's interested in marketing YouTube and shopping the last thing is we can also have a dictionary within a dictionary the way that looks is we might have a key value pair corresponding to a user and the value for this item will be another dictionary which includes user information then the other items in the dictionary might be the last login date and the membership tier of that user the next thing I want to talk about are variables which are abstract representations of underlying data so far we've just written data types explicitly as Sha or a set of numbers or a list or a dictionary if you had to continuously write out lists and dictionaries in your python code it would become very tedious very quickly this is where variables come in what we can do is instead of writing sha over and over over again in our code we might create a variable called username and set it equal to Shaw and then anytime we wanted to plug in the user's name we could just use the variable instead of writing it explicitly so if we printed the username variable we would see Shaw in the command line a more sophisticated example is we can Define two other variables one called the users age and another one called user interests which are a int and list respectively and then we can print those out in this way the output of this is Shaw is 29 years old his interests include AI music and bread not only does this save us from writing strings ins and lists over and over again in our Python scripts it also allows our scripts to be much more flexible because now let's say we wanted to print this sentence for another user so instead of just hardcoding this instead of writing this explicitly in our script we can simply plug in a new username a new user age and a new new user interests list another thing I want to call out here is here we're seeing yet another way to write a string so we saw we could write a string with single quotes double quotes and triple quotes here is an example of a formatted string this is helpful when you dynamically want to convert a set of data types to a string here we're seeing username which is a string user age which is an INT and user interests which is a list all dynamically concatenated together as a string and then we can print the resulting string like this these days where a lot of AI code might involve creating prompts and sending prompts to large language models formatted strings are a very handy functionality when creating prompt templates for these types of use cases so far we've covered data types and then creating abstract representations of these data types as variables now let's see how we can write Python scripts scripts are essentially text files with thep extension the simplest way to create a python script is to go to a basic text editor and instead of saving your text file with the txt extension you just change it to a py extension and it'll get saved as a python file which can then be understood by your python installation as being a program as opposed to a text file however this isn't always the most convenient way to write python script scripts rather it's common to use an IDE which stands for integrated development environment and all this is is a application that enables you to write Python scripts and really any other script for a programming language some very popular ones in the space are VSS code py charm and among data scientists Jupiter lab is very popular just to show you what that looks like here we have VSS code I'm going to open a folder I'm going to open this folder called python quickart which is freely available on the GitHub and so we see that there already some files here but I'm actually just going to create a new one so I'll create a new one and I'll call it python example and then I'll be sure to create it with thep extension and now we'll have this script here and we can start writing a python script writing a program here I'm going to create the ceremonial first program that every developer writes which is print hello world what this is going to do is if we go to our terminal I'm going to quit out of python I'm going to clear this I'm going to hit LS which allows us to see all the folders and files in the current directory and we see that our python example.py is sitting right here I'll type Python and then I'll copy paste this here we run that we see our program prints hello world that's the simplest python script that we might create and I used vs code to do that but you can of course use whatever IDE or application you like to create Python scripts another one that I've been playing around with recently is called cursor which kind of went viral on Twitter and the thing with cursor is that it's AI native it integrates large language models into the user experience to help you write code faster and of course there are other idees and co-pilots that do things like that as well next we're going to talk about loops and conditions at this point it'll probably be best if you're running these code examples to be typing these into a python script which you can run because these examples are going to run over multiple lines it won't be something that you can easily write and debug within the terminal itself and I can do that a little bit as we go along first let's talk about a loop and all a loop does is it runs a chunk of code multiple times one of the most popular Loops that you're going to C is a for Loop here's a very simple for Loop that's going to iterate over a sequence of numbers so Range Five creates a sequence of numbers of length five spanning 0 1 2 3 and four then what the for Loop does is that it's going to iterate along this sequence one element at a time then we're going to print each element as we go along so if we did this this is what would get printed out in the command line we can quickly see what this looks like with our VSS code script so let's just write 4 I in range five we'll make sure to put the colon there and then we'll just do print I and we'll save that then we'll open up our terminal again I'll hit clear and then I'm going to run that same command from before so Python and then python D example.py so if we print that we see our hello world from earlier but then we're also seeing each element in that range being printed out another example is we can iterate over items in a list let's say we have our user interests again of AI music and bread we're not just limited to iterating over numbers in a sequence we can also iterate over the elements themselves in lists if you're used to programming in c or one of the lower level languages this might seem like magic or something at least it was for me when I first started learning Python and this makes a lot of use cases pretty convenient when you can iterate over items themselves so if we run this script what's going to get printed out are each item in this list AI music and bread finally we can also iterate over items in a dictionary so if we create a dictionary and then here I'm going to iterate over the keys in this dictionary keys will extract each key from this dictionary and give us the ability to iterate over them so what this is going to be is a sequence of keys it's going to contain name age and interests so we're going to iterate over those and then we can access the values of the dictionary by using the key so here it's going to print the key itself like name and then we can print the value corresponding to that key using this syntax we have the user dictionary square brackets and then the key that we want to reference so if we run this this is the output that we'll get so we'll have name equals sha name is the first key sha is the value corresponding to that key then we have age and interests next let's talk about conditions which allow us to program logic with python a fundamental example of this are if else statements one way we can use this is if we wanted to check if the user is older than 18 for that we would simply write something like if the user's age here we're referencing the age value Val in the user dictionary here we're very intuitively checking if the value of the user's age is greater than or equal to 18 and then we have our colon here and then if this statement is true then python will execute the following command it will print user is an adult recall the value for the user age was 29 so this is true thus python should print this string here and indeed that's what happens alternatively we can also have an else not only will it do something if it's true if it's not true we can have python do another command here we're going to check if the user is older than 1,000 and if not we're going to print they have much to learn here the user's age is 29 which is not greater than or equal to 1,000 so instead of executing this command it's going to jump down to this else and print this and so this is what we'll see in the command line but we can also bring loops and conditions together so an example of this is if we wanted to count the number of users in a list that are interested in bread here we're going to define a user list which is a list of dictionaries and it's going to have two elements we'll have the first element being a dictionary corresponding to the user sha and then we'll have a second dictionary correspond responding to the user ify then we're going to initialize a count variable so we have this variable called count and we're going to set it equal to zero then what we're going to do is we're going to Loop through each element in this user list so we have two elements here so we'll start with this element and then go to the second element then for each user we're going to evaluate this condition we're going to see if bread is in the user's interest breaking this down for user one is going to be sha user is going to be a dictionary then we're going to extract the value corresponding to the interests key that's going to be this list here and then we're going to evaluate whether bread appears in that list in this first case bread indeed is in the list so Python's going to execute this line of code which takes the count variable which is currently zero it adds one to it and then it updates the count variable from being 0 to one and for people who are new to programming this may be unintuitive CU When we see equal sign we might think of math where the left hand side is supposed to equal the right hand side however equal here isn't saying that the left hand side is equal to the right hand side what it's saying is we're assigning this variable count equal to whatever is on the right hand side here so this is a very common way of updating a variable in a for Loop so we just did that for element one now we're going to go to element number two iy and we're going to run the same chunk of code so we'll evaluate the interests we'll see if Brad is in there and we can see that bread is not in iy's interests so nothing is going to happen it's not going to execute this line of code and it'll just continue so at the end of this we can print the count and what we'll see is that one user is interested in in bread and so you can already see through the very simple elements that we've talked about so far we've talked about lists strings integers dictionaries we've talked about for Loops we've talked about if El statements we've talked about printing values and just with these basic building blocks we're able to create this more sophisticated program that counts the number of users interested in bread next let's talk about functions these are operations we can perform on specific data types we've already seen some examples of functions one is the print function which simply prints the input to the command line here we have a for Loop that's going to iterate through all the keys in a dictionary and print the key and the corresponding value so here we're using user dict so if we recall it's just this dictionary here we run this script it's just going to print the key value pairs in this dictionary another example of a function is type which Returns the data type of an input variable here we're going to do a similar thing but instead of printing the value corresponding to the key we're going to print the type of the value if we do that what's going to Output is not the values themselves but the type of each value so name is a string age is an in in and interests is a list this is super handy because a lot of times you'll have these errors popping up in your code and simply printing variables and their types can often give you a better understanding of what's going on and help you debug the program another function is len which Returns the length of an input variable here we're going to print not the type of each value but the length of each value however when we run this we actually get a type error because this function Len is not defined for integers so although we got something for name the length of name was four because it consists of four characters Len is not defined for integers so it threw an error and as a bonus if we were to compute the length of interests we would get three because it consists of three elements Len is an example of a function that's only defined for specific data types so unlike print and type which are defined for every data type Len is not however there are several other examples of data type specific functions here's a set of functions defined specifically for Strings so let's say we have the string sha which lives in this dictionary here if we wanted to make all the characters in this string lowercase we could use this lower function that'll print the string like this we can similarly make every character uppercase and that'll print this we can also split strings to convert them into lists so we can split sha on H and a and what that'll create is a list with two elements corresponding to S and W another handy function for Strings is do replace which allows us to replace an arbitrary sequence of characters in a string with another so let's say we wanted to replace every W in this string with wh i n this will print shahen which which is my full name similarly there are a set of functions specifically for lists so let's say we have this list of interests here AI music and bread if we wanted to add an element to the end of this list we can use the append function if we did append entrepreneurship that would update the list to this if we wanted to remove a specific element from the list we can use the pop function and specify which element we want to remove so if we did zero that will remove the zeroth element of this list and return this finally if we wanted to insert an element into a specific place in the list we can use the do insert function specifying the element and where we want to insert that element so we did do insert one AI we would insert AI into the second place of this list and then finally we have a set of functions for dictionaries if we wanted to extract all the keys from a dictionary we can use the dot Keys function which is something we've already seen before so this will extract all the keys from a dictionary which we can iterate through using a for Loop for example we can do the same thing for values using the values function so now we see the values from the dictionary we can also access the items of a dictionary not just the keys or the values but the key value pairs that'll look like this and then if we print the items of the dictionary we see that name is no longer included then if we wanted to add a new item to a dictionary we can simply specify the new key name in square brackets of that dictionary and set that equal to a specific value so if we wanted to add name back in we could use this and then printing the items we can see that name has been added back into the dictionary however we're not just limited to Python's out of thebox functions we can also o create our own custom functions so these are called userdefined functions an example of this might be taking in a user dictionary and printing out all the users's information in a sentence so this is the same thing we did earlier using a formatted string however here we're going to define a function to do it you'll also notice that I Ed the triple quotes to create a string here that describes what the function does this is called a doc string and this is just a handy way to document what your function is doing so that other people can better understand your code and also helping your future self understand what this function is doing once we Define this we can use this function by passing in our user dict and it'll output this string which I call description which we can then print we print this we see it says sha is 29 years old and is interested in music however if we just wanted to do this once there's not a whole lot of upside in creating a custom function but if we wanted to generate this user description many many times functions are super helpful because now when we want to do the same thing for another user we simply pass in a new user dictionary to this user description function and print the result so if we want to do the same thing for iy we see it says iy is 27 years old and is interested in marketing next we can do something a bit more complicated and create a function to count the number of users interested in an arbitrary topic this will be pretty similar to what we did before but instead of just looking at the number of users interested in bread we'll have the topic of Interest be an input to the function here we'll initialize the count again as zero then we have our if condition within a for Loop so we'll Loop through an inputed user list we'll see if the inputed topic is in the user interests and if it is We'll add one to the running count then once we make make it through all the users in the list we'll return the count with this function we can define a user list so we'll have two users we'll have User Dictionary corresponding to Shaw and then we'll have new user dictionary corresponding to iffy and we'll select an arbitrary topic of shopping and then we'll compute the count using our userdefined function here then we'll print the count using a formatted string what the output of this will be is one user interested in shopping so far we've seen the power and flexibility of python and in fact python can be used to implement any arbitrary program but of course if we had to implement everything we wanted to do from scratch in Python this can easily become very timec consuming one of the key upsides of python is that there is a vibrant developer community and a robust set of open-source python libraries what this means is for virtually any anything you might want to implement in Python there's likely already an open-source library that exists for that so talking about the AI and data science space Here's a visualization of some handy python libraries of course this is a non-comprehensive overview but this is a set of libraries that will be helpful to any data scientist or AI engineer so under data frames which are convenient ways to structure data for analysis we have pandas and polers for data visualization we've got matplot lip Seaborn and plotly for basic machine learning we've got sklearn and XG boost for deep learning popular libraries are pytorch and tensorflow for more modern generative AI tasks there's the Transformers library and open AI API and then for web stuff for making API calls and formatting html text there's the request library and beautiful soup 4 respectively in order to install these libraries we can use pip which is Pyon 's default package manager so if we wanted to install say numpy which is a fundamental library for doing math in Python we can simply type pip install numpy at the command line with numpy installed we can do several things which are not possible with basic python the first is we can create this array data type which mimics a vector or a matrix from linear algebra so if we want to create a one-dimensional matrix we could do it like this if we wanted to multiply that matrix by two we could could simply multiply by two if we want to create a two-dimensional Matrix we can concatenate together multiple one-dimensional matrices here we see a 3X3 Matrix and then if we wanted to do matrix multiplication we can use the Matt mole function in numpy and so this will automatically do matrix multiplication for us and of course there's several other things that you can do in numpy which you can find in the example code on the GitHub repository and the last thing I'll say about libraries is creating virtual ual environments when using open- Source python libraries in your projects every project is going to have a set of dependencies a set of libraries that it needs in order to execute successfully for these types of projects it's a best practice to create a virtual environment all this does is keep track of all the libraries and their versions needed for your project to do that we do python dmvnv this is saying that we want to create create a new python virtual environment and then we can give it any name we like so here I called it my- EnV then we can activate this virtual environment with mac and Linux you can type Source my- EnV /bin activate or if you're in Windows you can just run this batch file like this then with that virtual environment activated you can install all the libraries you like using pip now let's get into the example AI project so up until this point we've just covered the basics of python we haven't done anything that could be considered AI so let's see what implementing a simple AI project might look like with python here what I'm going to do is write a program that can summarize and extract keywords from research papers this example along with all the other code Snippets we've seen so far in this tutorial are freely available at the GitHub repository linked down here and in the description below our first step is going to be to install all the required libraries for this example just like before where we did pip install and had a specific package name another way we can install libraries is from a text file that lists several libraries that are necessary for a project here I've created a requirements. text file available at the GitHub which lists all the libraries we can actually look at that here so I've opened it up in vs code and we can see that it lists several Library names along with their versions so when we run pip install dasr requirements what pip does is it goes line by line through this text file and installs each of these libraries and the correct version so we can actually see what that looks like real quick so we can do python DMV EnV and I'll call it my environment like before and then since I'm using Mac I can write Source my so whatever virtual environment name you chose bin SL activate now you can see that our terminal looks kind of different we have this my EnV sitting here what we can do next is to install all the requirements we see that I have the requirements file sitting right here in the current directory so I can install all of them like this so I'll do pip install Dash R and then I'll put the name of the requirements file.txt I'll run that then pip will go ahead and just automatically install all the project libraries then we can double check that by doing pip list so this will list all the packages and we can see that everything has indeed been installed next we can go into our python script and import the necessary libraries we're going to import fits which will allow us to extract text from PDFs we're going to use the open AI python API and then we're going to import Cy finally I will import my open AI API key here I have a separate python script called sk. piy and what I'm doing is importing a variable called myor SK myor SK is a string which contains my open AI API key I'm importing it in this way just for security reasons I don't want to hardcode my API key in the example code that's just not a good practice because if you forget that you put your API key in a script and you share it with someone or put it up on GitHub then other people will be able to use that API key and they might rack up a bill that you'd be responsible for just to see what this sk. py file looks like so if we go here I don't actually have my API key here because this is the version on GitHub but what you can do is just copy paste your API key into this python file and it should work fine alternatively if you're just running this locally and you want to do something fast you can just skip this step and just hardcode your API key but of course just be careful that you're not sharing that file with anyone we'll import this variable from this sk. piy script and then we'll set our open a. API key to my secret key next we're going to define a function that will read in the text from a PDF this function is actually kind of long so I'm going to break it down into three parts first I'm going to Define our function that's called extract abstract and if you're not familiar an abstract is like a summary paragraph of a research paper that is at the beginning of that paper extract abstract will take in one input which will be a string specifying the path of the PDF that you want to extract the text from given that PDF path we're going to grab the text from the first page of that PDF file the way we do that with this fits library is we do fits. openen PDF path as PDF basically what this is going to do is create this PDF object that allows us to access the text in the PDF so here we're going to get the first page of the PDF like this so we're grabbing the zeroth element from this PDF object and that'll correspond to the first page and then we can specifically grab the text from this first page using this function here. get text and then we'll just say text here now all the text from the first page is represented by this text variable next what we can do is extract the abstract from this text so text is just a string going to be a bunch of characters so what we want is to find where in that string the abstract lives the way I'm doing it here is that I'm going to find a start index and an end index basically at what element does the abstract start in the string and then at what element does the abstract end here what I do is I take all the text I make it all lowercase and then I do this find function so what this find function is going to do is going to find the word abstract in this text and then it's going to return the index number corresponding to where abstract is in this larger piece of text then we don't just want the start position we also want the end position of the abstract so this one's a bit more tricky because the PDF could come in many different formats could just have the abstract on the first page it could also have the abstract on the first page and then go into the introduction or many other possible situations so here what I do is if the word introduction exists in the text what I do is I find where introduction is then I set that as the end index however if introduction is not in the text I'll just set the end index as as none so basically we'll only care about where the abstract starts and then we'll just consider the text until the end of the first page what I do here is I grab the text starting from our start index all the way to the end index and then I use this strip function which will remove any trailing or leading white space from the text and then there's one more step because there's always the situation where the word abstract does not appear on the first page so we need to account for that basically what we're doing here is we're checking if the start index exists so instead of checking whether start index is equal to minus1 this is checking if start index is not equal to minus1 so if it's not equal to minus1 that indicates that abstract is on the first page of the PDF and so we can move forward with the abstract as we have it however if start index is equal to ne1 that means the abstract does not appear on the first page so we'll actually return none that's our extract abstract function next we're going to extract a summary and keywords given in abstract so we're defining another function here it'll take in the abstract we'll create a prompt using a formatted string The Prompt that we're going to send to gbt 40 mini is summarize the following paper abstract and generate no more than five keywords and then we're going to dynamically insert the abstract into the prompt then we're going to make our API I call here we're using open ai's chat completions API we can specify the model we want to use which here we use GPT 40 mini which is the cheapest and fastest model open AI has and then we'll send the model our prompt openi API has the ability to create system messages as well as user messages here we just have the system message as you are a helpful assistant and then the user message so this would be as if we went to chat gbt and type something in will be the prompt that we def find up here and then we set the temperature to 0.25 so loosely speaking temperature essentially sets the randomness of the model's responses a very low temperature means that the response will be very predictable and a very high temperature means that the response will be very unpredictable we'll make this API call and this is essentially like us typing this prompt into chat GPT and then getting a response we can extract the response like this the response will have a lot of different components to it but here we're going to extract the first choice we're going to extract the message from that First Choice and then we're going to extract the content from that message and then we'll return that as our summary and keywords finally we can bring everything together and so here we're going to grab the PDF path from the command line what this enables is that the user can type in the path of the PDF they want to summarize on the command line and then python will grab that text and store it as this PDF path variable and then we can pass the PDF path variable to our extract abstract function so this will return the abstract for us then we can use our summarize and generate keywords function to generate the summary however if the abstract is a nun type so basically the abstract did not exist on the first page of the PDF python won't execute this chunk of code and instead we'll jump to the else bit and print abstract not found on first page that's the end of the script once we've written that we can run that from the command line so we'll do python summarize DP paper. py so that's what I called the script in this case and then here is the command line argument that we're going to pass to this python script this is our PDF path so I have this files folder and in that folder I have a PDF called attention is all you need which is a PDF of the famous Transformers paper and then if we run this this is the output we get at the command line so we get a summary of the attention is all you need paper and then we get a set of keywords courtesy of gp4 many here we covered a ton of information however there's still so much to explore when it comes to Python and Ai and I find the best way to navigate this ocean of information is to learn by doing in other words by building your your own AI project in order to do that here I'm going to share three tips first and foremost use Google and chat GPT generously I don't know a single programmer who does not use Google or chat GPT or some combination as a sidekick any time that they're coding something up and it's simply because that a lot of times you run into error messages that just don't make sense and this is especially true at the beginning of your python Journey where you just don't have a lot of experience these tools are just great ways to help you accelerate your learning and progress next is figuring it out is a key skill that you need to develop as a programmer what this comes down to is just having the right mindset that even though something doesn't make sense to me right now I'm capable of figuring it out instead of saying things like I don't know how that works change your mindset to I don't know how that works yet so often we are just one Google search or one explanation away from unlocking something that doesn't make sense and this is something you'll run into over and over again as You Learn Python finally take the example code that I shared here and hack it to create your first AI project now that we have these large language models which are capable of solving so many problems and accomplishing a wide range of tasks integrating them into your Python scripts enable you to offload a large amount of software development and logic to the large language model which allows you to move a lot faster if you enjoyed this video but you want to learn more check out the blog and towards data science there I covered details I probably missed here and although this is a member only story you can access it completely for free using the friend Link in the description below I'll also call out the GitHub repository which contains all the example code that we saw in this tutorial and as always thank you so much for your time and thanks for watching"
FLkUOkeMd5M,2024-09-01T01:53:13.000000,Compressing Large Language Models (LLMs) | w/ Python Code,large language models have demonstrated impressive performance across a wide range of use cases while this is largely due to their immense scale deploying these massive models to solve real world problems can be challenging in this video I'm going to discuss how we can overcome these challenges by compressing llms I'll start with a highlevel overview of key Concepts and then dive into a specific example with python code and if you're new here welcome I'm I make videos about data science and Entrepreneurship if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make last year the Mantra and AI seemed to be bigger is better where the equation for creating better models was more data plus more parameters plus more compute and we can see over time large language models just kept getting larger and larger this is a figure from reference 11 down here and we can see over time models kept getting bigger and bigger so in 2018 large meant something around 100 million parameters in 2019 with gpt2 we were up to 1 billion parameters then came gpt3 which was around 100 billion parameters then we have more recent language models which have a trillion parameters or more there's no doubt that this equation actually works GPT 4 is objectively better than gpt3 and everything that came before it however there's a problem with creating bigger and bigger models simply put bigger models come with higher costs so just to put this into computational terms a 100 billion parameter model is going to take up 200 GB of storage and then if you want to use this model you got to fit this massive thing into the memory of your machine so needless to say this comes with high compute costs so it's probably not something that'll run on your laptop you're going to need a lot more compute than that it comes with higher Financial costs and then of course it comes with a higher environmental cost but what if there was a way we could make these Large Scale Models a lot smaller this is the motivation of model compression which aims to reduce the size of a machine learning model without sacrificing its performance so if we're able to pull this off by taking a massive model and shrinking it down to a smaller model this means we could run one of these models on our laptop or even on other devices like cell phones or SmartWatches or other types of devices not only does this Foster greater accessibility for this technology it also promotes user privacy because these models can be run on device and user information does not need to be sent to a remote server for inference this also means less Financial cost and of course the negative environmental impact can be a lot smaller here I'm going to talk about three different ways we can compress these models the first is quantization the second is pruning and the third approach is called knowledge distillation starting with quantization although this might sound like a scary and sophisticated word it's a very simple idea quantization consists of lowering the Precision of model parameters and you can think of this like taking a high resolution photo and converting it to a lower resolution one that still captures the main features of the image and to put this into computational terms you might take a model whose model parameters are represented in fp32 so using 32 bits and translating the parameters into int 8 just to get a sense of this the number seven represented in fp32 looks something like this so the computer has to keep track of all these binary digits just to encode a single parameter however if that same parameter is represented in in eight that'll look something like this it's a fourth of the memory footprint if you want more details on how quantization Works under the hood I talked more about it in a previous video of this series on Q Laura I'll link that here for those who are interested when it comes to implementing quantization on large language models there are two popular categories of approaches the first is called post trining quantization and the basic idea here is you train your model and then quantize it the key upside of this is that this allows you to take models that other people have trained and then you can take them and quantize them without any additional training or data curation using this approach you can take these off-the-shelf models that might be encoded in fp32 and convert the parameters to 8bit or even 4bit however if you want to compress Beyond 4bit post-training quantization typically leads to a degradation in model performance for situations where even more compression is needed we can turn to another set of approaches known as quantization aware training this essentially flips the order where one quantizes the model first and then trains IT training models in lower Precision is a powerful way to get compact models that still have good performance this means parameters can be encoded with even fewer than four bits for example in reference number six the authors were able to create a one bit model that mat matched the performance of the original llama model but of course the downside of quantization aware training is that it is significantly more sophisticated than post-training quantization because one has to train the quantized model from scratch the second compression approach is pruning which consists of removing unnecessary components from a model so an analogy here is that pruning is like clipping off dead branches from a tree it reduces the tree's size without harming it to put this in terms of parameters we might have a 100 billion parameter model but then through pruning we might reduce it down to 60 billion parameters while there are a wide range of pruning approaches out there they can be broadly classified into two categories the first is called unstructured pruning which consists of removing individual weights or parameters from the model showing that visually if this is our original model here unstructured pruning might consist of zeroing out the these two weights in the model the key benefit of unstructured pruning is that since it's operating on this granular scale of individual weights it can result in a significant reduction of non-trivial model parameters however there's a key caveat here since we're just taking model weights and turning them into zeros this is going to result in sparse Matrix operations to get predictions from our model in other words The Matrix multiplications involved in generating a prediction from our model will consist of a lot of zeros and this isn't something that normal Hardware can do any faster than nonsparse Matrix operations which means that one needs specialized Hardware that's designed to optimize these sparse Matrix operations in order to realize the benefits of unstructured pruning on the other hand we have structured pruning which instead of removing individual weights it removes entire structures from the model this can be think things like attention heads neurons or even entire layers so visually what this might look like is if this is our original model we might remove this entire neuron from the model which does not result in the sparse Matrix operations that we see in unstructured pruning while this does result in less opportunities for model reduction it allows one to completely remove parameters from the model if you want to explore specific unstructured and structured pruning techniques check out out reference number five which provides a nice survey of these approaches the final way we can compress an llm is via knowledge distillation this is where we transfer Knowledge from a larger model into a smaller one this is just like how we all learn at school where we have a teacher who has much more experience in a particular subject transferring their knowledge to the students in the context of large language models the teacher model might have a 100 billion parameters which are then distilled to a student model which might have just 50 billion parameters again there are a couple of ways that we can achieve this the first is using soft targets which consists of training the student model using the logits from the teacher model what that means is let's say we have our teacher model here and let's say it performs sentiment analysis so given a chunk of text it'll label that text as either positive sentiment or negative sentiment the way these model models work is that the raw outputs are not just a positive or negative prediction but rather there's a prediction for each class known as a logit for example let's say the logit for the positive class is 0.85 and the logit for the negative class is minus 0.85 what this is indicating is that the input text is more likely to be positive sentiment than negative sentiment and this is exactly how text generation models like l 3.1 or gp4 work under the hood however instead of having two output logits these models will have tens of thousands of output lits corresponding to each token in its vocabulary these lits are then converted into probabilities and then these probabilities can be sampled to generate text one token at a time so we can actually use these loits to do knowledge distillation so the way that works is we'll take our smaller student model have it generate predictions and then we'll compare those predictions to the teacher model's predictions for the same input text and the reason these are called Soft targets is because the predictions of the student model aren't compared to a zero or one ground truth but rather a softer fuzzier probability distribution this turns out to be an effective strategy because using all the output logits from the teacher model provides richer information to the student model to learn from another another way to achieve knowledge distillation is instead of using logits to train the student model one can use synthetic data generated by the teacher model a popular example of this was the alpaca model which took synthetic data generated from the original chat GPT and used it to perform instruction tuning on llama 7B in other words chat GPT was used to generate these input output pairs of input prompts from users and out put responses from the model which were then used to endow this llama 7B model with the ability to follow instructions and follow user prompts so now with a basic understanding of the key Concepts behind model compression let's see what this looks like in code as always the example code here is freely available on the GitHub additionally all the models derived here and the data set used for training is also freely available on the hugging face Hub and we'll be using python for this example as well as pytorch the example here is we're going to take a text classifier and compress it using knowledge distillation and quantization so that's one thing I actually forgot to mention is that these three different compression approaches of quantization pruning and knowledge distillation are often independent of one another which means that we can combine multiple approaches to achieve maximum model compression so here I'm going to combine knowledge distillation with quantization to achieve a 7x reduction in model size the first step here is to do some imports many of these are hugging face libraries so data sets is from hugging face we'll import some things from Transformers we're going to import some things from pytorch and then finally I'm going to import some evaluation metrics from psyit learn then I'll import the data set with one line of code and this is something I've made available on the hugging face Hub it consists of a training testing and validation data set with a 70515 split so it's 2100 examples of data in the training data set and then 450 examples in the testing and validation and the data set consists of two columns the First Column are website URLs and the second column is a binary label of whether that URL is a fishing website or not a fishing website so this is actually a very practical use case used by email providers or cyber security folks that may want to ensure that links are safe before presenting them to end users with the data loaded in we'll load in our teacher model here to speed things up I used the freely available GPU on Google collab so I'm importing that GPU as a device here next I'm going to load in the teacher model which is a model I ftuned on This fishing classification task we can load in the model's tokenizer and the model itself using these two lines of code here and then using this two method I'm loading the model onto the GPU then we can load in our student model so here we're going to create a model from scratch I'm going to copy the architecture of distill bir to initialize the model however I'm going to drop four of the attention heads from each layer additionally I'm going to drop two of the layers from the model in other words each attention layer has 12 attention heads so I'm going to reduce that to eight and the original architecture has six layers and I'm going to reduce that down to four then I'm going to use this distill BT for sequence classification object what that does is it'll load in this distill BT architecture with these modifications and then slap on top of it a classification head in other words the model instead of generating text is going to perform text classification we're also going to load the student model onto the GPU and just to get a sense of the scale here the teacher model has 109 million parameters and takes up 438 megab of memory while the student model here here consists of 52.8 million parameters and takes up 211 MB of memory the reason I'm using relatively small models by today's standard is that this is what I can easily run on the free GPU on collab but if you have beefier gpus or more compute at your disposal you can take this code and just plug in bigger models and it should work just fine so the data set that we loaded in consists of plain text with a label so before we can actually use the this data we'll need to tokenize it here I defined a simple pre-processing strategy so what's happening here is each URL is being converted into a sequence of Tokens The Tokens are being truncated so they're not too long and then within each batch of examples the shorter sequences are going to be padded so all the examples have the same length and this is important so we can convert it into a pytorch tensor and efficiently do the computation with the gpus this the pre-processing function the the actual transformation happens in this line of code so we take the data set and then we map it into tokenized Data making sure that we are using batches and then we're converting it into a pytorch format where we have columns for the tokens the attention mask and the target labels another thing we need to do is Define an evaluation function so this will allow us to compute evaluation metrics during model training and so there's a lot happening here so I'm going to go through it line by line first we're putting the model into eval mode instead of training mode we're initializing two lists one list is for model predictions another list is for labels here we're going to disable gradient calculations then batch by batch we're going to do the following first we're going to load all the data onto the GPU so that's the input tokens the attention mask and the labels then we're going to perform the forward pass so we're going to compute model outputs and then we're going to extract the logits from the outputs this lits variable here will actually consist of two numbers one one corresponding to the probability that the URL is fishing and another corresponding to the probability that the URL is not fishing so in order to turn this into a binary classification in other words the URL is fishing or is not fishing we can take the ARG Max of this logits variable and then that'll be our prediction and then we can append the predictions and the ground truth label to these lists we initialized earlier once we do that for all the batches we can compute the accuracy precision recall and F1 score for all the data in one go next we're going to define a custom loss function and the way we're going to do that is we're going to use both soft Targets in other words the logits from the teacher model and the ground truth labels and so the way we're doing that here is we're going to compute a distillation loss as well as a hard loss and then we're going to combine those into a final loss so to get the distillation loss we'll first compute the soft targets so these are the teachers logits and then we're going to convert those logits into probabilities in order to generate probabilities from the teacher models logits we can use the soft Max function and it's common practice to divide the teacher logits by a temperature parameter which will increase the entropy of the probability distribution so we generate a probability distribution corresponding to the teacher's prediction and then a probability distribution corresponding to the students prediction and now that we have two probability distributions one from the teacher model one from the student model we can compare their differences using the KL Divergence pytorch has a built-in method that does that so we can just easily compute the difference between these two probability distributions using this line of code here and then we can compute the hard loss so instead of comparing the student model's predictions to the teacher model's predictions we're going to compare them to the ground truth label and then we'll use the cross entropy loss to compare those probabilities distributions and then finally we can combine these losses by adding them together and adding this Alpha parameter which controls how much weight we're giving to the distillation loss versus the hard loss next we'll Define the hyperparameter so here I use a badge size of 32 put the learning rate as .001 we'll do five Epoch we'll set the temperature that we use in our loss function at two and then we'll set the alpha so the relative weights of the distillation loss versus the hard loss as 0.5 so we'll give equal weight to both types of losses then we'll Define our Optimizer so we'll use atom then we'll create two data loaders we'll have a data loader to control the flow of batches for the training data as well as the testing data then we'll train the model using pytorch so we put the student model into train mode and then train it we have two for Loops here so we have one for the epoch one for the batches and it's a similar thing as to what we saw in the evaluation function so we'll load each batch onto the GPU we'll compute the outputs of the teacher model and then since we're not training the teacher model there's no need to calculate gradients so we can avoid that using this syntax here then we'll pass through the student model to generate its outputs and extract its logits we'll compute the loss value using our distillation loss that we defined earlier and then we'll perform the back propagation sorry the script was too long so I have to extend it like this but once we make it through every single batch we can print the performance metric tricks after each Epoch so we'll print the accuracy precision recall F1 score for the teacher model and then the accuracy precision recall F1 score for the student model and then we'll be sure to put the student model back into train mode because this evaluate model function that we defined earlier puts it into eval mode so I know this was a ton of code maybe way more code than you were hoping to get into but here are the results of the training so we have five Epoch here and we can see the loss is going down which is a good sign so it bumped up in Epoch 4 but then it dropped back down in Epoch 5 which is very normal and then we can compare the performance of the teacher and student models so of course since we're not updating the teacher model its accuracy is going to stay the same across all Epoch cuz it's not changing but we can see the student model performance get better and better across each Epoch and then once we get to Epoch number five the student model is actually performing better than the teacher across all evaluation metrics next we can evaluate the performance of the teacher and student models using the independent validation data set so the training set is used to update model parameters the testing data set is used in tuning the hyperparameters of the model and the validation set wasn't touched so this will give us a fair evaluation of each model and for that we again see that the student model is performing better than the teacher model across all evaluation metrics this is one of the other upsides of model compression if your base model if your teacher model is overparameterized meaning that it has way too many internal parameters relative to the task that it's trying to achieve actually compressing the model not only reduces the memory footprint but also it can lead to better performance because it's removing a lot of the noisy and redundant structures within the model but we can go one step further so we did knowledge distillation let's see how we can quantize this model first I'll push the student model to the hugging face Hub and then we'll load it back in using the bits and bytes integration in the Transformers Library so we'll use the bits and bytes config so we'll load it in for bit we'll use the normal float data type described in the Cur paper and all this is is a clever way of doing the quantization to take advantage that model parameters tend to be normally distributed so you can be a bit more clever in how you quantize the values and I talk more about that in the Cur video that I mentioned earlier next we'll set the compute data type as brain float 16 and then finally we'll do double quantization which is another thing described in the Cur paper once we set up this config we can simply just load in our student model from the hugging face hub using this config file so the result of that is we have still the same number of parameters 52.8 million but we've reduced the memory footprint so we went from 21 megab down to 62.7 megab then comparing that to our original teacher model we started with we cut the number of model parameters in half and then we reduced the memory footprint by about 7x but we're not done yet so just cuz we reduced the model size that doesn't mean that we still maintain the performance so now let's evaluate the performance of the quantize model here we see we actually get another performance game post quantization so intuitively we can understand this through the aam's razor principle which says that simpler models are better so this might be indicating that there's even more opportunity in knowledge distillation for this specific task all right so that brings us to the end if you enjoyed this video and you want to learn more check out the blog in towards data science and although this is a member only story like all my other videos you can access it completely for free using the friend Link in the description below additionally if you enjoyed this video you may enjoy the other videos in my llm series and you can check those out by clicking on on the playlist linked here and as always thank you so much for your time and thanks for watching
kmrekqjWE8o,2024-08-26T14:15:08.000000,Why You Should “Keep Going” #entrepreneurship,you often hear entrepreneurs say things like keep going and just don't quit here's statistically why this is good advice this plot shows survival rates of businesses established in 1994 all the way up to 2003 it demonstrates the low odds of entrepreneurial success with 50% of businesses closing within 5 years however something interesting happens when we look at this same curve for businesses that have been around for a few years here we see the curve incrementally moving up the longer the business stays alive in other words the probability of a business's survival increases the longer it survives for example although only 50% of new businesses make it to year five these odds jump to 57% just after one year in business so this is a reminder to all the entrepreneurs out there keep going each day that you survive your odds of success go up
3JsgtpX_rpU,2024-08-16T14:02:17.000000,3 AI Use Cases (that are not a chatbot),"the roles that have the highest score are it professionals it seemed like it professionals was a promising customer avatar for these workshops we have Consultants they seem like a promising customer here and then finally data analyst seems like a promising Avatar these actually were the three avatars that seem to be most interested in this type of Workshop large language models have taken over the business world and all the big companies are trying to use generative AI although tools like chat GPT are clear cly powerful what's not so clear is how businesses can use this technology to drive value in this video I'm going to talk about three ways to use AI for something that every business cares about sales I'll talk about each use case at a high level and then dive into example python code using real world data and if you're new here welcome I'm Shaw I'm a data scientist turned entrepreneur and if you enjoy this content please consider subscribing that's a great no cost way you can support me and all the content that I make for most businesses that I've interacted with using AI typically means building a chatbot a co-pilot an AI assistant or agent while these are great solutions to some problems they are far from a cure all a key challenge is that large language models are inherently unpredictable getting them to solve a particular problem in a predictable way often comes at a major cost for example popular tools like chat GPT and GitHub co-pilot were notoriously losing money in 2023 however businesses can use AI for more than just building chatbots and the like here I'm going to talk about three AI use cases that are not a chatbot these are all going to be in the context of sales which is something that every business has to do the first use case is data augmentation the second is structuring unstructured data and the third one is lead scoring I'm going to talk through each of these use cases one by one and then dive into some example python code implementing these use cases on real world data from my business data augmentation consists of adding data to your data set this can look two different ways you can either add more variables to your data set so here more columns or you can add new rows or examples and the reason this is powerful is because businesses can use data to solve problems and make decisions for example a popular business are data vendors who sell data to companies to help them make decisions one of the most popular data vendors that we all know is FICO they generate these credit scores for basically every potential lender and then sell them to financial services companies that are writing loans for people another reason data augmentation is valuable to businesses is because it can help augment their machine learning and data analytics efforts an example of that in a sales context is let's say we have have a list of names and a resume for each person on that list if we wanted to do some kind of analysis on these resumés we might want to extract key information such as years of experience or industry from each of these resumés there are a few ways we could go about this so one way is we can just read every resume and manually write out the years of experience or the industry of that person but of course this would be very tedious and if you have thousands or tens of thousands of resumés this is not something that's practical another idea is to try to automate this process with some kind of computer program but if you have resumés coming from many different sources this also may not be feasible because people format their resumés in many different ways so this is one area that large language models can help because they can better comprehend a body of text like a resume to extract things like years of experience or industry more readily from text another key AI use case that a lot of people are talking about is structuring unstructured data so what does that mean structured data is simply data that can be represented by rows and columns typically consisting of numbers on the other hand unstructured data cannot fit into a table examples of this are text documents PDFs audio files images these type of data are not organized in a table the reason that this distinction is important is because when it comes to using data it's much easier to extract insights from it and do analysis is on it when it's in a structured format while if it's unstructured you're going to have to do some extra steps before you can do any kind of analysis on that data one of the exciting things we're seeing these days with large language models is the Improvement of the so-called text embeddings and if you're not familiar text embeddings I got a whole video on it so if you want to dive into the details you can check out that video but at a high level all a text embedding does is it takes a chunk of text and translates it into a meaningful set of numbers so basically what this means is we can take a stack of résumés or any type of text and organize it into a structured format into a table that we can readily analyze and then the final use case I'm going to talk about is lead scoring essentially what this consists of is taking customer data and from that information generating a prediction of How likely it is that that customer will buy your product or service what this typically looks like is you'll take data such as job title the revenue of the company that they work at customers's Behavior how much they interacted with your website or social media platforms or something like that and then where the lead came from as well as many other potential indicators and all this information is synthesized into a single metric that represents How likely it is that that customer will buy your product while lead scoring has been around for a very long time it's not something new that came around with large language models however the opportunity that large language models have presented is not so much in generating the score itself but rather improving the inputs that we can pass into lead scoring models now I want to bring together all three of these use cases and apply them to a case study from my business so I've recently been trying to validate doing AI workshops as a new Revenue source for my business my first step in validating this idea was reaching out to a 100 people on LinkedIn the Outreach script look something like this I'd say hey I'd add some personalization and then just ask for their feedback on doing a AI workshop and gave them some details so from these 100 DMS 58 people responded which is pretty good 18 people said yes they would attend and 19 people said no they wouldn't attend then everyone else who responded is basically a maybe the goal of this initial Outreach is to identify promising customer avatars so what I did here is I sent out DMS to a wide range of people of all different job titles and backgrounds and then did an analysis trying to find the common factors between the people that responded said yes and said no here I'm going to walk through the code of this analysis which involves the three use cases we saw earlier and as always the code is freely available at the GitHub repository however I can't share the data because I was analyzing people's resumés and don't want to share that publicly due to privacy concerns okay so here we have the example code for the first use case we start as always by importing some helpful python libraries I'm using polers to organize the data in data frames I'm using the open AI python API to do the data augmentation and then finally I import numpy the first step here is loading the data there was actually a Step Zero which consisted of extracting text from people's resumés where I got the resumés was actually from LinkedIn so if you go to anyone's LinkedIn profile and you go to more and then you do save to PDF what will happen is LinkedIn will aut automatically generate a resume of that person which will look something like this for everyone that I reached out to I went and manually downloaded this resume and then I have this python script here which will actually crop the resume to only keep this white area and extract the text from it that's what this code does it extracts the text and then it saves it in a polar data frame consisting of two columns one corresponding to the name and then the second Corr responding to the text from the resume so that's what we're importing here I saved it to a CSV file and so now I'm reading it in as a data frame at this point the only two variables in our data frame are the names and the resumés so let's see what it would look like to try to extract the years of experience from the text in people's resumés here I'm going to use the open AI python API to do this that'll involve us defining two different prompts so I have this system prompt which will be used in open ai's chat completions API this is the system prompt you are a resumé analysis assistant your task is to classify RS into one of the following experience level buckets based on the number of years of professional experience listed in the resume and then a defines these five buckets and then it gives some more instructions so I actually generated this system prompt using chat GPT I just asked it to create me a system prompt for chat GPT and I think that tends to be a pretty good starting point for prompts because Lar language models tend to think differently than we think so it's better to have data generated from a large language model if you're going to pass it in to another large language model typically just a rule of thumb but we also need a user prompt that we're going to pass into the model the way of implemented that is using this prompt template so this is essentially a function that takes in the text of a resume and then dynamically inserts that text into this prompt then we can just pass this whole thing to the open aai API to generate a response what this prompt is asking is to take the text from this resume and then output either 1 2 3 4 or five based on the levels of experience so once we have that we can pass each resume over to the open AI API to generate a response to basically classify each person's level of professional experience and so the way that looks is very similar to what we saw in the AI assistance video so if you haven't checked that out I'll link it on the screen here basically what's happening is we're going through the data frame one row at a time we are extracting the text from the resume passing it into the prompt template and defining that as our prompt and then what we do is we pass the system prompt and the user prompt both to GPT 40 there are also a few arguments that we can specify which will improve the responses from the model so here I just want the model to respond with a single number however large language models like I said earlier are inherently unpredictable you'll ask it to do a very straightforward task but then it'll have some weird formatting maybe instead of just returning the number it'll first try to put experience level colon and then the number in its completion so that's a very reasonable thing to do is just trying to format the text in a nice way but if we're trying to parse the responses from the model in a predictable way that's going to cause some problems the way I control for that is I just set the max number of token that the model can respond with to one which will hopefully correspond to a number between 1 and five I set the number of completions as one so this will only generate one completion but you could also have it generate multiple completions and then you can pick the completion that was returned most frequently and then finally I set the temperature at 0.1 and temperature essentially increases the randomness of the responses so a very high temperature will generate completions that don't make any sense and then a very very low temperature will generate completions that are very predictable so what I do is for each resume I generate the completion and then store it in this experience level list it's also worth calling out that there are multiple models we can use here I use GPT 40 That's apparently the most intelligent large language model open AI has I also tried GPT 40 mini which was significantly faster and it's about 20 times cheaper we can see this here I think I ran it twice with GPT 40 mini which cost a total of 4 cents but running it twice with GPT 40 cost 1.20 so it's about 30 times more expensive to use GPT 40 than GPT 40 mini so which makes the most sense will just depend on your use case in an earlier version of this I used GPT 40 mini but instead of just generating one response I generated three responses although I didn't do a proper comparison of the two I feel that the result of running GPD 40 mini multiple times was similar to running gp40 just once for a small use case like this it's not super important which is better GPT 40 mini done three times versus GPT 40 just run once but you can imagine if you're working with a lot more data and it's a difference of $6,000 versus $60,000 then that's something that'll make a huge difference okay so this experience level list all the elements are strings so I just convert them to integers and store them in a numpy array and then they look like this this and then I add this numpy array as a new column in the original data frame this is what it looks like there people's names here so I have to blur it out but now we went from just having name and resume to having name resume and experience level but of course we could have done other things like job title industry and other pieces of information that are obvious by looking at someone's resume but if you have a th000 or 10,000 resume is not viable for a human to go through and read everything you can just pass it off to a capable large language model to extract that information dynamically use case number two structuring unstructured data although extracting data from text in the way that we just did is sort of structuring unstructured data using text embeddings is a lot cheaper and captures much more information from the underlying text here I'm going to do some imports again we're going to use polers for the data frames and then I'm using this sentence Transformers Library so open AI has text embeddings via their API but you know their API money on the other hand sentence Transformers is completely free and open source so I'm going to use that here here I'm going to load in the data that we just generated from the previous notebook and then doing a couple things here so here I'm doing some string manipulation to the rume text I'm removing any trailing white space and then I'm removing the first line which includes the person's name the person's first and last name probably doesn't have a whole lot of useful information for us in doing these text embeddings so it's better to just remove it and then also added this thing here to replace the people's names with numbers so I can show it in this example one of the great things about the sentence Transformers library is that they have a wide range of compatible embedding models that you can choose from all mini LM L6 V2 is by far the most popular but then there's this other one that I'm using here which is paraphrase multilingual mpet base V2 and the reason I'm using this is that it works on both sentences and paragraphs but of course there are many other text embedding models that you can choose from I like to go to hugging face and look at their model repository if you select this filter sentence similarity all the embedding models will come up so we see that all mini LM L6 V2 is here this seems to be a very popular one I haven't used it before it's from by it's called BG M3 Alibaba has it nomic AI so there are tons of embedding models so many from sentence Transformers here so we're going to load in the model using the sentence Transformers module we can generate the embeddings like this what is happening here is we take the resumé column from the data frame we convert it to a list and then we pass it into the embedding model to generate the embeddings and then here I'm doing some fanciness so I'm defining a schema for a new polar data frame to organize all of these embeddings and then I append this DF embeddings data frame to the original data frame then it looks something like this we have the name here the resume and the experience level so this is everything we generated in the first notebook but now we we have all these embeddings there's 768 of them so what we've done now is essentially converted this resumé column into a set of numbers that we can use for further analysis which leads us right into use case number three which is lead scoring and customer segmentation here I've got a whole lot of imports we've got polar and numpy again mat plot lib will allow us to make some plots for training the lead scoring model and some machine learning stuff I'm going to use sklearn I've got random Forest classifier imported logistic regression the Au score which is a way we can evaluate binary classification models and then finally a function to generate a train and testing data set so first step is we're going to import the data that we just created in that previous notebook but now we're going to import Outreach data so we haven't seen this yet what this consists of is a list of everyone's names their rle kind of defined by me whether they responded to the Outreach and whether they said yes no or maybe that's what's in this DF Outreach then we're going to join these two data frames together so we join DF and DF Outreach and we'll drop any duplicates then again I have this line of code here to replace people's names with numbers so I can show it in this example code so the basic idea of lead scoring is that you're going to take some inputs like job title revenue of company customer Behavior so on and so forth and you want to predict the probability that that person buys your product I'm actually going to do something a little different since I don't have sales data I only have whether they said yes or no to the offer I'm going to train two different models one model is going to predict the probability that a person will say yes to my offer based on their resume and then another model to predict the probability that a person will say no to my offer based on their resume and the resumés will be represented by all the different text embeddings which is what's happening in this line of code here as well as experience level which we generated in that first notebook but as a first step since we have 768 of these embedding Dimensions I'm going to do a dimensionality reduction and so the way I'm going to do that is I'm going to train this auxiliary model so this model isn't going to predict the probability of someone saying yes or no but rather it's just going to predict the probability that they resp respond to my Outreach and here I'm going to use a random force classifier and I set a very large number of estimators with a pretty narrow depth just because I'm trying to get a sense of the most important predictors in predicting whether someone will respond or not do that in two lines of code so we initialize the classifier and then we fit it to our data we can then evaluate the classifier using the Au score Au score of 100 means it's perfect but of course it's overfitting because because we used all the data to train it but we don't really care about the performance of this model cuz we're not actually going to use it directly the only reason I trained this model is so I can get a sorted ranking of the most important features now we have a list of the features which are most helpful in predicting whether someone will respond and then these features are least helpful and it's funny that the experience level variable that we went through so much trouble of generating is one of the least predictive variables here which kind make sense so the people that would attend this Workshop they're from all different experience levels they'll be complete beginners or people who have been working in some field for decades now that we have a sense of which predictors are most helpful now we're going to train the yes and no models what I do first is I'm going to create a new data frame that only involves people who responded to the Outreach because if someone didn't respond then they had no way of saying yes or no so it's not really fair to put them into the training data from this new data frame I'll Define two columns one corresponding to whether they said yes and then another corresponding to whether they said no once I have that I can train the yes and the no model here I do it in a little sophisticated way I do it in a for Loop just because I didn't want to copy paste the same code for the yes model and no model cuz it's very similar so what I do here is I initialize a list that will save the classification models here I have a list of the target names the target names are what we just created here yes and no and then this is sending how many of the variables to use in the yes and no models and so again we have 769 variables that we could use but here I'm only going to use the 75 most predictive based on this initial analysis I did up here and then I initialize two more lists to store the evaluation metrics of the models based on the training and testing data and then here I Define the feature names to use in these models this is a data frame involving the most important features and then I'm just going to extract the variable names up until the 75th row so I'm going to essentially grab the 75 most important predictors now I just do this little for loops and so the first model corresponds to the yes model so this will get passed in I'll create two data sets have the predictors here and the target here then I'll create a train test split where I use 20% of the data for testing 80% for training then I'll train this logistic regression model and then once that model's trained I'll just append it to this list I initialized earlier and then I'll compute the Au score for the train and testing data set this will happen for the yes model and the no model and then at the end of that this is the result this is the performance of each model so the first is the yes model the second is the no model this is the performance on the training data and then this is the performance on the testing data so the big biggest problem with this analysis here is that there are only 100 examples really for something like this and we have so many embeddings you probably want at least a thousand examples or on that order but I still think there's some clues that can be extracted from this small data set at this point we've trained both a model to predict the probability that someone will say yes and the probability that someone will say no and so what I do here is I apply both of these models to the entire data set that's generating this array yes score and this array no score which just has the probabilities of yes and no respectively for each person in that original data frame then I'm adding The Columns to the original data frame the first is corresponding to this yes score so essentially the probability that someone will say yes to the offer the no score is essentially the probability that a person says no and then I generate this score which I'm defining as the yes score minus the no score so it's kind of like you want to maximize the probability that someone says yes while minimizing the probability that someone says no so this is just like a hacky way of doing that and then finally I'm adding yes and no columns to the original data set okay so now we've generated these scores for every lead in the data set now let's do some analysis good thing to do in any kind of analysis is to plot a histogram here's a histogram of the scores higher score is better we can also break this down by role this is pretty insightful so we can see that the roles that have the highest score so basically they have the highest yes score and lowest no score are it professionals and this kind of agrees with my intuition through doing the Outreach it seemed like it professionals was a promising customer avatar for these Workshops the next role here is data architect but this isn't something we can take too seriously because there was only one person with this role in the data set next we have Consultants so this also aligns with my intuitions they seem like a promising customer here data manager seem promising but again there's only two of them and then finally data analyst seems like a promising Avatar these actually were the three avatars that I had written down in my notes that seem to be most open or most interested in this type of Workshop but it's not just about like who is the best customer but who's like the anti- customer who do you want to avoid and make sure you don't reach out to it's interesting like data scientists and dat students are among the bottom which again aligns with my experience data SS seem to be split like half people like are so anti- generative Ai and llms others are excited about it but neither is super interested in doing these workshops because they either already know the stuff if they're into it and if they're not into it they don't care to learn this stuff of course there are data scientists out there who would attend workshops like this but bu and large as a population they're not super into it and then students were another one typically students just don't have money money to pay for a workshop like this so that also aligns with my experience you know another thing we can do which I just thought of just so that experience level doesn't go to waste we can include that here as well so even though it wasn't very predictive we can still use it to segment the data a bit okay so this is interesting people with the least amount of experience so just beginners seem to score most highly and then these midc careers so if we go back to this data augmentation script one was the entry level folks so people with 0 to one years of experience and then three were the midlevel folks so people in their careers for a bit so they seem to be the most promising avatars based on this analysis and then everyone else has negative score so more likely to say no than yes that's an interesting pattern of course like there are only four people that have 0 to two years of experience so take that with the grain of salt and overall there are only 100 people in this data set so this whole analysis needs to be taken with the grain of Sal and then the last thing we can do is Define customer segments and so this is helpful because now what this allows you to do instead of like getting so granular for someone's score you know sales is a lot of times very unpredictable so the more precise your metrics are when it comes to this kind of analysis probably the worse off your strategy is going to be in practice it just makes your strategy very fragile if it's dependent on the third decimal place of this score that you generated using machine learning that's why it's really common to Define grades based on these scores so basically what you do is you take all the people in some sample you generate scores for them and then based on that sample you'll Define segments so you'll basically say anyone with a score higher than 0.05 will put in grade A anyone with a score below minus 0.05 will put in grade C and then everyone in the middle will put in Grade B the way I did it is that I put the bottom 20% in grade C the top 10% in grade A and then everyone else I put in Grade B so that's what this is showing here and then what we can do is we can use these grades and then apply it to New resumés or New Leads so as you identify a list of a thousand potential people to reach out to or 10,000 or 100,000 or a million just like more leads than you could ever reach out to or maybe it costs a lot of money and effort to reach out to a lead so you want to like distill it down to the most important you can take resumés pass it into this pipeline of the yes score and the no score and then compute the grade now you have different segments of customers of a grade leads b-grade leads and c-grade leads this might be a better way of segmenting it because let's say we just did the roles I identified the Consultants data analysts and it professionals as good people to reach out to and then data scientists and data students as bad people to reach out to but then we see a data scientist appearing right here with a grade of a and they had a pretty solid yes score and a low no score it's just that they didn't respond and maybe if they had responded they would have said yes to the offer that's one of the reasons why doing the lead scoring and then segmenting customers based on the lead score can be beneficial Beyond some generalization of a particular role or something else okay so that brings us to the end again the code is freely available on the GitHub and a Blog will be coming out soon if you enjoyed this topic of real world AI use cases that are not a chatbot I have about a dozen other use cases that I didn't include here but I could include in a future video so if that's something that you'd be interested in please let me know in the comment section below and as always thank you so much for your time and thanks for watching"
bwW1I6JlI30,2024-08-12T17:47:20.000000,How to Learn Anything #learning #shorts,had a PhD in physics here's my four-step framework for learning anything step one consume content I'm always shocked by how much Clarity I can get from Reading one good article or watching One Good YouTube video this is usually enough to get the ball rolling on learning something new step two mentorship learning from those ahead of me is one of the greatest hacks I've discovered for developing a new skill set and the best info usually comes from those that are just one or two steps ahead step three do it learning from content and others can only take you so far eventually you need to put your learnings into practice and do it and step four teach in my view teaching is the ultimate way to learn forcing myself to construct a narrative around a set of ideas is a great way to get more clarity this is the number one reason why I make YouTube videos
3PIqhdRzhxE,2024-07-29T19:20:52.000000,Local LLM Fine-tuning on Mac (M1 16GB),with the rise of open- source models and efficient fine-tuning methods it's never been easier to build custom ml solutions for example anyone with a single GPU can now fine-tune a large language model on their local machine which is exactly what I did in a previous video of this series however since my machine is an M series Mac which doesn't have an Nvidia GPU I had to use the free GPU on Google collab to run that example this is somewhat disappoint pointing because using collabs free gpus is somewhat restrictive and not as convenient as running something on my local machine that's why in this video I'm going to share an easy way to fine-tune an llm locally on Mac and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make if you've been keeping up with machine learning over the past decade then you're probably familiar with Nvidia and all their different gpus it turns out that gpus are super helpful for machine learning because they can much more efficiently train and run machine learning models than traditional CPUs they've also demonstrated an ability to take Nvidia from a hundred billion valuation all the way up to 3 trillion in the past 5 years nvidia's dominance of the GPU Market has greatly influenced the available open-source tools for running and training neural networks the result of this is that a lot of open- source tools work seamlessly with Nvidia Hardware while this is great for Windows and Linux users it often leaves Mac users like me left sitting on these Sidelines after a failed attempt to locally fine-tune llama 2 on my local machine my impression was pulling this off is something that would take several hours of effort that was until I discovered the mlx python library mlx is a python Library developed by Apple's machine learning research team for efficiently running Matrix operations on Apple silicon so we see the documentation here it's inspired by Frameworks like pie torch Jacks and array file one of the notable differences with mlx is that it allows you to use the unified memory model of these M1 chips no longer do you have to worry about RAM and vram being separate things M1 chips just have a single memory so that means me with my Mac Mini M1 2020 with only 16 gigs of memory am capable of fine-tuning a large language model locally on my machine while mlx is a pretty low-level framework it's not going to have highlevel abstractions for loading and training models like hugging face for example there is this example implementation of Laura which is very readily hackable and adaptable to another use case which is exactly what I'm going to do here similar to the previous Cur video here I'm going to fine-tune a quantized version of mistol 7B instruct to respond to YouTube comments in my likeness however instead of using hugging face in Google collab here I'm going to use the mlx library and my local machine and again here are the specs of my machine it's a Mac Mini M1 from 2020 with only 16 GB of memory so by today's standards my machine is hilarious but despite that it's still good enough to implement ment this fine-tuning example so I've put together some example code and that's available on GitHub if you go to my YouTube blog repo go to the llm tab here we see all the different code and videos and blogs of this series so now there's a new one called kulur mlx so we click on that what I've done here is I have this notebook that walks through the example code and then I've taken all the scripts from that mlx example implementation so I've put them in the scripts folder and then here I've prepared data so this is data of my YouTube comments and I prepared it in this Json L format but we won't talk about this right now talk about it a little later okay so the first step in running this example is to go to the repo and clone it so I'll copy this Ur here I'll go over to my terminal let me zoom in a bit okay so we're going to clone the repo get clone might take a while unfortunately there's not a way to clone a specific subfolder of a GitHub repo if you you want to download some code from a repo you have to clone the entire thing go to the repo we just cloned and then the code is in the llms subdirectory and then it's Q mlx so here we see everything we've got our example code requirements file some scripts some data and a readme okay so I cloned the repo and I navigated to this folder so now let's create a python environment that allows us to run the code so here I'll create a new virtual environment called mlx DV and then I'll activate this environment so this how you do it in bash and zsh and I guess every Mac User uses this so this is how you'll activate the environment all right so now you can see we're in this mlx DV environment but the only thing in here is PIP so you see you do pip list all there is is PIP so let's install all the required libraries so we can just do the PIP install D requirements. text and this will install everything while that's happen happening we can look at what is in that so here we have mlx we have mlx LM so this is a library built on top of mlx specifically for large language models also we have the Transformers library and numpy and then finally since I wrote the code in a Jupiter notebook we'll install Jupiter lab in IPI widgets okay so we installed all the requirements and if you run into trouble in the installation steps so here are some important notes from mlx is documentation first and foremost you need the M series chip like that's the whole point of this video and this Library the second is that you are using a native python that's greater than or equal to 3.8 so I think here I'm using 3.12 if I just do this yeah so I'm using 3.2.2 then you have to have at least Mac OS 13.5 but they recommend that you have Mac OS 14 which is what I'm running on my machine right now so now that we've cloned the repo and we set up our environment let's run through the example code so to do that we'll do Jupiter lab okay so here we've got the example code video link and blog link coming soon cuz I'm making it right now and then we're going to zoom in because it's probably tiny on your screen all right so mlx fine tuning we're going to do some imports so we're going to import subprocess because all the example code that we're hacking runs from the command line and so subprocess allows us to run terminal commands through Python and then here I'm importing the mlx LM library to run inference on a model a little later so I defined some helper functions here they're not super important right now so I'll just come back to them as we encounter them in the code and then this is an optional step that example code from mlx comes with this convert. py script which is capable of taking any model from the hugging face Hub and converting it into the proper mlx format and additionally quantizing it so I actually did this for mistol 7B instruct version 0.2 and then there there's this argument that you can pass that will push the model to the mlx community so I guess that's worth calling out right now mlx has this page on hugging face here are several mlx compatible models many of which are quantized so there's Google Gemma mistol quen code Gemma llama 3 53 whisper llama 3.1 even so this page seems pretty active and basically any model that you would want to find tune is probably already available here and if it's not you can simply just go find the model that you want to use in mlx let's go to mistal AI I'll go to Mistral 7B instruct so this is the one I used given this hugging face model path we can convert it to the mlx format and quantize it using this convert. py script so there was this typo so I had to add this quantize flag so what this will do is grab the hugging face model convert to the mlx format and this quantize flag converts into 4bit quanti ization so I actually didn't run the command yet so you could run this command using the subprocess module but I found it's better to just print the command like this and copy paste it into the terminal because when you run these shell commands in a jupyter notebook you don't see the same like progress metrics that you would normally and this will actually save a mlx version of that model on your local machine that you can run for inference and fine-tuning since here we're going to use a model that's already available on the hugging face hub here's the model card for it we can just skip this quantized model step and so here what's happening is I'm going to build the prompt that is defined here so again here we're creating a YouTube comment responder what this model will do is you'll pass in a comment and it'll respond to that comment hopefully in my likeness so for this example I'm not just going to pass the raw comment into the model itself for inference I have actually constructed this prompt which is the same prompt that we saw in the previous Cur example to help the model generate better responses I created a Lambda function for this to find this instruction string and then did some string manipulation to incorporate the comment into it dynamically the result of that is this prompt Builder Lambda function which takes in the comment and so here we're just doing a very boring comment great content thank you and then we'll Define the max number of tokens and then here we're going to use the mlx LM library to do inference so the syntax looks very similar to hugging face which is pretty convenient if you're comfortable with hugging face already so here what we're going to do is we're going to load in the model so this is that same quantize model and then we're going to have the model generate a response so we pass in the model the tokenizer which was loaded automatically The Prompt that we defined using our prompt Builder the max number of tokens which I set to 140 and then I put verbos equal to True okay so we can take a look at the prompt here which is all this stuff I'm not going to read all this you can read it if you like but this is just to help nudge the model in the right direction then we have this please respond to the following comment and this is the comment that we spliced in using that prompt Builder Lambda function and then down here is the model's response so this is the raw quantized model no fine-tuning whatsoever and this is the response well first and foremost it put sha GPT in the wrong place this is supposed to be at the end of the response not at the beginning but it says thank you for your kind words I'm glad you found the content helpful and enjoyable if you any specific questions or topics you'd like me to cover in more detail please feel free to ask I would never respond to a comment like this this is something we've seen in the previous fine-tuning videos where the unfin tuned responses tend to be pretty verbose and when I respond to comments or really any kind of communication I try to keep it as concise as possible so this is very different than something I'd actually write in response to a comment so let's see how we can fine-tune this model to generate responses that sound a lot more like me here again we're going to use one of the scripts from that mlx examples repo and I'm going to construct the command in the same way so I put everything in a list and the reason I have it in a list is because that's how you can run these terminal commands in Python you'll pass it in as a list to the subprocess module but I realized that even with this fancy helper function that chat GPT wrote me to continuously print outputs from my terminal command it still wasn't printing the loss during training so I actually found an alternative strategy to be more helpful which is to just take this command variable here and to translate it into a string that I can copy and paste into my terminal and so this construct shell command is just a pretty simple helper function up here that's just doing some basic string manipulation to convert this list into a string what I'll do is I'll copy this string and then we can go over to terminal and then I'll paste the command here here okay so walking through this a little bit I'll try to zoom in even more hopefully that's legible we're running a python script so that's what this is the python script we're running is called lowra and it's in this scripts subdirectory and then we're just going to define the parameters for training so here we're specifying the model which is that 4-bit version of mistal 7B instruct we have this train flag because we're going to run training we're going to set the number of iterations so we're going to do 100 iterations that means it's going to run through 100 batches and the default batch is four next is steps per eval so this is the number of batches that the training will run through before Computing the validation loss so I set it to 10 which is the same as the number of steps per training loss evaluation the Val batches is the number of examples to include in the validation loss calculation setting it to ne1 goes through everything in the validation data set and here that's only 10 examples next we set the learning rate at 1 to the minus 5 which is actually the same as the default but I have it explicitly written here cuz I was playing around with this I probably ran this a dozen different times trying to find the best hyper parameters and then low R layers 16 which is also the default parameter but I went through a lot of iterations to just end up coming back to the default and then finally we use this test flag which computes the test loss at the end of training I guess before we run this it's worth talking about the data that we're using here we have three data sets so we have have this train. Json L test. Json L and valid Json L the way I make these data sets are here in this jupyter notebook also available on the GitHub what I'm doing here is I'm taking a CSV file of YouTube comments and responses which looks like this so I've got 70 comments these are real comments and 70 real responses from me and so this is the way I'm going to train the model to respond to comments in my likeness and way I do the train test validation split is I have 50 examples for training 10 examples for testing and 10 examples for validation I won't walk through this code cuz I did it in the Cur video and I did a similar thing for the open AI fine-tuning API video but if you're curious about how I'm doing the data preparation feel free to check this out we're using the same prompt in the training data as I used at inference in the example code we just saw very similar strategy the Json l format if you're familiar with python is essentially a list of dictionaries so a dictionary is a set of key value Pairs and a list is just a sequence a collection of elements Json L is just going to be a collection of these key value pairs where each key value pair each dictionary consists of one key and one value so super simple the key is text and the value is the example that you want to use to train or evaluate the language model notice that this contains the instructions essentially of the model this bit is the comment the real comment from the user and then it ends with this instruction token and then here's the real response from me so all of these are packed together to form the example and we have 70 of these and 50 are going toward training 10 are going for testing and 10 are going for validation so here I randomly select examples for testing and validation and then I just write everything to these Json L files so hopefully this example code is easy for you to follow and hack and adapt for your own use case and if you get stuck you know feel free to drop a comment or reach out I'm happy to help try to get you unstuck okay so that brief aside was the data preparation now let's go into training so I actually haven't done this while running OBS so my computer might blow up if I do this so I've got activity monitor and it's not looking great because OBS is already using a gig of memory and when I was running this the fine-tuning script was taking like 14 G gabt of memory and so I'm going to execute this script but if my computer blows up and I'm not able to post this video I'm so sorry all right here goes nothing nothing has blown up yet we'll just keep the activity monitor here so we can monitor the memory pressure so we see that the fine-tuning script is taking about 10 gbes of memory there's this other python script which I guess is the Jupiter notebook taking up 4 GB of memory M and then we got OBS here taking up 1 gab of memory so when I was doing this for real I was basically doing nothing else on my machine I was just allowing as much memory as possible to be dedicated to the fine-tuning script but here it seems like the mlx library is handling it pretty well you know it seems to just dynamically adapt to however much memory is available okay so it computed the V loss I don't think anything's going to blow up so that's great and kudos to Apple and their ml research team for writing a good software library but but I do think since less memory is being allocated to this compared to just not running anything else on my machine this is going to take a lot longer to run before when I was just allowing the training to run all by itself I didn't have the jupyter notebook running either this was hitting like 14 GB of memory 13 to 14 or something got kind of close to 13 there and it took about like 15 or 20 minutes to fine-tune the model with the hyperparameters shown here batch size of four and has 50 training examples so I'm not going to sit here for 20 minutes to wait for this to run cuz I've already done this and I can show you the finished product once again like those cooking shows where they show you how to prepare the food and put in the oven and then magically they had the lasagna that they made last night and they're going to eat it in front of us okay so here's me eating the lasagna we're going to quit out of this process killed that and then what we can do is to run inference with the fine tuned model so this doesn't have the adapter file so what I'll do is open up a new one of these we'll do this in private once training is done so we'll say 20 minutes goes by you go get a sandwich or something while this is running this adapters. npz file will appear in the repository these are the low weights learned during training once that is here we can continue with the Jupiter notebook and we'll use these adapters to run inference again so this will be our fine-tuned model so we're going to run it in a similar way but instead of printing a command and copy pasting it into the terminal here we can just run it in the notebook because there's really not much to see you can see that it failed cuz the adapters file wasn't there but now that it's here we can run this again so now it loaded the pre-trained model it passed in the prompt and the prompt is including the comment which is just that simple great content thank you and then we have the response from Sha GPT glad you enjoyed it smiley face sha GPT so this is much more aligned with something I would actually say to a short comment like this going back this long and poorly formatted comment is not what we want but just after 50 training examples we see that the model is noticeably responding in a different way and then we can even run this a few times to see what else it comes up with glad you liked it so I guess it'll probably keep generating responses that are similar to that yeah glad you're here happy to help okay yeah these are great these are things that I have said in comments so it's doing a good job but that's a really easy comment to respond to let's try a different comment so here's a harder one that at I think this is more recent I don't know this is not in the training data or anything so let's see how sha GPT mlx handles this one so the comment is I discovered your channel yesterday and I'm hooked great job it would be nice to see a video of fine-tuning shot GPT using hugging face I saw a video you running it on Google collap 7B any chance of doing a video on your laptop Mac or using hugging face spaces so that's exactly what this video is let's see what shot GPT thinks hi thanks for your kind words sure thing I'll do a video about fine-tuning HF version on Sha GPT on my Mac then has this YouTube link hope it helps sha GPT okay so let's see what this takes us to ah the video doesn't exist probably because I'm making the video it'd be really crazy if when I post this video this becomes the URL you know it's responding appropriately you know it's like thanking them it's pretty short it puts sha GPT at the end this sentence here doesn't make a whole lot of sense so I'll do a video about fine-tuning HF versions of Shaw GPD so hugging face versions of sha GPD on my Mac see what else it says so I guess this is a kind of hard question to respond to how would it know what I want to do glad to hear it glad you found the channel useful okay well this is a nice response but it doesn't answer the person's question so let's try another one hey glad you're enjoying the channel okay refuses to respond I guess another thing is we can check out the memory spikes during inference so let's do this one more time take a look at the memory pressure so we see that it kind of goes up a bit I guess it's opening up another python instance to run these subprocesses and it takes about 4 GB of memory yeah GL here so it's refusing to respond to this guy's comment I think I got lucky when I uploaded this because it had a really good response let's see glad you enjoyed it I'm looking forward to doing a fine-tuning video on my laptop I've got a Mac M1 mini that runs the latest version of the HF API so the great thing about this one is that it's got right that I have a M1 Mac Mini does run the latest version of the hugging face API but we didn't use the hugging face API but yeah I guess we did so we imported Transformers so Transformers is working under the hood so that was the example super simple but I will say there was one thing I forgot to mention which is that in this L.P file I went through like a dozen different sets of hyperparameters to try to get this thing working which is just the reality of machine learning machine learning is much more art than science or at least for now but I did want to point out one thing that I had to do so I had to go in here and kind of hack one set of hyper parameters okay here so adjusting the rank of these Lowa adapters is not something that this L.P file exposes as a command line argument so you can't just say oh I want to try rank four or rank eight or rank 16 or whatever from the command line I had to go in to the file and just manually change it I think it was eight originally and I changed it to four and this improved the training performance before I did this it was just kept overfitting and I tried a lot of different sets of hyper parameters but reducing the rank worked a lot better and this aligns with results from the low R paper if you've taken a look at and if you haven't check it out it's a really good read rank four rank eight seem to be that sweet spot for the results I guess I can pull it up real quick so in table six of the lowette paper you can see they were comparing what weights they were applying the adapters to and the different ranks of the adapters in the qar example on Google collab I just applied fine tuning to the query layer using rank eight but in this example I applied it to both the query and the value layers and I used rank four and you can see that at least in these examples the rank four rank eight is kind of like this inflection point in the performance like it kind of flattens out and actually starts to get a little worse as the rank gets too big okay so that brings us to the end of this walkthr the example code is again freely available on the GitHub repository shown here and I'll link it in the description below if you enjoyed this video or you have suggest sus for future content please let me know in the comment section below and as always thank you so much for your time and thanks for watching
y7xbHtFostg,2024-07-22T13:46:22.000000,Data Science Explained in 60 Seconds,data science explained in 60 seconds by a data scientist 12 years ago data science was the sexiest job of the century today it's still an in demand role offering pretty cushy salaries but what do they actually do the way I see it the fundamental role of a data scientist is to answer questions using data in my experience this consists of four Key activities first is getting a question to answer by working with clients and business stakeholders two is acquiring the relevant data this is the boring yet most important part third is analyzing the data once data are in a usable form data scientists use tools like Python and R to compute statistics and generate data visualizations and fourth is to build models this is probably what gave data science the sexy label data scientists can use data to construct predictive models to solve problems and support decision making
f4Gsbq10j1s,2024-07-10T13:00:45.000000,"How Much YouTube Paid Me for 1,000,000+ Views (no fluff)",n/a
TwVl4aA5qtk,2024-07-05T18:06:10.000000,I Was Wrong About YouTube (what I learned),n/a
suxHNXk5jp0,2024-07-01T19:36:39.000000,Why Businesses Should NOT use #AI,n/a
baxaZI_j71I,2024-06-19T21:57:56.000000,3 Reasons Businesses Should NOT Use AI,there are a lot of good reasons for businesses to use AI it's a powerful technology their customers are asking for it and they don't want to fall behind their competition however these points tend to overshadow the other side of the coin why one should not use AI over the past year I've consulted 65 smbs on using AI in their businesses through these conversations I've seen a few common problems that limit businesses in their ability to effectively use AI to create value to hopefully help you avoid these problems here I'm going to discuss three reasons why smbs should not use Ai and if you're new here welcome I'm Shaw I'm a data scientist turned entrepreneur and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make the first Common problem for businesses is lack of AI infrastructure what I mean by this is the combination of two main things one one data infrastructure and two technical Talent data infrastructure makes it easy to access and use data relevant to your business this is important because it significantly reduces the barrier to entry for AI projects such as Rag and fine-tuning which are ways to tailor an AI system to a particular business context or use case another reason why existing data infrastructure is beneficial is because it provides additional opportunities for Value creation more more specifically AI can be used to enhance existing data workflows by introducing new ways to capture and structure data for example capturing information from customer reviews company documents and public web pages to augment current data sets and decision-making the second side of AI infrastructure is technical Talent although using AI is easier now than ever before building custom projects is still technically demanding this requires having the right people to support its development in your business generally speaking there are two options for this option one is to have an internal AI team the upside of this is it keeps costs relatively low compared to hiring outside contractors and the team can better integrate with other parts of the business but of course this comes with the downside that technical teams take time to develop thus are a long-term investment option two is to hire an external AI team in other words hiring a vendor in contrast to option one these teams can get started relatively quickly and with low initial investment however this comes with a larger cost in the long run and runs the risk of developing a dependence on a third-party vendor the second thing to consider is that AI often introduces unnecessary costs and complexity in business value is generated through solving problems the trouble with using AI to solve a particular problem is that it often comes with significant overhead namely you'll need to curate the data and the technical t talent to implement the solution however even if you have the data infrastructure and teams in place some AI Solutions might be overkill for instance one of the most common use cases clients approach me with is a chatbot that can interact with users and retrieve information from a knowledge base while this is a new and Powerful way to solve the search problem it is significantly more complex to engineer than the standard search interface and the marginal value generated may not justify this additional complexity the third and final reason to not use AI is AI risk this is something that can easily get eclipsed by all the excitement we're seeing these days surrounding AI risk is often a statement of ignorance and there's a lot we don't understand about how AI systems like chat gbt work under the hood while this creates many potential risks the two I want to focus on here are lack of explainability and hallucinations more powerful AI systems often come at the cost of less explainability this means we don't know exactly why a system generated a particular output which can pose significant risks if this output is being used to make business decisions or influence how customers are treated the second risk are hallucinations these are when generative AI systems generate false outputs that can potentially be harmful a recent example of this was an Air Canada chatbot provided false information to a customer about their bement policy leading the customer to sue the company and win about $800 while this single case is a negligible cost for the company it's not clear what the impact of future hallucinations might be of course there are many other AI risks such as prompt injection leaking proprietary data and unknown model biases however I'll save a deeper discussion of AI risks for a future video I don't want to give the impression that small to medium-sized businesses should give up on AI because of all these potential challenges in fact I believe the opportunity for smbs is even greater than for larger companies because they're still flexible enough to explore new ways of doing business which can help them replace the incumbents AI is reshuffling the deck which means the quote unquote little guys have the most to gain from it toward that end I want to share three recommendations for how smbs can approach using AI the first is to start with the problem not the technology there's a quirk of human psychology called The Hammer problem which says when you have a really nice Hammer like AI everything starts to look like a nail this leads us to go around and whack every problem with it while you might hit a few Nails in this process you might break a few things too that's why it's important to think critically about the problems you apply AI to ask yourself why do I want to use AI to solve this problem is there a simpler solution the second recommendation is to start with chat GPT often much of the value of an AI use case can be realized completely for free using chat GPT and other similar tools this is a great low stakes way to experiment with and validate use case ideas put another way if you can't realize the value using chat GPT then this might be a red flag for the feasibility of a more sophisticated solution however if chat GPT does provide a good Solution that's a good indicator that a more sophisticated solution might be worth pursuing the third and final recommendation is to keep it low stakes Beyond experimenting with ideas for free using Chad GP it's important to limit the costs and the risks of new AI applications this can be achieved in many ways such as validating new ideas with quick and lowcost poc's keeping a human in the loop of the AI system and applying it to areas where the cost of being wrong isn't significantly higher than the benefit of being right that brings us to the end this video is part of a larger series on AI for business so if there are any topics you'd like me to cover in future videos of this series please let me know in the comments and as always thank you so much for your time and thanks for watching
X8ZR6yFdg1Q,2024-06-17T14:12:45.000000,AI Explained in 60 Seconds #ai,n/a
sxvyBxLVvKs,2024-06-13T23:44:59.000000,How to Communicate Effectively (as a Data Scientist),"let's talk about technical communication this is universally one of the biggest challenges that data scientists face however it also presents one of the greatest opportunities here I'm going to discuss the seven tips that have been most helpful to me in becoming a more effective Communicator you might be thinking Shaw I'm a data scientist my job is to code and build AIS and understand statistics why do I need to Be an Effective Communicator can I just leave that to the business folks of course this is a narrow perspective because in most business contexts the way data scientists create value is not through writing code or building AIS but rather through solving problems and the problems that they solve aren't their own problems but they are the problems of stakeholders or clients in order to effectively solve these problems the data scientist must be able to effectively communicate with the stakeholders this highlights the point that it doesn't matter how powerful your AI is or how impressive your analysis is if you can't effectively convey these things to these stakeholders they're never actually going to be used to solve the problem this presents a key bottleneck most of the time data scientists aren't limited by their technical ability but rather their ability to effectively communicate with stakeholders and clients and when I was working at a big Enterprise this was one of the biggest factors that would hold back data science scientists from advancing so if you want to make greater impact you want to drive more value and you want to get those promotions improving your communication is one of the best ways to do that as a data scientist some might think that communication is an innate skill meaning that it's either something you're good at or you're not good at and this of course is false communication like any other skill is something that needs to be developed through practice and I am living proof of that where five years ago I was an overly technical physics grad student who probably spent too much time in the lab but after 5 years of dedicated effort I now get invited to do public speaking events my YouTube videos have been viewed over 1 million times which is just a mindboggling number to me and my medium articles have been read over 300,000 times all that to say if this guy can do it you can too and so here I'm going to share the top seven communication tips that have helped me over these past 5 years the first is to use Stories the second is to use examples third use analogies the fourth is structuring information as numbered lists fifth is always following the principle of less is more six is to show rather than tell and the seventh and final one is to slow down so let's talk about these one by one the first tip is to use stories this this is something I picked up from a book called The storytellers secret there the author describes how our brains are wired for stories stories just make sense to us and they are one of the most powerful tools we can use to communicate information effectively and when I say story here I don't mean something like a news article or a novel but rather any three-part narrative some examples of that include status quo problem solution this is my go-to storytelling format and you'll see this throughout my medium articles my YouTube videos and Linkedin posts let's see a concrete example of this AI has taken the business World by storm while its potential is clear translating it into specific value generating business applications remains a challenge here I discuss five AI success stories to help guide its development in your business another structure I like is the what why and how and I actually used this to structure this talk I started with the high level technical communication I talked about why it mattered then I dove into the how which are these seven tips another structure I like is the what so what and what now so in a business context what this might look like is website traffic is down 25% this has led to a 150,000 Revenue drop the analytics team is investigating the root cause this is a natural way to structure information for instance imagine if we didn't use a story here and we said something like the analytics team is in investigating the root cause of website traffic being down 25% and revenue dropping $150,000 it doesn't really have the same flow and ring to it it kind of feels like a barrage of information however structuring it into this three-part narrative makes it a bit more digestible and ends the communication on a positive note the next tip is to use examples examples are powerful because they ground abstract ideas with concrete examples what this might look like in practice is you might get a ping from a stakeholder asking you what's a feature because maybe you mentioned it three times in an email to them to which you could say features are things we use to make predictions while this answers the question it's still a pretty abstract statement and so an easy way to make this more clear is to add something like for example the features of our customer churn model include age of account and number of logins in the past 90 days this allows the other side to connect this abstract idea to a specific example to which they might respond with a heart emoji a related idea to using examples is to use analogies analogies are powerful because they map the unfamiliar to The Familiar for example the word algorithm is unfamiliar to many people however the word recipe is very familiar almost everyone has followed a recipe before another example is a feature to which you could make the analogy to an ingredient in a recipe an analogy I used in my fine-tuning video video was comparing fine-tuning to turning a raw diamond into a diamond that you'd actually put on a diamond ring the next tip is to use numbered lists which is what I'm doing in listing out these seven communication tips and the power of numbered lists is that numbers tend to stick out to us especially when trying to navigate an ocean of words and language this is something that really hit me when I read the great book by the late Daniel Conan Thinking Fast and Slow where he he framed these two systems of the mind so what people might call the conscious or the subconscious or the automatic thinking system and the deliberate thinking system he simply called them system one and system 2 these labels really stuck with me and consequently I started using this strategy throughout my content creation some examples are in my fine-tuning content I talk about three ways to fine-tune in my llm intro I talked about three levels of using llms my causal Discovery video I talked about three tricks for causal Discovery when listing takeaways I'll often add a number to them like my four key takeaways or my three takeaways and in my causal inference blog I talked about the three gifts of causal inference and then these will be followed up by listings like we're seeing in this article where it's like tip one tip two tip three this way of structuring information tends to make it a bit more digestible for the audience the fifth tip is less is more which is the fundamental principle of all types of communication what really convinced me of the power of less is more is work again from Daniel Conan in his capacity Theory whose basic premise is our attention and ability to exert mental effort is limited applying that to a communication context your audience's attention is finite so it is important that you be very economical in how you spend your audience's attention do you want to spend it on small talk and fluff or do you want to spend it on key information and while you might think having less things on your slides or using less words in an email should take less time the exact opposite is usually the case and this is captured well by the famous quote from Mark Twain which goes I didn't have time to write a short letter so I wrote a long one instead a related idea to less is more is to show don't tell and the basic idea here is to use pictures over words at every opportunity to demonstrate this let's look at the fine-tuning analogy I made on a previous slide fine tuning is when we take an existing model and refine it for a particular use case this is like taking a raw diamond base model and transforming it into a diamond we can put on a diamond ring fine-tuned model while it's good we used an analogy this is a lot of work and mental effort our audience needs to expand to get this information so let's see what this could look like through images this is a slide from my fine-tuning video and it conveys the point much more concisely we have an ugly RW Diamond here and it becomes a very beautiful Diamond here and these things are labeled by base model and fine tune model the seventh and final tip is to slow down this was another tip that had a significant impact on my communication skills before I had a tendency to rush through talks typically because I was nervous and I didn't want to waste the audience's time but the thing about a rush talk is that from the audience's perspective it feels like getting blasted in the face with a fire hose and it doesn't leave anyone very happy it's painful to listen to and it's hard to digest the information on the other hand a well-paced talk is like a soothing stream that is easy to digest and leaves the audience like this the irony of it all is that a rush talk even if it takes less time is more draining than a well-paced talk that might be an extra 3 to 5 minutes which might leave the audience energized and excited to ask questions I'll also throw in a bonus tip which is to Know Thy audience knowing your audience allows you to effectively frame all aspects of your communication so what this might look like in practice is if I'm explaining a new AI project to someone in the sea Suite I'll use terms like AI however if I'm talking to a bi analyst who has a bit more hands-on experience with generative AI I might use the term llm while if I'm talking to a fellow data scientist I might be more specific and say the specific model that is being used in the project like llama 38b however this isn't just about using the right terminology when talking to your audience different audiences care about different things so if you're talking to someone in the sea Suite you often want to focus on the what and the why leave the conversation at a relatively high level focusing on the business impact if talking to a bi analyst you still want to discuss the what and the why however you might give more technical details and in addition to the business impact you might talk about the relevance of the project to their specific workflow finally if talking to a fellow data scientist you would talk about the what why but also the how so this might include all the nitty-gritty technical details you still want to talk about business impact because everyone wants to know the why they want to know how their effort fits into the bigger picture the key difference however is that you might be talking about the how the specific implementation steps for the project so that brings us to the end if you enjoyed this video and you want to learn more check out the blog posted on medium and as always thank you so much for your time and thanks for watching"
XQWhJsXu0sY,2024-06-07T18:32:43.000000,What Nature Can Teach Us About Business...,n/a
wJ794jLP2Tw,2024-05-30T15:41:30.000000,Automating Data Pipelines with Python & GitHub Actions [Code Walkthrough],this is the sixth video in the larger series on full stack data science in this video I'm going to be talking about automating data pipelines I'll start with a highle overview of how we can do that and then dive into a concrete example using GitHub actions which gives us a free way to automate workflows in our GitHub repos and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's great no cost way you can support me in all the videos that I make so I'll start with a story about a friend of mine back in grad school so it seemed about every Friday to cope with our existence of being physics grad students me and many other grad students would find themselves at a bar close to campus and there was one grad student in particular who would show up after doing research for several hours and he would be drinking a beer and he'd say something like technically I'm working right now because my code is running and of course we would always laugh about it but this is a sentiment that I find a lot of data scientists and other developers share which is this sense of satisfaction when some piece of software that they wrote is off and running all on its own while they are off doing something else that they enjoy doing like having a beer with fellow grad students so this is at least one motivation toward automating data pipelines and other aspects of the machine learning pipeline here I'm going to talk at a high level about two ways we can automate data pipelines the first way is via an orchestration tool which is something I mentioned in the previous video on data Engineering in this series this includes things like airflow dagster Mage and many more the second way is what I call python plus triggers this is Python scripts that run given particular Criterion such as it's a particular time of the day or a file appears in a directory let's dive into each of these approaches the first way an orchestration tool so I'll start with airflow because from the dozens of interviews I've had with data engineers and ml Engineers it seems that this has emerged as a standard tool for many building data and machine learning pipelines one of the biggest benefits of airfow is that it can handle very complicated workflows that include hundreds and even thousands of tasks this is a major reason why this has become an industry standard among data Engineers managing and Enterprise scale data pipelines one downside of airflow as someone who was trying to learn it specifically for this series and this project that I've been working on is that it can be pretty complicated to set up and maintain these workflows and of course that comes with a steep learning curve these challenges of getting set up with airflow have created a market for airflow rappers such as prefect dagster Mage and Astro the upside of these wrappers is that you can tap into the power of airflow with potentially simpler setup in maintenance however one downside of these airflow wrappers is that this ease of use is often coupled with managed or paid services for these rappers however to my knowledge most of these have open-source versions if you want to go the self-managed route regardless of what orchestration tool you want to use for many machine learning applications where the there's a relatively simple data pipeline these tools may be Overkill and may over complicate your machine learning system which motivates the second approach to automating data pipelines using python plus triggers you can say that this is the old fashion way because before we had these orchestration tools if you wanted to build these data and machine learning workflows you'd have to build them from scratch so to make this a bit more concrete let's look at a specific example say we wanted to implement a very simple ETL pipeline consisting of one data source and one target database what this might look like is we pull data from a web Source we'll run an extraction process via a python file call it extract. py we'll transform the data in some way in a python script called transform. piy then we'll load it into our database using another script called load. piy so this is a simple ETL pipeline which is something I discussed in the previous video video in this series on data engineering given these three Python scripts there's nothing stopping us from creating yet another script that consolidates all these steps we can call that script ET L.P so now instead of running three scripts consecutively we can just run one script although this has streamlined the process this is still not automated because we still have to manually go to the command line and type python space L.P this is where the idea of a trigger comes in so let's say we wanted to run this ETL pipeline every single day one thing we could do is run a KRON job KRON is a command line tool which allows you to schedule the execution of some process so let's say we wanted to run our ET L.P file everyday at midnight we would type something like this into the command line and then that would get saved in our system as a Cron job and every day at midnight This would run but of course if we do this this would be running the Cron job on our local machine which may not be something that we want to do some Alternatives could be doing this process on a server that we manage or spinning up a cloud resource that runs this process but both of these options come with a bit of overhead in maintenance for setting up these compute resources so it would be great if we could set up this trigger to execute our ETL pipeline without the extra effort of setting up these compute resources this is where GitHub actions are super helpful gith GitHub actions is github's builtin cicd platform cicd stands for continuous integration and continuous delivery the typical schematic of cicd looks something like this where I suppose this is supposed to be some kind of infinite Loop of integrating new updates into your software system and delivering those updates in real time hearing this you might be thinking sha what does continually deploying code have to do with data pipelines well although data may not play a role in traditional software development when it comes to building machine Learning Systems data play a central role in the software development put another way when it comes to machine learning we use data to write programs to write algorithms which is manifested as a machine learning model so if we're talking about continuously integrating and delivering a machine learning application that will require us to continually integrate and up update the data that we feed into that system two key benefits of GitHub actions is that firstly the compute to run these workflows is provided for free for public repositories this is great for poor developers like me who just want to put out example code like for this video or for those who are building proof of Concepts or building projects for their portfolios and of course there are paid versions for Enterprises and businesses and the second thing is that we don't have to worry about setting up compute environments whether that's an on premise server or a cloud resource all the setup happens via writing a. yaml file with that highle overview let's walk through a concrete example of automating an ETL pipeline to turn YouTube video transcripts into text embeddings if you're unfamiliar with text embeddings I have a whole video where I talk about that but understanding what they are is not necessary for this example here the steps we're going to walk through are one we're going to create our ETL python script two we're going to create a new GitHub repo three we're going to write our workflow via the yaml file four we're going to add repo secrets to our GitHub repo this will be necessary to allow the GitHub action to automatically push code to the repo and also to make API calls to the YouTube API without exposing my secret API key and then finally we'll just push and commit all our code to the repo let's start with the first one creating our ETL python script and so here I have a fresh Visual Studio code I created a new folder and now I'm going to create a new file I'm going to call it data pipeline. pi and I've already pre-written the code here so I'll just paste it over and explain it one by one the first thing I'm going to do is some imports the first line here is we're going to import a set of functions from another python script called functions. piy which we'll write in a second and then we'll import the time module and the date time module the reason we want time and date time is that it'll allow us to print when this python script is run and how long each of the following steps took to run toward that end first I'm going to print the time that the pipeline is starting to run we can do that using the datetime module next I'll start importing the different steps of our data pipeline so the first step is the extraction the first thing it's going to do is extract the video IDs of every single video on my YouTube channel that whole process is baked into this get video IDs function which is going to be in this functions. piy script that will create in a second this is something I walked through in the previous video of the series on data engineering some other fanciness that's happening here is that I'm just capturing the time just before and just after this step is is run so I can print how long it took to run this function and this is helpful for debugging purposes and observability once we have the video IDs extracted we can use a python library to extract the transcript of each video given its video ID using similar process here where this whole get video transcripts process is abstracted as this function and we're capturing how long it took to run that step next I have this transform data function which is just doing some data cleaning so ensuring that the data have the proper types and handling any special character strings and then finally step four we're going to take the titles and the transcripts from all the YouTube videos and make the text embeddings with these files in place we'll go ahead and create another file and this will be the functions do Pi file and here we will put all the different steps that we just put into our data pipeline there's a lot here actually more functions than are used in the data pipelines script I'm not going to walk through this because I've talked about it in previous videos but of course this code is freely available on the GitHub so you can check that out if you're interested one thing I'll point out is that at each of these key steps in the data pipeline we're not outputting any data structures or data frames as intermediate steps the data are actually being saved to this data directory this is actually something we'll have to create so we'll create a folder called Data so that the files have somewhere to go one last thing you'll notice is that we're importing a ton of libraries here so I'll go ahead and create another file called requirements. text where we can put all the different requirements for this data pipe line polers is what I'm using to handle all the data frames YouTube transcript API is the python library that allows us to pull the YouTube transcripts given a video's video ID I'm using the sentence Transformers library to create the embeddings and then I'm using the requests library to make API calls to the YouTube API all right now that we've written all the code for our ETL pipeline let's create a fresh GitHub repo we can push the code to we can create a GitHub repo from the command line or from the web interface I'll do it from the web to do that you just go to your repositories Tab and you click new it will give it a name I'll call it data pipeline demo and I'll just say demo data pipeline via GitHub actions we'll keep it as a public repo one benefit of doing it as a public repo is that GitHub won't charge you for the compute costs of the GitHub actions I'll add a readme file I'll also add a giit ignore using the python template and then I'll choose a license I'll just do Apache 2.0 all right so we now have our repo and then what we'll want to do is we'll want to clone our repo open up our terminal to whatever folder we want to store this repo in and then we'll do get clone and then we'll download this newly created repo we can see we have the license and the read me now that we have our repo we can go ahead and write our yaml file which will Define the work flow that will automate the execution of our data pipeline okay so the next thing we want to do is add all the code we just wrote to this new repo we created so the code we wrote earlier I stored it in this data pipeline demo temp folder so what I'm going to do is just copy and paste that over we go back here hit LS we see that all those files are here and then what we want to do is create a new folder we'll call it GitHub and then in that GitHub folders we'll create a new folder called workflows the reason we do this is that GitHub will look in this GitHub workflows subdirectory for all the workflows that we want to run as GitHub actions with that directory created we can open up our GitHub repo locally so it's called Data pipeline demo we'll open that up and we can see all the code is here and our GitHub workflows folders here so what we wanted do is create a new file in this workflows folder we'll call it data pipeline. yml and here is where we'll Define our GitHub action there are a few key elements of a GitHub action the first is its name which we Define like this and if you're not familiar with yaml files they're essentially python dictionaries so they consist of these key value pairs that can additionally be nested so the simplest of these when it comes to GitHub actions is this one we wrote here which is the name of the workflow which I'm just calling data pipeline workflow the next element of the workflow is the trigger so this will Define when this workflow is run that's specified by this on syntax and from here we have a few different options we can make it so that this workflow runs anytime new code is pushed to the repository we just do that by writing this another thing we can do is have the option to manually trigger this workflow using this option which is workflow dispatch so we'll see later that setting this option a button will appear under our GitHub actions tab which will allow us to manually run this workflow but the one that we probably care most about is this option which allows us to schedule our workflow as a Cron job which is what we saw in an earlier slide to make this Aon job we simply type cron here and then we specify the arguments for the Cron job what I want is for this workflow to run every night at 12:35 a.m. so the Syntax for that looks like this and if you're not familiar with the syntax of running a Cron job there's a great website called cron tab. Guru it has a guide for scheduling Cron job so this is what we wrote in our emo file it's saying at 0035 it's going to run and it actually tells you the next time that this job would run alternatively we could do all all asteris which means this is going to run every minute however when it comes to GitHub actions 5 minutes is the fastest that it can run so you would write every fifth minute like this but I tried this and even if you specify to run every 5 minutes it still won't run that fast it'll run closer to every 15 minutes so there are some limitations on how quickly you can run a workflow using GitHub actions but here we don't want to do anything crazy and just running it once a day is fine because I'm posting videos every week so running this workflow every day is more than sufficient now that we have the name of our workflow and we' specified when it's going to run the next thing we want to do is Define the workflow itself workflows consist of jobs which have names so the names of the jobs are specified like this and then what we can do is specify what system this specific job will run on so we'll say auntu latest so this will just run on the latest version of Ubuntu which is a Linux operating system and then jobs will consist of steps which we can specify like this here we're going to have a handful of steps to run this whole workflow the first step we'll call it checkout repo content and we can actually make use of pre-built GitHub actions provided by GitHub this one is called checkout version 4 what this step of the workflow is doing is pulling all the code from our GitHub repo and if you're curious about this specific action we can Google it and then there's a repo on GitHub that has a lot more information about this action you can check this out if you like it's at github.com actions checkout then I'll do one more thing here and I'll add this with thing and I'm going to specify a token what I'm doing here is giving this workflow access to one of the repository Secrets which will give it read and write access since it's a public repo it doesn't need any special permissions to pull the code onto this auntu instance at spun up but later in the workflow we're going to push code to this repo which will require special access and so that's the reason I'm adding this token to give the action the proper permissions and we'll create this token in a sec and I'll show you how to make it a repository Secret okay so that's step one of our job all we did was create a fresh auntu instance and then pull the code from our repo next thing we're going to do is set up python I'll call this step of the workflow set up python I'll use another another pre-built action called setup python version 5 and similarly if you're curious about this you can just Ty it into Google and then you can pull up the repository for setup Python and it'll show you the basic use usage and have more information on that but it does exactly what it sounds like it does then we can add this with thing and specify the python version here I'll use 3.9 I'll also add this option which will cach the libraries that we install with Pip so it doesn't have to install these libraries from scratch every single time it runs the workflow it can just install it once and then it can reuse those libraries from Cache that second step we set up python now we'll do another step and we'll install the dependencies here we're going to use a different command so we'll use this run thing this is essentially like running a command from the command line so we'll just do pip install requirements. text this is just as if we opened up our terminal and typed this in the command line and the reason we can do that is because we've just installed python on the machine so now we can run pip on it with the libraries and installed we can now run the data pipeline so I'll Define another step call it run data pipeline I'll need to do one thing here which is import a environment variable called YouTube API key and this will be another repository secret that will Define later and what this is is my YouTube API key which is needed to run this function here the get video IDs so once we've done that we can now run our data pipeline so we do that like this so we'll use the Run command again and we just type python data pipeline. piy just like we were typing this into the command line now we've run this python script on this machine that just got spun up and then what we can do next is see if any changes were made to the git repository after running the data pipeline give it an ID and then we'll run another command I'll use this vertical bar syntax which will allow me to import mult multiple lines of commands but we can go through this one by one What's Happening Here is that first we're configuring the GitHub account then we're adding all the local changes what this line is doing is that it's checking the difference between the staged changes and the last commit and this quiet option will set the exit status of the command to zero if there are no changes and we'll set it to one if there are changes so if the exit status is one we'll create a new variable called changes and that'll be equal to true and then we'll store this in the GitHub environment what that allows us to do is do one last step we'll call it commit and push if changes what we can do is basically have an if statement so if changes so this environment variable that we just created equals true we'll run this command so again we'll use this vertical line to do a multi-line command and we'll commit the staged changes and then we'll push it to the repository this is where we need the special permissions from the personal access token that we defined earlier and then one last thing we need to do is add a file to the data folder because since it's empty GitHub won't actually push any empty folders to the repo so we should just have a file in here and then we can just say something like data go here and with that we've created our workflow in this data pipeline. yaml file we have our data pipeline here we have all the functions that make it up here we have our requirements file with all the libraries that are needed to run our data Pipeline and we have a data folder in which we can store all the data produced from our data pipeline in this example we can get away with storing the data on GitHub because it's super small it's not going to be more than 100 megabytes or so but if you're working with data sets much bigger than that talking about gigabytes or even more in that case you probably want to store that in a remote data store and those are changes you would just make in your data pipeline itself for example instead of reading and writing to a local directory you would read and write to an S3 bucket or to Google Drive or to a database so that'll just depend on your specific use case before we can push all these local changes to our GitHub repository we need to create two secret variables that are available to to the GitHub actions in order for this pipeline to run successfully and so to create a repository secret we'll go over to settings we'll scroll down to secrets and variables and we'll click actions we'll see a screen like this and we'll see this repository Secrets section this will allow us to create repository secrets that will be accessible to our GitHub actions the first one we want to create was this personal access token but we need to actually create it personal access token for this and so in order to do that I'll click on my profile here scroll down to settings open that in new tab and then I'll scroll down to developer settings click on personal access tokens and then I'll click on tokens classic and you can see that I've already created some personal access tokens but what I'll do is create a new one it asks for a note so I'll call this data pipeline demo P expiration we'll just leave at 30 days and then we can select the scope so we only need this repo scope here and the reason is we just want our GitHub action to be able to push code to our public repo it has read and write access and then we can actually leave everything else unchecked to do that we'll just hit generate token and then this token will appear you shouldn't share this token with anyone I'm sharing it with you because I'm going to delete it right after this demo but we can copy that and then we'll come back over here and we'll paste that into our secret now this personal access ACC token will be accessible as an environment variable to our data workflow we just hit add secret and then I'll do a similar thing for my YouTube API key this I will not share but if you're importing your own YouTube API key or any kind of API key it's important to just paste the RW string and not put quotations around the API key I'll just add that and hit add secret now we have two repository Secrets here the personal access token and the YouTube API key so now with the secrets in place we can commit and push all our changes to the GitHub repo and watch the workflow run at our local changes we'll commit our local changes first push and then we can push all the local changes to our before we do that let's add one thing to our git ignore file if you're on Mac you'll notice that your GitHub repos will always add this DS store thing I don't want that so I'm going to include that in the git ignore file and we'll do get push now we pushed it to the repo and I guess I didn't properly remove this so I'll just go ahead and manually delete this so we see all our code has been pushed to the repo we can see that this little pending dot is appearing so if we click that we can see that our workflow is running and so to watch that we can click on the actions tab if we click on this and go to run data pipeline we can see that run data pipeline was the name of our job in the data pipeline. yo file and then these were all the steps that we defined so we'd setup job check out repo content set up python install dependencies and then we have run data pipeline check for changes commit and push if there are changes and then these steps are automatically generated from the pre-built actions of setup Python and the checkout action so we'll wait for these dependencies to install all right so the dependencies install took about 2 minutes now it's going to run the data pipeline so this will actually take longer than the dependenc because what's happening is we're making the API calls to the YouTube API to grab all the video IDs and there's another step of grabbing all the transcripts for each YouTube video and then finally we have to generate the text embeddings for those transcripts and the title of those YouTube videos so this might take a few minutes we can see that everything we printed in our data pipeline is showing up here and then we're checking for changes and they word changes so it's pushing the code but it failed and it's probably because I deleted that DS store after this workflow got kicked off this is a great opportunity to actually go to our workflow and use the manual trigger so this was that dispatch workflow option that we created so we can actually run this manually like this and so once we click that we can see that now it's running again now we go through that whole process all over again all right so this time we see the data pipeline ran successfully and it committed and pushed the changes so we can see that it added these three files and then it's going to going to just do some post setup of python and then post checkout of repo content while that's finishing up we go to our data folder and we can see that the data from our workflow is here now these files can be used in some Downstream task Okay so we've successfully automated the data pipeline the next natural step is to integrate this automated data pipeline into the final machine learning application I actually do that in this repo here which I'll also Link in the description below so we can just click that and we can see that we have our workflows folder here and we have our data pipeline. yamof file so this is the same thing that we just wrote and then we have our data pipeline here and so these were the files that we defined earlier and the rest of this is similar to what we saw in the previous video of this series where we created a search API endpoint using fast API and Docker however here instead of deploying this API endpoint to AWS I deployed it on Google Cloud platform specifically using the cloud run service and the reason I did that is because they have a free tier also they support continuous deployment so anytime A change is made to this GitHub repo it'll redeploy the API running on Google cloud and then the front end is publicly available and I'm hosting it on hugging face I'll also put this in the description below but this is now live so you can go and see the fruits of all the labor of this video series the first run is going to be slow because it has to wake up the container on Google Cloud but eventually this spins up search results so if I type in llms it'll bring up all the videos on llms I can do something else like data freelancing and then it shouldn't take as long for the second run cuz the container's already awake and we get the results for data freelancing and then we can of course do this series on full stack data science and see the search results came out much faster the container's now awake you can see all the different videos relevant to full stack data science well that brings us to the end of the series we've come a long way talking about the four hats of a full stack data scientist being that of a project manager a data engineer a data scientist and an ml engineer if you're curious to learn more about fullstack data science or the details behind this semantic search app check out the other videos in the series and as always thank you so much for your time and thanks for watching
pJ_nCklQ65w,2024-05-18T15:24:22.000000,"How to Deploy ML Solutions with FastAPI, Docker, & AWS",this is the fifth video in a larger series on full stack data science in the previous video I walked through the development of a modelbased Search tool for my YouTube videos here I'm going to discuss how we can take this tool and deploy it into a production environment I'll start with an overview of key Concepts and then dive into the example code and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this video please consider subscribing that's a great no cost way you can support me in all the videos that I make when we think of machine learning we probably think neural networks or other models that allow us to make predictions although these are a core part of the field a machine learning model on its own isn't something that provides a whole lot of value in virtually all situations in order for a machine learning model to provide value it needs to be deployed into the real world I Define this deployment process as taking a machine learning model and turning it into a machine learning solution we start by developing the model which consists of taking data passing it into a machine learning algorithm and obtaining a model from that training process deployment can look a lot of different ways it could simply be making predictions available to programmers and other developers it could be using the model to power a website or a mobile application and then finally it could be embedding the model into a larger business process or piece of software but the key Point here is that the model that comes out of the training algorithm and is sitting on your laptop doesn't provide a whole lot of value however the model when integrated into a website into a piece of software or made available to end users through an API is something that provides value natural question is how can we deploy these Solutions while there are countless ways to do this in this video I'm going to talk about a simple three-step strategy for deployment that is popular among data scientists and machine learning Engineers the first step is to create an API in other words we create an interface for programs to communicate and interact with our model what this looks like is we take our model and we wrap it in this API which is represented by this box here and then people can send requests to the API and receive responses from it and so in the case of a model the request will be the inputs of the model and the response will be the outputs two popular libraries for doing this in Python are flask and fast API the next step is to take the API and put it in a container here container is a technical word referring to a Docker container which is a lightweight wrapper around a piece of software that captures all its dependencies and makes it super portable so you can easily run that piece of software across multiple machines and then finally we deploy the solution and since we put everything into a container now it's super easy to run that container on someone else's computer some server you manage or most commonly into the cloud The Big Three Cloud providers of course are AWS Azure and gcp so with this high level overview let's see what this looks like in code here I'm going to walk through how we can use fast API Docker and AWS to deploy this semantic Search tool that I developed in the previous video this video is going to be pretty Hands-On so I'm not going to talk too much about fast API Docker or AWS from a conceptual point of view but if those are things you're interested in let me know in the comments and then I'll make some follow-up videos specifically about those tools okay so here we're going to create a search API with fast API then we're going to create a Docker image for that API then we'll push that image to the docker Hub and then finally we'll use that Docker image to deploy a container on aws's elastic container service so let's start with the first one creating the search API with fast API which is this python library that does all these great things apparently and using it to write this example here it was super easy for me to learn and they have this great tutorial for those just getting started I walked through this to make this example and it probably took me like an hour or something to do it so super easy to learn especially if you've been coding in Python for a while anyway coming back to the code first thing we want to do is we're going to make a file called main.py and we're going to import some libraries so we'll import fast API to create the API and then the rest of these libraries are so we can Implement our search function like we did in the previous video so we use polers to import data about all my YouTube videos we use the sentence Transformers library to compute text embeddings we use psyit learn to compute the distance between a users query in all the videos on my channel then I have this other file called functions. piy that has this return search results function which I Define here and I'm not going to go into the details CU it's not critical for the deployment process but essentially what it does is that it takes in a user's query and then it'll spit out the top search results for that query coming back to the main script the first thing we do is I'll Define the embedding model that we're going to use from the sentence Transformers Library so by default the library will download the model when it's Run for the first time but here to avoid that I just save the model locally so what that looks like is here we have the main python file that we were just looking at and then I have this folder called data and in it I have this folder which contains all the files for this embedding model and then we have this parquet file which has all the data about my YouTube videos we can then load the model from file like this we can load the video index using this line of code and this is the same way we did it in the previous video and then we can import the Manhattan distance from sklearn which I did like this and again since I talked about this at length in the previous video I'm not going to get into the details of how the Search tool works here but you can check out that video if you're interested so everything we just did had nothing to do with the API this was just the implementation of that search function to create the API we create this object called app and and it's this fast API object and then we can simply create these API operations here I'm strictly defining these get requests which allows users to send requests to the API and receive back responses the other most common one is a put request which is often used to send data to an API and load it in the back end for example if we wanted to update this parquet file in some way we could use a put request to do that anyway here I Define three operations and that's done using this syntax here where we have this decorator What's Happening Here is we're saying that we wanted to find this get request at this end point here for the API and it's going to operate based on this python function this is a common practice where you have the root end point be a health check so it doesn't take in any input parameters but anytime someone calls this m point they'll receive back this string response of health check okay and a similar thing here so I created another endpoint called info which just gives some information about the API so it doesn't take any inputs but it returns back the name which I called YT search and I have a description for it which is search API for shots Levy's YouTube videos this one's not completely necessary but you can imagine if you have multiple users using this API and maybe you have multiple apis having an info endpoint can be helpful but the one we care most about is this search endpoint What's Happening Here is we're defining this search function that takes in a query from the get request and then it'll pass it into this function return search result indexes defined in this functions. piy file it'll pass it in as well as the video index the embedding model and the distance metric and then we can use the output of this function to return the search results and so a lot of fanciness here maybe it's not super easy to read but what's happening is we use the select method to pick out the title and video Columns of the data frame the video index we use this collect method because we didn't actually load the data frame into memory because we used scan paret instead of read paret and then once we load this in we pick out the indexes from this search result and then finally we convert that data frame to a dictionary that dictionary will have two Fields one corresponding to the title and the other field corresponding to the video IDs and it'll have up to five search results for each field that's the code to make the API was super easy it's great for me as someone who's very comfortable with python and knows very little about a lot of other programming languages especially ones that have to do with web development but now we can run this API on my local machine and we can interact with it so the way that looks is make sure we're in the right direction we see we have this app folder and then we can go into app and we see that we have our main.py file to run that we can do fast API Dev main.py all right so now it's running running on this port 8000 had to make a couple of changes to the main file so I had to add this app in front of functions and then I had to remove app from these paths here because it was running from a different directory than a previous version of the code but that should be working now now we'll see that we have this little URL here which we can copy and then I have a notebook here that allows us to test the API the URL is already here and it's at Port 8000 by default and then we want to talk to the search endpoint of the API that we created so we can actually run this you have this query called text embeddings simply explain and then we can pass that into our API and then we can see we get a response so it took about 1 second so that's actually pretty long long but maybe if I run it again it'll be faster yeah so maybe that first one is just slow but run it a second time 76 milliseconds and then we can see the search results here just kind of taking a step back the response in its raw form looks like this so it's just a text in the Json format which is basically a dictionary and then we can use this Json library to convert that text into a proper python dictionary and then we can access the different fields of it like this so these are all the titles from the top five search results and then we can also look at the video IDs so that looks like that now we've confirmed the API is working locally so coming back to the slides the next thing we want to do is to create a Docker image for the API the steps to make a Docker image from a fast API API is available on their documentation and it's a few simple steps we'll create a app directory so a folder called app we'll create an empty init.py file and then we'll create our main.py file we've actually already done this if we go back we see that the app directory already exists and we have the main.py file and we already have the init.py file taking one step out of that directory we see that this app folder is in another folder with a few other files so we have this requirements. text file which is shown here this is just your typical requirements file that you might have for any kind of python code here you can see we have all the different libraries we used in the main.py file so we have fast API polers sentence Transformers psychic learn and numpy we also have this Docker file which is essentially the instructions for creating the docker image this consists of a few key steps we start by importing a base image there are hundreds of thousands of Docker images available on the docker Hub the one we're importing here is the official python image version 3.10 and so we can see that on the docker Hub here it's an official image so I guess it's by Docker it's called Python and then they're all these tags so these are all different versions of this image we did 3.10 which I guess is going to be this one the next thing we're going to do is change the working directory you know imagine you just installed Linux on a machine or something so the working directory is just going to start as the root and then we can change the working directory to this folder called code next we can copy in the requirements file into the docker image so we take the requirements file from our local directory here and put it onto the images directory this code directory here and then once we've moved the requirements file onto the image we'll install all the requirements we do that with this line of code here and then I have this line of code to add this code app to the python path this might not be necessary because we actually changed this main.py file so I'm going to actually try to comment this out and see if it still works next we're going to add this app directory to the image so we're going to move it from our local machine to this code subdirectory on the docker image finally we Define a command that will be run automatically whenever the container is spun up to build the docker image we run docker build we specify the tag so we'll give it a name I'll call it YT search image test and then oh I forgot this we got to specify where the docker file is so it's going to be in the current directory so we do that and now it's building the docker image okay so now the image is done building you can see it took about maybe a minute to run the run times are here the longest was installing all the python libraries and so now if we go over to the docker desktop app we see that this image is here under the images tab so I have a previous version of the image but the one we just created is called YT search image- test and then we can actually run this image the way to do that is we can go let me clear it out so we do Docker run and then we specify the container name I'll put YT search container test and then we specify the port we want it to run at do 8080 yep cuz that's what I put here in the docker file finally we'll specify the image here's the code Docker run- d-- name of the docker container and then we specify the port and then specify the image so we can run that that's not the right image name it is YT search image test now the container is running locally we can actually see that if we go to our image here and it says in use this is the container that is using that image alternatively we can just go to this containers Tab and we can see all the containers saved locally here we can see that the container stopped running and that means something went wrong and we can see that the folder with the model in it is not a local folder so it didn't run because the model folder wasn't on the python path we could add the data subdirectory to the python path but alternatively we can just go back to the main.py file and add app to these directory names here and the reason we need this is that we Define the working directory as code all the code is going to run relative to this directory here that means if the python script is looking for this model path you have to put app here because it's running from code alternatively you could add this python path thing here but I don't want to do that to make this Docker file more simple now let's try to run it again so we'll build the image so that was nice and quick and then we'll run the container so now the container is running click this and indeed it is running successfully so we can click on it and we can see that it's running at this URL here we can test this if we go over to the super notebook we test the API locally so now let's test the docker container running locally so this should be the correct path so yep and so it's the same thing we have the URL and then we have the endpoint name we're going to use the search operation we'll Define a query and then we'll make a API call it ran slower because it's essentially talking to a different machine but we can see that the API response is the same we can similarly call the info endpoint or we can call the base endpoint as well we can see those generate different responses now that we've created the docker image next we're going to push the image to the docker Hub the reason we want to do this is that once the image is on the docker Hub it makes it easy to deploy to any cloud service that you like so here specifically we'll be using aws's elastic container service but dockerhub integrates with other Cloud providers so not just AWS but also gcp I'm sure it also connects with Azure even though that's not something I've checked to push the image to the docker Hub the first thing we want to do is create a new repository so I already have one called YT search but let's go ahead and create a new one from scratch so we'll call this one YT search demo and then we'll say demo of deploy deploying semantic search for YouTube videos and then we'll leave it as public and we'll hit create reposit doesn't have a category so let's just do that I'll call it machine learning AI so that's it the repository is made now what we can do is we can go back to our terminal we can actually list out the docker images like this we can see that these are all the images that I have saved locally what we want to do is push this one to the Docker hub first thing we need to do is Tag the image so it matches the repository name on the docker Hub essentially we're going to create a new image and this is going to have the same name as that repository we just created if we go back here we see that repo is called shahin which is my dockerhub username and then the name of the repo we just created YT search- demo and then the next thing we need to put is the name of the local image so here we had YT search image- test so I actually have it backwards we need to put the local image name first so YT search image- test and then we need to put the dockerhub repo name we've now created a new image which we can see here called shahin t/t search- demo and now we can just push it to the docker Hub so that's really easy so Docker push shahen T YT search demo and then we can add a tag to it as well but that's not not necessary so it's using the default tag of latest and now we can see that it's pushing up to the docker Hub so now it's done running if we go back to the docker Hub and hit refresh we see now the image is here now that we've pushed the image to the dockerhub the last step is we can now deploy a container on AWS using their elastic container service the way to do that is we can go to our AWS account if you don't have a AWS account you'll need to make one for this tutorial and then we can go to elastic container service so we can just type in ECS and it should pop up once we do that we'll see we come to a screen like this we can see that I already have a cluster running but let's start one from scratch the first thing we can do is go over to task definitions and click this create new task definition I'll call this one YT search demo we'll scroll down to infrastructure requirements we'll select AWS fargate as opposed to Amazon ec2 instances and the upside of fargate is that you don't have to worry about managing the infrastructure yourself that's all handled behind the scenes and you can just worry about getting your container running and using it as a surfice the next important thing is selecting the operating system and architecture this will depend on the system that you're running for Mac they use Arm 64 so that's the architecture and then Linux is the operating system of our image next we can go to the task size I'll leave it at one CPU but I'll actually bump down the memory to 2 gb and then task roll you can actually leave this as none if this is your first time running it and it'll automatically create this task roll called ECS task execution rle but since that already exists for me I'll go ahead and click that now we're going to specify the container details so I'll call this YT search container demo and then here we'll put the URL of the image which we grab from the docker Hub so I'll grab this and then I'll add the tag of latest and we'll leave it as a essential container port number we'll leave at 80 we'll leave all this as the same we'll leave all this stuff as default we won't add any environment variables we won't add any environment files and then logging leave that all as the default and then we have a bunch of these optional things that we can set like a health check startup dependency ordering container type timeouts so on and so forth we can also configure the storage so there's ephemeral storage so just like shortterm I'll just leave this as the default of 21 we can also add external storage using this add volumes thing which is good if you wanted to talk to some external data source and there's this monitoring Tab and tags tab but not going to touch any of that just going to keep it super simple here and then I'm going to hit create all right so now the task definition has been successfully created now we can go here and we'll see we have this new task definition now we can go over to clusters and we'll hit create cluster I'll call this one YT search cluster demo again we'll use AWS fargate for the infrastructure and then we won't touch the monitoring in the tags hit create now it's spinning up the cluster so this might take a bit so now the cluster has been created we can click on this so we see that the cluster is running but there's nothing running on it there are a few things we can do we can create services or we can create tasks services are good for the we service kind of like this API we're creating a task is better for something that's more of a batch process that runs like once at a predictable time increment but here we'll create a service so to do that we'll click services and then click this create button we're going to use the existing cluster the Wht search cluster demo click on launch type and we'll leave that as fargate and latest we'll make the application type a service we'll specify the family of the task definition and then we can give a service name call it YouTube search API demo we'll leave the service type as replica we'll have the desired tasks as one deployment options we'll leave those as default deployment failure detection leave that as default we won't do service connect I'm not sure what that is service Discovery networking we'll actually leave all this the same we'll use an existing Security Group and then we can enable load balancing if we like but I won't do that here service auto scaling we can automatically increase the number of containers that are running or decrease the number of containers that are running again we can configure the data and what not but we're not going to touch any of that and we'll just hit create so now it's deploying the search API so the API has been successfully deployed it took like 5 minutes or something but now if we scroll down and we click this YouTube search API demo something like this will pop up and we can go over to tasks and we can click this task here and we can see that it'll have a public IP address so what we can do is copy this public IP and then I have a another piece of code here and we'll just paste in the public IP and then we'll make an API call so just 100 milliseconds to make the API call it's actually faster making the API call to AWS than locally which is pretty interesting so this ran just fine here but one thing I had to do yesterday to get this working was go to the YouTube search API demo click on configuration and networking and then go down to security groups that'll open this VPC dashboard thing and I had to add this rule that allowed all inbound traffic from my IP address specifically so if you do that it'll you know have some default Security Group and then you'll hit edit inbound rules and then you can add a additional rule that allows all inbound traffic from my IP you can also have custom IPS which you specify one by one you can have any IP version 4 or any IP version 6 so it wasn't working for me but once I added this inbound rule it was working just fine now that the API is deployed on AWS it makes it a lot easier to integrate this functionality this Search tool into a wide range of applications to demonstrate that I'm going to spin up a gradio user interface that can talk to the API I'll just run this whole thing and this is essentially the same thing that I walked through in the previous video of the series so if you're curious about the details be sure to check that out but now we can see that this user interface got spun up we can search something like full stack data science and we see that search results are coming up this is the great thing about running the core functionality on AWS now we just have this lightweight front end that can interact with the API and return search results through a web interface so you can see that the other videos in this series are popping up in the search results we can search other things like finetuning language models and I had a typo but it doesn't matter and we can see all the content on fine tuning and large language models pops up and I'll just call out that all the code I walked through is freely available on GitHub so if you go to my YouTube blog repository and the full stack data science subfolder you'll see that all this code is available in this ml engineering folder and then you can check out other videos in this series and all the medium articles associated with this series this was supposed to be the last video of this series but then I got a comment from cool worship 6704 on my video on building the data pipeline for this project and they were asking how would you automate this entire process and so that's a really good question and it wasn't something I originally was going to cover but since you have this question here I assume other people have the same question and so just to recap what we did here is we took the Search tool wrapped it in an API put that into a Docker container and deployed it onto AWS so now users and applications can interact with the Search tool but one limitation of how I coded things here is that the video index the videos that are available in the search API is static it's a snapshot from a couple of weeks ago when I made the video on making data pipelines so the OB vious Next Step here would be to create another container service that automates the whole data Pipeline on some sort of time Cadence whether it's every night or every week or whatever it might be and then feed the results of that process and update the search API so that new videos will be populated in the search tool so that's going to be the focus of the next video of this series so that brings us to the end this video was a lot more Hands-On than a lot of my other content I'm experimenting with new new format so let me know what you thought in the comment section below if you want me to dig deeper into any of the tools or Technologies discussed in this video let me know and I can make follow-up videos on those topics and as always thank you so much for your time and thanks for watching
6qCrvlHRhcM,2024-05-11T15:00:08.000000,How to Build ML Solutions (w/ Python Code Walkthrough),n/a
OnIQrDiTtRM,2024-05-03T12:40:43.000000,How to Build Data Pipelines for ML Projects (w/ Python Code),n/a
eayzAZltV9U,2024-04-29T13:54:55.000000,4 Lessons from AI Consulting #freelancing,are four things I've learned from AI Consulting one trust is more important than anything else in my experience what differentiates clients from prospects is the belief that I could solve their problem and that I was on their side two don't skip the discovery phase when providing Technical Services it's easy to jump right into the coding the problem with this however is that you can spend a lot of time and money solving the wrong problem three find your number one sales Channel although there are several ways to get clients for example upwork referrals conference speaking content creation ads and more I and others that I've met in the space get most of their leads through one key sales Channel fourth and finally it's not real until the money's in the bank this is a lesson I've learned over and over again from Great Discovery calls that ended in nothing
03x2oYg9oME,2024-04-25T15:16:00.000000,How to Manage Data Science Projects,this video is part of a larger series on full stack data science in the previous video of this series I introduced this idea of a full stack data scientist and described the four hats that it involves in this video I'm going to dive into the first of these four hats which is that of a project manager I'll start by introducing a five-step project management framework specifically for data science and then I'll walk through a concrete example of the project manager role in implementing this framework and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make before diving into the framework I think it's helpful to take a step back and ask ourselves why do we need project management while there surely cases where project management feels more like red tape and constraints as opposed to something that pushes a project forward the way I see it just just a little bit of planning and structure can go a very long way when it comes to implementing projects that involve a lot of moving pieces here are a few reasons for that first project management involves all of the planning and scoping for a project just like how an architect will draw out a blueprint for a building before it's actually built project managers will draw out the blueprint of a project before it is implemented next data science projects often involve bringing together multiple pieces this includes data sets compute Technologies and of course this involves people these could be individual contributors they could be your stakeholders or other people involved in making the project happen and then finally project management helps keep projects on time and on budget so while this video is specifically about data science projects everything that I said here is not something unique to data science projects in fact these three things are going to be relevant to project management in any domain however there are some unique considerations for data science projects which brings up a five-step project management framework that I like to use When approaching a model build or the development of some other datadriven solution the framework is shown here where it's broken down into five phases starting from phase zero which is what I call the problem definition and scoping phase phase one is the data acquisition exploration and preparation we can also think of this is all the data engineering work next is phase two which is the solution development this could be training a model or it could be doing some kind of analytics this will typically involve a data scientist or maybe even a data analyst next phase three is the solution deployment so this is actually taking the solution and putting it into the real world so that it can actually have an impact and then finally phase four is the evaluation and documentation step let's dive into each of these phases a bit more deeply starting with phase zero the problem definition and scoping phase this will involve formulating the business problem basically asking ourselves what problem are we actually trying to solve next is designing a solution to that problem any given problem will have countless ways it can actually be solved so it's important to make a decision on the best way one can solve the problem at hand finally defining the project requirements and road map so in other words what do we need to make this project happen and what will the implementation look like next we have phase one which is the data acquisition exploration and preparation so this might involve looking at internal datab bases looking to third-party data vendors or any other data source evaluating available data also means asking the question do we have sufficient information to solve the problem at hand another key step of this phase is acquiring and exploring the data so this includes things like exploratory data analysis where one is just trying to get a sense of the information that can be gleaned from the data set and then finally the development of data pipelines so this is your extract transform and load pipelines or your extract load and transform pipelines next we have phase two which is the solution development so this might be the part most people are excited about which is the model development so developing Ai and Data Solutions but it's not just about training a model or building some other analytic solution it's also about evaluating that solution in terms of its validity and the value that it generates and this evaluation process will often involve iterating with stakeholders next we have phase three which is solution deployment so the basic idea here is to integrate the solution into the real world business context whether that's automating a particular workflow or making information available to stakeholders via a dashboard maybe updating a widget on a website so this can take many different forms additionally for some sort of intervention you know you're updating a business process you also may need to implement a solution monitoring pipeline it's not just about making a change and walking away but making a change and consistently evaluating the efficacy of that change and then finally phase four is the evaluation and documentation step so this involves assessing project outcomes and comparing them against the expectations from the outset of the project this is also delivering technical documentation and user guides and then finally doing a perspective reflecting back on the project looking at things that went well looking for opportunities for improvement considering the Future Works and the limitations of the current project and starting to talk about the obvious next steps and of course we have these feedback loops in this five-step framework some key ones are shown here where you may come to phase one and you might be evaluating the available data and you may find that we thought we could solve this problem we could build this model with the data we have internally available but upon further investigation that's not the case and we'll have to figure something else out so that'll require going back to phase zero and asking ourselves okay how can we get this data do we look to a data vendor do we try to get it from publicly available sources another key feedback loop is between phases 2 in phase one so say during model development there seems to be some kind of bias in the model and then upon further investigation you find that a particular exception wasn't properly handled in the transform phase or the data pre processing step so that'll require going back to phase 1 updating the data preparation Pipeline and then returning back to phase 2 to train the model another feedback loop is from Phase 2 to phase zero so the data may be properly pre-processed and there's nothing wrong with it but still upon training the model the predictive performance may not be as good as the existing solution so this might require going back to phase zero and reassessing the solution design and asking is there a better way we can solve this problem the final feedback loop is if everything goes according to plan value is generated but of course no project is ever complete there are always opportunities for improvement and then You' return back to phase zero to start building a broader solution so while the project manager is ultimately responsible for the successful implementation of the project and ensuring that each of these phases happens the key contribution of the project manager is this phase zero because if this phase zero is is not done properly it's going to cause problems in every subsequent phase of the project so this brings us to the key role of the project manager which is this phase zero the problem definition and scoping and as we saw on the previous slide this consists of three key steps the first is the problem diagnosis it's often the role of the project manager to collaborate with stakeholders and project owners to get a clear understanding of the problem that they're trying to solve and it's critical to get this step right because if you think about it if you don't diagnose the problem properly you can spend a lot of time and effort solving the wrong problem which no one wants the stakeholder doesn't want that and the team doesn't want that so being able to facilitate these conversations with stakeholders is a key skill of any project manager while I won't go too deeply into what this might look like I do have an entire video dedicated to this topic which I'll link up on the screen here so once you have a clear idea of what problem you're trying to solve then comes the task of identifying the best way to solve that problem so this requires a project manager to bring together a lot of different information they have to bring together the business context the priorities of the stakeholder they have to consider the available Technologies the available resources the available data the available budget the available timeline and they're also might be competing priorities and beyond that any given problem will have a wide range of potential Solutions and varying degrees of complexity and scope so at the end of the day the project manager has to synthesize this information to help the stakeholder or the project owner make the BET of what's the best way to solve the problem and then finally is the implementation plan once the solution has been defined there's still the matter of the details of what this thing's actually going to look like and how it's going to be built and even still any given solution can have a wide range of potential implementations so to make this more concrete I'm going to walk through a concrete case study the point of this is to one perform the phase zero for this project that I'm building but the second is to walk through step by step what it looks like being a full stack data scientist and so here this is me putting on the project manager hat for this project starting with a bit of background I make content on YouTube and write articles on medium what this looks like is I'll make content about whatever is interesting to me whatever I'm curious about or things that I've personally experienced since I am just talking about whatever's interesting to me it might be difficult for people to navigate all the different pieces of content I make across these two platforms which brings up the problem potentially I talk about too many topics across too many platforms so to kind of give a flavor for this I'll talk about things from topological data analysis AI for business causal inference how much I made writing on medium how to get a data sign job more philosophical things like what it means to be antifragile more personal things like my struggles with anxiety and now to this series of becoming a full stack data scientist so someone who's seeing this content for the first time or is trying to navigate this very diverse landscape of content might have trouble finding the content that's most relevant to them which brings up the proposed solution which is a semantic search function for my YouTube videos what this might look like is a page where users can type in a natural language query or a question and then the web page will return search results of YouTube videos relevant to that query so this only addresses half of the problem I mentioned on the previous slide which is I make too many topics on too many channels this only solves that first problem of too many topics so the hypothesis is if these topics are more easily searchable and they're easier to navigate then more people will engage with the content and then more people will share the content and that will promote the growth of the YouTube channel and so you might be thinking sha why are you just solving half the problem why not solve the whole problem that brings up this broader question of should I build a POC or a proof of concept I know there are a lot of different opinions on poc's and some people love them some people hate them I personally believe that poc's are very valuable when it comes to building machine learning projects you can do all sorts of interesting things with machine learning but of course of all these interesting things you can do only a small subset of them actually generate value so the first reason why I believe in Po's is that they help you validate the idea I can put together this proof of concept version of the web page I can make it available to some people in my audience I can make it available to some people that I know and they can provide me feedback which will help in making this decision of is this something worth pursuing or should I just cut my losses and move on to something else the second reason is often with data science projects efforts tend to stack on top of each other so if I properly build this proof of concept for YouTube videos it should only be a marginal effort to include my medium articles into the same interface now that we have a clear picture of what the solution will look like how do we actually make that happen that brings us to the implementation plan the first part of which are the project requirements so this includes the roles the data the compute infrastructure and the Technologies needed to implement the project here I list off the roles of the full stack data scientist so these are the four hats mentioned in the previous video the first is the project manager then we have the data engineer data scientist and ml engineer and what's important to note even if you are a full stack data scientist maybe you still want to bring in a data engineer or a data scientist to help you build the project or maybe you want multiple data Engineers or maybe you want to bring in someone to do the data science and the ml engineering piece while you just do the project manager and data engineering piece so it's important to note that roles do not equal people multiple people can do multiple roles and a single role can be done by multiple people the next is the data requirements so here I put an evaluation data set which will consist of 50 query video pairs that will help evaluate the quality of the search so in other words I need a data set I can use to evaluate the performance of the search function that I built so what that'll look like is a list of 50 queries and an associate ated YouTube video ID with that query for infrastructure I'm going to use AWS light sale which makes it super easy to deploy Docker containers and then finally Technologies I included all that I could think of at least at this point so I'm going to use Python for basically everything I'm going to use the YouTube API to pull in some data there's a python library that downloads the automatically generated captions of a YouTube video going to use pandas or polers to handle the data structures the sentence Transformers library to generate text embeddings fast API to make an API Docker to containerize scripts and then gradio to spin up a front end and then finally I'm going to put everything in a GitHub repo and have the documentation be part of that repo next we have the project road map so this consists of a few different things these are the project Milestones which I map to phases 1 through four of the five-step project management framework discussed earlier then we have a task description so what are the tasks that make up this Milestone or this phase assigning a role to each task and assigning a due date to each task you can add more things here like a more detailed description or acceptance criteria but I would say that these four elements of Milestone task roll and due date are the bare minimum you need to make a proper project roadmap I'll just kind of briefly fly through these I don't want to spend too much time reading it but you can always pause the video and read through these if you like phase one is all the data engineering stuff so this is extracting the transcripts and saving them as a paret file phase two will consist of exploring multiple embedding models and testing the search function using the evaluation data set and then once that's done creating a video index that is searchable phase three will consist of building an API for this search function containerizing it testing it locally building a simple gradio UI and then deploying it on AWS SP war will consist of creating the documentation and doing a project retrospective so future videos of this series will walk through each of these steps and I haven't built this you're seeing this thing live you're seeing this thing raw so I'm sure there's going to be some changes here things are going to come up that I didn't expect and I'm going to have to come back to phase zero maybe I fall behind on the due date since this is an independent project I don't have a manager I don't have a broader team holding me accountable but committing to these due dates on this video and committing to posting a video every single week on YouTube is helping give me structure to this project and keeping me motivated so everyone's comments from the previous video saying I'm excited for the next video of this series were actually very helpful to me to sit down and be productive and work on this project the next video of the series is going to walk through phase one of this project so everything I described in the previous slide so at a high level this will consist of building a data pipeline more specifically an extract load and transform pipeline so what that'll look like is we'll start by extracting video captions from from YouTube using python then we'll load these video captions into a paret file and then we'll transform these captions into text embeddings so this is our data pipeline that I'll discuss in the next video since I am using an elt extract load and transform Paradigm here technically this will actually be in Phase 2 elt sounds better than e so that's why I put it all here okay so that brings us to the end I hope you got some value out of this video like I mentioned in the previous video this whole series is part of my own own personal learning process so if you have any questions or suggestions on this project please drop those in the comment section below those are very valuable to me and as always thank you so much for your time and thanks for watching
O5i_mMUM94c,2024-04-19T14:05:54.000000,How I’d learned #datascience (if I had to start over) ￼,here's how I'd learn data science if I had to start over I start by making a YouTube playlist with high level introductions to data science and putting together a list of Articles to read on the subject next I'd set up 10 interviews with data scientists to learn from people that are actually doing it I'd ask basic questions like what is data science how did you get into it and do you have any advice for me next I'd do a project learning from others can only get you so far eventually you need to get your hands on the keyboard if I was super green I'd start with a basic project using data from kaggle however if I'm starting with some adjacent experience I try to grab some data from The Real World and solve a real problem then finally I teach someone what I learned by making a YouTube video about the project or writing an article
xm9devSQEqU,2024-04-18T15:59:02.000000,4 Skills You Need to Be a Full-Stack Data Scientist,n/a
Z6CmuVEi7QY,2024-04-11T10:00:27.000000,How I'd Learn Data Science (if I could start over),when I was first learning data science it was easy to get overwhelmed by the mountain of buzzwords and technical details and hard to know exactly where to start while the number of buzzwords has only seemed to increase since I started I now have a much clearer view of the space and its essential elements in this video I'll answer one of the most common questions I receive which is how would you learn data science if you had to start over today I can honestly say this is the exact approach I would use because I'm currently foll following it to learn data engineering and ml engineering and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way you can support me in all the videos that I make I've been doing data science for the past 5 years in a few different contexts including research freelance and corporate one of the biggest challenges of learning data science is that it consists of a constantly changing ocean of Technologies the best way I found to cope with this constant change is to just get really good at learning and the way I see it there are four ways we can learn the first way is through consuming content this includes reading books taking courses watching YouTube videos second is through mentorship and learning from those who are ahead of you third is by doing so getting your hands on the keyboard and the fourth is by teaching when it comes to learning a comp completely new subject or field I combined these four methods into a four-step strategy here I'm going to walk through what that might look like for data science step by step the first step is to consume content this is especially important if I know absolutely nothing about the field I'm always amazed by the amount of clarity I can gain from Reading one good article or watching One Good YouTube video on a subject unfortunately what makes a good resource will vary from person to person for example I prefer to read revieww papers and other scientific articles when available however if you would have given me a paper to read when I was an undergrad I probably wouldn't have read it and even if I did I probably would not have gotten much out of it so it's important to find which modalities work best for you to get started I'd go to Google Scholar and search something like data science review here I'd be mainly looking for titles that sound interesting and relevant and looking at the number of citations each article has received to be clear this isn't meant to be a literature review so I'll typically pick out one to three articles that seem relevant next I'd go to YouTube and search something like what is data science similar to Google Scholar I'd evaluate videos based on their title and number of views so while doing these searches it's important to point out that I'm not actually reading these papers or watching these videos I'm just getting all these resources together to consume them later to actually consume the content I'll block time on my calendar specific spefically for learning this is important because it increases the probability that I'll actually do it I personally like to do my reading first thing in the morning while watching YouTube videos is something I can really do anytime either way I'll have my notes app open while consuming the content to take notes on key things that stand out to me and ensuring I site my sources in case I want to come back to them later or share it with anyone else although I recommend you do this kind of search for yourself I've linked a few resources in the description below in case it's a helpful jumping off point after getting a basic grounding of data science through consuming content I start reaching out to professional data scientists to see if they'd be willing to talk to me about their experience learning from those ahead of me is one of the greatest hacks I've learned for growth this is something that I did when first exploring entrepreneurship in the data space and something I'm doing currently in learning data engineering and ml engineering I found that interviewing 10 people is a good number because it gives a nice diversity of perspectives to help give a broad view of the field but of course getting 10 Busy professionals to talk to you is much easier said than done here's how I would do it I'd go to LinkedIn and search for data scientists that I'm directly connected with then I'd shoot them a DM saying something like hey hope things are going well I'm currently doing interviews with data scientists as part of an effort to expand my technical data skills would you be willing to do a call sometime this month if the person doesn't respond within a few days I'd follow up with something like hey I wanted to follow up did you get a chance to look at my previous message doing this recently for data and ml Engineers more than 50% of people responded to my messages of course this will vary from person to person but if we assume that 50% of the people you reach out to respond and of those 50% another 50% agree to talk to you that means you need to reach out to 40 data scientists in order to talk to 10 of them I'm personally connected to hundreds of data scientists on LinkedIn so this would be no problem for me however if I was only connected to say 20 data scientists I'd take the following approach I'd add more personalization to that initial reach out and be sure to follow up a third time for those who don't respond to the first two messages this should increase the number of people that actually respond to my request if this doesn't get the 10 interviews I'd expand my Outreach to data scientists I have at least one mutual connection with reaching out to strangers naturally has a lower success rate so personal iation and follow-ups are even more important for these contexts when actually getting on the calls with these professionals I'd ask basic questions like what is data science how did you get started what kind of projects are you working on these days what does your Tech stack look like and do you have any advice for me I'd be sure to take notes on these calls so I can compare different interviewees perspectives and do any deep dives into specific tools or resources that they mention in other words i' jump back to Step One to to fill in any gaps that may have come up during the interviews consuming content and learning from others can only get you so far eventually you need to get your hands on the keyboard and start doing this is where doing a data science project comes in if I was completely new to programming and analytics I would start with a very basic project like creating a simple data visualization what this might look like is installing python on my machine downloading an interesting data set from kaggle loading it in with pandas or polers processing the data in some way whether that's cleaning the data or doing some sort of analytics and then finally visualizing the data with matte plot lip if I was starting with some adjacent experience in say python or statistics I'd try to grab data from The Real World and solve a meaningful problem some examples of this might be creating a model to automatically create chapters for my YouTube videos predicting the click-through rate of a video based on the thumbnail and the title or building a web app that allows people to do semantic search over all my YouTube videos and blogs I find that doing projects to solve real world problems are not only more rewarding but much more instructive than purely academic ones as I make progress on the project I'd likely be jumping back to steps one and two as needed so to either read up on a specific approach or technology or to ask people with more experience for guidance in case I get stuck once I've completed my first project i' move on to the final step of teaching in my opinion teaching is the ultimate way to learn and the main reason why I make YouTube videos when explaining something to others I'm forced to distill my understanding into clear and concise language in this process I find that I'll stumble upon gaps in my understanding that I didn't realize were there this provides me the opportunity to bridge these gaps and learn more the three teaching modalities that I found most helpful are making YouTube videos writing medium articles and giving presentations I really like the first two methods cuz I can do them anywhere at any time the third method is also great because I get the added pressure of talking to a live audience and taking their questions but of course this requires a bit more work to make happen so to keep things simple I'd make a slide deck outlining my project and key learnings and then make a YouTube video of me walking through it although many will agree that teaching is a great way to learn most won't follow through on this they might be thinking what if I say the wrong thing what are people going to say about me what if I embarrass myself the best strategy I found for overcoming these fears is focusing on my own personal development and growth in other words it doesn't matter what other people think as long as I'm learning and for me these fears actually serve as F to help make sure that I really understand what I'm sharing and in the inevitable case that I do make a mistake that's beneficial too because those those public mistakes tend to stick much better than the private ones while this four-step strategy is simple putting it into practice may present some unexpected challenges to help with that here are three key habits I find helpful for learning first is to make room for it like I mentioned earlier I find it critical to block time for learning to ensure that it actually gets done for me this has helped develop this habit for continual learning the second is be willing to look dumb one of the biggest mistakes I made early in my educational career was an unwillingness to look dumb what this typically looked like was staying quiet and not asking questions when someone was telling me something I didn't really understand when I overcame this fear in grad school it unlocked a whole new level of learning and progress and third pursue curiosity again data science is an ocean of Technologies even if you spend every waking moment of everyday learning you still wouldn't come close to learning everything that's why I find it much more productive to pursue topics that peique my interest this is what makes data science fun even if the topics are technically challenging at first so that brings us to the end I hope this video was helpful to you in some way if you have any data science questions feel free to share them in the comments below or feel free to set up some office hours using the link in the description and as always thank you so much for your time and thanks for watching
INlCLmWlojY,2024-04-04T18:45:00.000000,I Was Wrong About AI Consulting (what I learned),"last year I quit my corporate data science job to pursue entrepreneurship full-time my plan was to sell data science Services as a way to fund the development of a product I could build a business around while this made a lot of sense on paper pursuing this path over the last N9 months has made me realize this plan was flawed in this video I'm going to share my experience and some key Lessons Learned in case it is helpful to anyone on a similar journey and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make right out of grad school I went to work as a data scientist at Toyota this was in many ways my dream job and an incredible learning experience for me however after about 6 months in the role that initial excitement and learning curve began to flatten out and I slowly began to realize that the role was no longer aligned with my longer term goal of running my own business so after about a year in that role I decided to pass on a senior data scientist promotion and tank my income from over 10K a month down to basically zero since I had done some freelance work in grad school and had grown a small audience on YouTube my plan was to bring these things together and leverage my content to sell consulting services and to my surprise it worked over the next 8 months I took 36 Discovery calls of these 36 calls two of them turned into contracts and last month one of these contracts turned into an even bigger opport opportunity of over $25,000 where I was sitting in a project manager role and not doing any of the coding myself while it may sound like things were going great something was off this was similar to what I felt facing the promotion at Toyota it was a great opportunity on paper but something about it didn't feel aligned with my long-term goals so I made the tough decision to pass that opportunity off to another consultant looking back it's clear that my expectations of Consulting didn't match reality when I started this journey I saw Consulting as an easy way I could make cash while I explored other Ventures however after pursuing it as my main source of income as opposed to a side hustle like I did in grad school it became obvious that running a Consulting business wasn't as simple as I expected not just because of the technical challenges of building AI projects but also selling yourself nurturing leads working with subcontractors and the list goes on and on in fact most of the work were these non-technical aspects of the job with the biggest piece being the sales process as I've learned there are many unique challenges in selling AI Services three of which are as follows one for most businesses AI is a nice to have rather than a musthave so a lot of times it's not the client's number one priority two building AI projects requires a lot of experimentation and iteration which introduces a lot more uncertainty than the traditional software development process and reduces the perceived value of your offer and three since these are typically High ticket contracts they often require multiple touch points with the client before they close and I found this extra time commitment difficult to manage as a solo operator although I was learning a lot Consulting was taking up much more of my time and attention than I had anticipated so much so that my content output began to slow down and I virtually had no time to work on my own projects which was supposedly the main goal of all this this experience led me to take a step back and reminded me of some advice I had received from a successful product entrepreneur about a week after quitting my job I had asked him if Consulting was a good stepping stone to product development to which he immediately responded no the advice he gave was simple if you want to build a product then build a product looking back it's kind of funny that it took me 9 months to realize what he had told me 9 days into this journey but here's what I didn't fully appreciate building a product is hard building a consultancy is hard building a brand is hard entrepreneur ship is just hard the trick at least in my opinion is to pursue the hard thing that gets you fired up and that you find fulfilling and after trying it for 9 months I realized that selling AI projects to clients didn't get me as fired up as some of the other things I was working on that's why last quarter I removed the discovery call option from my website and passed that first major contract off to another consultant although building a consultancy wasn't for me I still believe it's a great business for those who enjoy it it also taught me a ton about sales marketing and working with customers which are universally applicable skills and Entrepreneurship if I had to boil it down here are my four key takeaways from this experience first is trust is more important than anything else for me what differentiated clients from prospects was the belief that I could solve their problem and that I was on their side through a lot of trial and error I eventually landed on the following approach be curious be transparent and be yourself more specifically be curious about the client's problem and where they're coming from be transparent about the limits of my skills and knowledge and to just be myself not trying to put up a front and pretend to be something that I'm not the second takeaway was not to skip the discovery when providing Technical Services like data science it's easy to dive head first into the coding the problem with this is that people end up spending a lot of time and energy solving the wrong problem that's why at the outset of every project it's critical to put on your project management hat so you can understand the business problem and fully scope a proposed solution the third takeaway is to find your one sales Channel although there are countless ways you can get clients upwork Fiverr cold Outreach LinkedIn content creation speaking at conferences referrals and the list goes on and on I and most of the people that I've interacted with in the space have just one main lead source for me my main source was my YouTube channel Channel and my funnel looks something like this someone would watch a YouTube video book a discovery call after the discovery call we would do a paid Discovery phase where the goal was to get a clear understanding of the client's problem and to scope out the project requirements and goals following the paid Discovery is building a proof of concept and then after the POC building an MVP the fourth and final takeaway is that it's not real until the money's in your bank account this is a lesson I had to learn over and over again and maybe I still haven't learned it there were many times I would have a great discovery call or multiple calls with prospects and it seemed like they were ready to move forward but then days and weeks would go by and I wouldn't hear from them and so while there's always excitement in sales I had to adopt this mindset to avoid going on these weekly emotional roller coaster rides at this point you might be thinking sha if you're not selling your data sign skills how are you going to make money while contract work has great short-term earning potential it is not my only Revenue source there are three other ways I've generated Revenue these past 8 months this includes revenue from my YouTube channel my medium blog and ad hoc paid Consulting calls which have generated a total of $766 38 although this isn't enough to pay the bills there's another thing here that's worth taking into consideration since quitting my job my YouTube channel has grown from 2,000 subscribers to 18,000 subscribers along with that my revenue from YouTube went from $100 in the first 3 months to 1,600 in these past 3 months which brings me to my new plan post one YouTube video a week while this might sound like an overly simplistic and also super risky plan here's my reasoning one YouTube is actually working for me two it allows me to focus on one thing three making one video a week gives me a clear quantifiable goal I can use to structure all of my efforts for instance here's a list of things that can go into making a YouTube video reading papers writing medium articles writing code examples talking to people conducting interviews building projects workshopping content ideas on other social media platforms and probably a lot more now here's a list of things that can result from making a YouTube video learning a new skill or topic getting more paid calls more speaking gigs more inbound leads more people joining the data entrepreneurs more content from my other channels and growing my audience nevertheless committing to one thing is scary especially something unpredictable like YouTube however the longer I spend on this journey the more I realize that commitment and focus are necessary ingredients for Success because this is the only way that every ounce of your effort can go in the same direction and to quote a fellow entrepreneur and friend Michael Lynn if you're doing less and less that means you're going in the right direction and indeed this feels like the right direction at least for now 9 months into this entrepreneurship journey I have three Reflections that are top of mind the first is I could have a very successful Consulting business and I could have a very successful YouTube channel but I can't have both I have to pick one and personally I just like making YouTube videos more the second is a subtle mindset shift which is instead of asking yourself will this thing work ask yourself how could I make this thing work it may seem like a subtle shift but this is the mindset that I'm adopting this quarter in making YouTube my main focus and the third and final mindset is to trust yourself trust that you'll figure it out trust that if you're backed into a corner your survival Instinct will kick in and you will solve the problem thanks for watching to the end I hope you got some value out of this if you have any specific questions about my journey feel free to drop them in the comment section below and as always thank you so much for your time and thanks for watching"
sNa_uiqSlJo,2024-03-29T15:57:34.000000,"Text Embeddings, Classification, and Semantic Search (w/ Python Code)",n/a
Ylz779Op9Pw,2024-03-18T17:32:21.000000,How to Improve LLMs with RAG (Overview + Python Code),this video is part of a larger series on using large language models in practice in the previous video of this series we saw how we can efficiently fine-tune a large language model to respond to YouTube comments in my likeness while the fine-tune model did a really good job at capturing my style when responding to most YouTube comments it didn't do so well when responding to technical questions which required more Niche and specialized knowledge in this video I'll discuss how we can improve llm based systems using retrieval augmented generation or rag for short I'll start with a highle overview of rag before diving into a concrete example with code and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this video please consider subscribing that's a great no cost way you can support me in all the content that I make a fundamental feature of large language models is the ability to compress World Knowledge the way this works is you take a huge slice of the world's knowledge through more books and documents than anyone could ever read in their lifetime and you use it to train a large language model and what happens in this training process is that all the knowledge and Concepts and theories and events that have happened in the world that are represented in the text of the training data they get represented and stored in the model's weights so essentially what has happened is we've compressed all that information into a single language model well this has led to some of the biggest AI Innovations the world has ever seen there are two key limitations for compressing knowledge in this way so the first limitation is that the knowledge that is compressed in a large language model is static which means that it doesn't get updated as new events happen and new information becomes available and for anyone that's used chat GPT and tried to ask it a question about current events probably have seen a message like this as my last update in January 2022 I don't have access to real-time information so I can't provide specific events from February 2024 the second limitation is that these large language models are trained on a massive Corpus of text the result of that is that they're really good at general knowledge but they tend to fall short when it comes to more Niche and specialized information mainly because the specialized information wasn't very prominent in their training data and so when I asked chbt how old I was it said that there was no widely available information about shahim tab's age he might be a private individual or not widely known in public domains one way we can mitigate both of these limitations is using retrieval augmented generation or rag for short starting with the basic question what is rag this is where we augment an existing large language model using a specialized and mutable knowledge base base so basically we can have a knowledge base that contains domain specific information that is updatable where we can add and remove information as needed the typical way we'll use a large language model is we'll pass it a prompt and it will spit out a response this basic usage will rely on the internal knowledge of the model in generating the response based on the prompt if we want to add rag into the mix it would look something like this so instead of starting with a prompt we'll start with say a user query which gets passed into a rag module and what the rag module does is that it connects to a specialized knowledge base and it will grab pieces of information which are relevant to the user's query and create a prompt that we can pass into the large language model and so notice that we're not fundamentally changing how using the large language model it's still prompt in and response out the only thing we're doing is augmenting this whole workflow using this rag module which instead of passing in a user query or prompt directly to the model we just have this pre-processing step to ensure that the proper context and information is included in the prompt one question you might have is why do we have to build out this rag module can't we just fine-tune the large language model using specialized knowledge so that we can just use it in the standard way and the answer to that question is yes so you can definitely fine-tune a large language model with specialized knowledge to teach it that information so to speak however empirically fine-tuning a model seems to be a less effective way of giving it specialized knowledge and if you want to read more about that you can check out Source number one Linked In the description below with this basic understanding of what rag is let's take a deeper look into this rag module to see how it actually works the rag module consists of two key elements first is the Retriever and second is the knowledge base so the way these two things work together is that a user query will come in it'll get pass to the retriever which takes the query and searches the knowledge base for Relevant pieces of information it then extracts that relevant information and uses it to Output a prompt the way this retrieval step typically works is using so-called text embeddings before we talk about how we can use text embeddings to do search let's talk about what they are exactly put simply text embeddings are numbers that represent the meaning of some given text so let's say we have a collection of words like tree lotus flower daisy the sun Saturn Jupiter basketball baseball satellite spaceship text embeddings are a set of numbers assoc associated with each word and concept that we're seeing here but they're not just any set of numbers they actually capture the meaning of the underlying text such that if we are to plot them on an XY AIS similar concepts are going to be close together while Concepts that are very different from each other are going to be spaced far away here we see plants tend to be located close together celestial bodies tend to be close together these Sports balls tend to be close together and things that you typically see in space tend to be close together and notice that the balls are closer to celestial bodies than they are to say plants because perhaps balls look more like celestial bodies than they do plants and trees so the way we can use this for search is say each of these items is a piece of information in our knowledge base you know we have some description of this tree a description of this lotus flower the description of Jupiter and so on and so forth what we can do is represent each item in our knowledge base as a point in this embedding space and then we can represent the user's query as another point in this embedding space and then to do search we simply just look at the items in the knowledge base that are closest to the query and return them as search results that's all I'll say about text embeddings and text embedding base search for now this is actually a a pretty rich and deep topic which I don't want to get into in this video but I'll talk about in the next video of this series next let's talk about the knowledge base say you have a stack of documents that you want to provide to the large language model so you can do some kind of question answering or search over those documents the process of taking those raw files and turning them into a knowledge base can be broken down into four steps the first step is we'll load the documents what this consists of is getting together the collection of documents you want to include in the knowledge base and getting them into a ready to parse format the key thing here is that you want to ensure the critical information in your documents is in a text format because at the end of the day large language models only understand text so any information you want to pass to it needs to be in that format the next thing you want to do is chunk the documents the reason that this this is an important step is that large language models have a fixed context window which means you can't just dump all your documents into the prompt and pass it to the large language model it needs to be split into smaller pieces and even if you have a model with a gigantic context window chunking the documents also leads to better system performance because it often doesn't need the whole document it might just need one or two sentences out of that document so by chunking it you can and ensure that only relevant information is getting passed to The Prompt the third step is to take each of these chunks and translate them into the text embeddings we saw on the previous slide so what this does is it'll take a chunk of text and translate it into a vector or a set of numbers that represents the meaning of that text finally we'll take all of these numbers all these vectors and load them into a vector database over which we can do the text embedding based search we saw on the previous slide so now that we have a basic understanding of rag and some key Concepts surrounding it let's see what this looks like in code here we're going to improve the YouTube comment responder from the previous video with rag we're going to provide the fine-tuned model from the previous video articles from my medium blog so that it can better respond to technical data science questions and so this example is available on on Google colab as well as in the GitHub repository the articles that we use for the rag system are also available on the GitHub and the fine-tuned model is available on the hugging face Hub so we start by importing all the proper libraries this is code imported from the Google collab so there are a few libraries that are not standard including llama index the PFT Library which is the parameter efficient fine-tuning library from hugging face there's Auto gptq Q which we need to import the fine tune model as well as Optimum and bits and bytes and if you're not running on collab also make sure that you install the Transformers library from hugging face with all the libraries installed we can just import a bunch of things from llama index next we're going to set up the knowledge base there are a few settings we need to configure in order to do this first of which is the embedding model the default embedding model on llama index is actually open AI but for this example I wanted to keep everything within the hugging face ecosystem so I used this hugging face embedding object which allows us to use any embedding model available on the hugging face Hub so I went with this one from ba AI it's called BGE small version 1.5 but there are hundreds if not thousands of embedding models available on the hugging face Hub the next thing I do is set this llm setting to none and the reason I do this is that it gives me a bit more flexibility in configuring The Prompt that I pass into the fine-tuned model and then two things I set here are the chunk size which I go with 256 characters and the chunk overlap this wasn't something I talked about but we can also have some overlap in between the chunks and this just helps avoid abruptly chopping a chunk in the middle of a key idea or piece of information that you want to pass into the model with all these settings configured we can can create this list of documents using this simple directory reader object and the load data set method here I have a folder called articles which contains three articles in a PDF format from my medium blog and what happens is this line of code will just automatically go through read the PDFs chunk it and store them in this list called documents so there's actually a lot of magic happening under the hood here the next thing I do is just a little bit of ad hoc pre-processing of the text there are chunks that don't include any relevant information to the meat of the article itself and the reason is these PDFs were printed directly from the medium website so there's a lot of text that is before and after the article itself that's not really relevant to the use case here so here are just three ad hoc rules I created for filtering chunks the first thing I remove is any chunk that includes the text member only Story the reason is this will typically be the text before the article and it'll look something like this it'll say member only story then it'll have the title of the article and then it'll have my name the author's name and then it'll say like where it was published it was 11 minute read when it was published and it'll have the image caption and some just irrelevant text to the article itself another rule I use here is that I remove any chunk that includes the data entrepreneurs this is text I include in the footer of each of my articles which links the reader to the data entrepreneurs community so you can see what that might look like is this is the last sentence of the article although each approach has its limitations they provide practitioners with quantitative ways of comparing the fat tailed inness empirical data probably not helpful to any questions that you're going to ask about this article and most of it is just text from the footer of the article and then finally I remove any chunk that has Min read which typically comes up in the recommendations after the article so we can kind of see that in this chunk of text here and of course this isn't a super robust way of filtering the chunks but a lot of times your pre-processing doesn't have to be perfect it just really has to be good enough for the particular use case and then finally we can store the remaining chunks these remaining 61 chunks into a vector data store using this line of code so now we have our knowledge base set up so index is our Vector database that we're going to be using for retrieval with a knowledge base set up the next thing we're going to set up is the retriever first we're going to define the number of docs to retrieve from the knowledge base and then we're going to pass that into this Vector index retriever object the two things we need to pass here are the index or the vector database and the number of chunks to return from the search next we assemble the query engine so the query engine brings everything together it takes in the user query and spits out the relevant context and so we can use the query engine in the following way so let's say the query is what is fat tailedness which is the same technical question we passed to the fine-tuned model in the previous video of the series and the query engine spits out this response object which includes the top three most relevant chunks but it also includes a lot of other information such as the file name that the chunk was retrieved from the page number the date accessed and some other metadata so in order to take this response and turn it into something we can actually pass to a large language model we'll need to do a little reformatting which I do in this chunk of code here and then we can print what that looks like and so this text is probably small on your screen but you can see that there are three chunks of text and this is ready to go and be passed into a promp pred and subsequently fed into a large language model so at this point we now have all the ingredients of our rag module we have our knowledge base which was created using three PDFs and then we set up the retrieval step which takes in a user query and Returns the relevant context from the knowledge base so the next thing we need to do is to import the fine-tuned model so that we can generate a response to the user query and so here we're importing a few things from the PFT and transform forers Library this is the base model that we fine-tuned in the previous video and then here we transform the base model into the fine-tuned model based on the config file available on the hugging face Hub and then we load the tokenizer this is all stuff I reviewed in the previous video of the series so I would check that out if you're curious to learn more so now that we have the fine-tuned model imported let's use it to respond to a technical question without using the rag system so the way that'll look is we'll create a prompt this is the same prompt from the previous video and I'm creating a prompt template using a Lambda function which will dynamically take the instruction string here and the user comment which I Define here to create the prompt so when we print it it looks something like this where we have the instruction start and end special tokens here we have the instruction string here and this goes on for a long time and then here we have the comment so with that prompt defined we can pass the prompt into our tokenizer to translate it from words into tokens we can then pass those tokens into our model to generate response and then we can print the output this is what the model says without any context from the rag system great question fat tailedness is a statistical property of a distribution in simple terms it refers to the presence of extreme outliers or heavy tails in the distribution for instance consider the distribution of heights in a population a normal distribution would have most people cluster around an average height with a few people deviating slightly from the mean however in a fat tailed distribution you would observe a larger number of people being significantly taller or shorter than the average in finance fat tailedness is often used to describe the presence of extreme Market moves or outliers in financial returns it's an important Concept in risk management and financial modeling I hope this helps let me know if you have any questions sha GPT overall the tone in the format is really good here it explains things in simple terms however the description here doesn't really match the description in my video on fat tailedness or the article on fat tailedness so let's see what happens when we inject the proper context into the prompt using the rag System created earlier we'll again create a Lambda function so here we're not just passing in the comments but we're also going to pass in the context of the comment and so this is something we'll get from our rag system but given that we have both we'll have the same instruction string as before but now we're going to inject the context into the prompt and then we'll put the comment in the same place as it was before so with this new prompt template we can pass in the context and comment to get a new prompt we'll pass in the prompt to the token tokenizer in exactly the same way as before to generate tokens and we'll pass those tokens into the model to generate an output and then we'll print the results here's the model's response when we include context using rag great question fat tailedness is a property of a distribution that describes how much the presence of extreme values rare events affects the overall statistics of the distribution in the context of the video the author discusses how mediocris and extremist distributions differ in terms of fat tailedness mediocris distributions have a relatively small impact from extreme values while extremist distributions are heavily influenced by extreme values the author also mentions that fat tailedness is a more General concept than specific distributions like Paro or power laws instead it lives on a spectrum ranging from thin-tailed gausian to very fat tailed Paro 8020 I hope that clarifies things a bit let me know if you have any questions shot GPT so this does a much better job at capturing the way I describe fat tailedness in my video it defines fat tailedness as how much rare events Drive the overall statistics of a distribution and it even talks about mediocris and extremist which is something I talked about in the video to frame the difference between thin-tailed and fat tailed distributions I also like that it mentioned that fat tailedness is not like a binary thing which is something I talked about in the video but rather it lives on a spectrum from not fat tail to very fat tail looking ahead to the next video of the series I'm going to dive more deeply into text embeddings which was an essential part of the rag system so I'll talk in Greater detail about text embeddings and discuss two major use cases namely semantic search and text classification if you enjoyed this video and you want to learn more check out the blog in towards data science and even though this is a member only story you can access it completely for free using the friend Link in the description below and as always thank you so much for your time and thanks for watching
XpoKB3usmKc,2024-02-27T22:52:38.000000,QLoRA—How to Fine-tune an LLM on a Single GPU (w/ Python Code),"fine-tuning is when we take an existing model and tweak it for a particular use case although this is a simple idea applying it to large language models isn't always straightforward the key challenge is that large language models are very computationally expensive which means fine-tuning them in a standard way is not something you can do on a typical computer or laptop in this video I'm going to talk about Cur which is a technique that makes fine-tuning large L language models much more accessible and if you're new here welcome I'm Shaw I make content about data science and Entrepreneurship and if you enjoy this video please consider subscribing that's a great no cost way you can support me in all the content that I make since I talk in depth about fine-tuning in a previous video of this series here I'll just give a highlevel recap of the basic idea so like I said before fine tuning is tweaking an existing model for a particular use case so an analogy for this fine tuning is is like taking a raw diamond and refining it and Distilling it into something more practical and usable like a diamond you might put on a diamond ring in this analogy the raw diamond is your base model so this would be something like gpt3 while the final Diamond You Come Away with is your fine-tuned model which is something like chat GPT and so again the core problem with fine-tuning large language models is that they are computationally expensive to get a sense of this let's say you have a pretty powerful laptop and it comes with a CPU and a GPU where the CPU has 16 GB of RAM and your GPU has 16 GB of RAM let's say we want to finetune a 10 billion parameter model each of these parameters corresponds to a number which we need to represent on our machine standard way of doing this is using the fp16 number format which requires about two bytes of memory per parameter so just doing some simple math here 10 billion parameters time 2 bytes per parameter comes to 20 GB of memory just to store the model parameters so one problem here is that this 20 GB model won't fit on the CPU or GPU but maybe we can get clever in how we distribute the memory so the load of the model is split between the CPU and GPU and that allows us to do things like inference and make predictions with the model however when we talk about fine-tuning we're talking about retraining the model par parameters which is going to require more than just storing the parameters of the model another thing we need are the gradients these are numbers that we use to update the model parameters in the training process we'll have a gradient which is just going to be a number for every parameter in the model so this adds another 20 GB of memory so we've went from 20 to 40 and now even if we get super clever with how we distribute it across our CPU and GPU GPU it's still not going to fit so we'd actually need to add another GPU to even make that work but of course this isn't the whole story you also need room for the optimizer States so if you're using an Optimizer like atom which is very widely used this is going to take the bulk of the memory footprint for model training where this is coming from is an Optimizer like atom is going to store a momentum value and variance value for each parameter in your model so we'll have two numbers per parameter additional these values need to be encoded with higher Precision so instead of the fp16 format these are going to be encoded in the fp32 format and so when it's all said and done there's about a 12x multiplier for the memory footprint of these Optimizer states which means we're going to need a lot more gpus to actually fine-tune this model these calculations are based on reference number two which is a paper about zero which is a method for efficiently fine-tuning these deep neural networks works so we come to a grand total of 160 GB of memory required to train a 10 billion parameter model of course these enormous memory requirements aren't going to fit on your laptop and it's going to require some heavyduty Hardware to run so 160 GB if you get like a 80 gb GPU like the a100 you'll need two of those at least and those are about $20,000 a pop so you're probably talking about like $50,000 just for the hardware to fine-tune a 10 billion parameter model in the standard way this is where Cur comes in so Cura is a technique that makes this whole fine-tuning process much more efficient so much so that you can just run it on your laptop here without the need for all these extra gpus before diving into Cur a key concept that we need to understand is quantization and even though quantization might sound like this scary and sophisticated word it's actually a very simple idea whenever you hear quantization just think splitting a range of numbers into buckets so as an example let's consider any number between 0 and 100 obviously there are infinite numbers that can fit in this range you know there's like 27 55.3 83.7 823 and so on and so forth what quantization consists of is taking this infinite range of numbers and splitting it into discrete bins one way of doing this is quantizing this infinite range using whole numbers so what that would look like for our three numbers here is that 27 would go into this 27th bucket 55.3 would go into this 55 bucket and then 83.78% would go to 20 55 would go to 50 and 83 would go to 80 so that's the basic idea and the reason this is important is that quantization is required whenever you want to represent numbers in a computer and the reason is that if you wanted to encode a single number that lives in an infinite range of possibilities this will require infinite btes of memory it just can't be done at some point when you're talking about a physically constrained system like a computer you have to make some approximations and so if we go from this infinite range to this range quantized by whole numbers this would require about 0.875 bytes per number and then if we go one step further and just split it into these 10 different buckets it would require about half a bite per number one thing to point out here is that there's a natural tradeoff you know we could have a lot of buckets which would give us a lot of precision but it's going to increase the memory footprint of our model however you could have very few buckets for quantization which would minimize the memory footprint but this would be a pretty crude approximation of the model you're working with so balancing this tradeoff is a key contribution of Q Laura there are actually Four ingredients that come together to make up Q Laura the first is 4bit normal float the second is double quantization the third are paged optimizers and then finally is loraa I'm going to talk through each of these ingredients one by one starting with ingredient one one 4bit normal float all this is is a better way to bucket numbers it's a better way to do quantization so let's break it down when we say something is 4 bit what we mean is we're using four binary digits to represent that piece of information and since each digit can be either zero or one this gives us 16 unique combinations which means with a 4bit representation we have 16 buckets at our disposal for quantization compressing a range of numbers into just 16 buckets is great for memory saving you know we only have four bits which translates to half a bite per parameter so if we have 10 billion parameters that's going to translate to 5 GB of memory but of course this brings up the same problem I mentioned earlier which is we have this tradeoff it's like yeah we get huge memory savings but now we have a very crude approximation of the number we're trying to represent the way ingredient one this 4bit normal float works is it buckets the numbers in a particular and clever way suppose we have all the parameters in our model and we plot their distribution when it comes to these deep neural networks it turns out that most of the parameter values are going to be around zero and very few values are going to be much smaller and much larger than zero what that means is we have something that resembles a normal distribution when it comes to our model parameters so if we follow a quantization strategy that I talked about a couple slides ago where we just split the numbers into these equally spaced buckets we're going to get a pretty crude approximation of these model parameters because most of our numbers are just going to be sitting in these two buckets here with very few numbers sitting in these end buckets here an alternative way we can do quantization is instead of using equally spaced buckets we can consider using equally sized buckets so instead of mapping each parameter into these eight buckets we map these parameter values into these eight buckets so now you can see that we have a much more even distribution of model parameters across these buckets and this is exactly the idea that 4-bit normal float uses to balance that tradeoff between low memory and accurately representing model parameters so the next ingredient is double quantization which consists of quantizing the quantization constants I know the word quantize is appearing way more than anyone would ever like on this slide but let's break it down step by step to see what this all means so consider this simple quantization strategy so let's say we have this array of numbers X that's represented using 32 bits and we want to translate it into an 8bit representation on the left hand side here and then we want this 8bit representation to have values in between minus 127 and 127 essentially what we're doing is we're quantizing by whole numbers forcing it to live in this range ofus 127 to 127 so that's what we're trying to do so a simple way of doing that is we rescale all the values in this array by the absolute maximum value in the array and then we'll multiply it by the new maximum value which is 127 in our quantized range and then we'll round it just so that there are no decimal points so this is a very simple way we can quantize this arbitrary array encoded in 32bit into a 8bit integer representation and just to make this more simple we can translate this prefactor here into a constant encoded in 32bit so while this simple quantization strategy isn't how we do it in practice cuz again if we're doing the equally sized buckets it's not just going to be this linear transformation that we're seeing here but this does illustrate the point that anytime you do Quant ization there's going to be some memory overhead involved in that computation so in other words these constants are going to take up precious memory in your system so as an initial strategy you might think well if we have this input tensor or input array and we rescale all the parameters we're only going to have one new constant a 32bit number for all the parameters in our model what's the big deal about that what's another number compared to 10 billion parameters for example so while this does have trivial memory implications it may not be the best way to quantize our model parameters because this is going to be very sensitive to extreme values in our input tensor and the reason is if we're talking about these model parameters where most of them are close to zero but then you have this one parameter way far off in the tails that is your absolute Max it's going to introduce a lot of bias in your quantization process so this standard quantization approach does minimize memory but it comes with maximum potential for bias an alternative strategy could be as follows where we take the input tensor we reshape it to look like this and then we split this tensor into buckets and then within each bucket we do the rescaling process so this significantly reduces the odds of one extreme value skewing all the model parameters in the quantization process this is called blockwise quantization and although it comes with with a greater memory footprint it has a lot less bias so to mitigate the memory cost of this blockwise quantization approach we can employ double quantization which will do this quantization process here but then we'll do the quantization process once again on all these constants that pop up from this blockwise approach so if we just kind of repeat this very simple strategy here now we have an array of constants we have multiple constants popping out they're encoded in 32bit and then we can quantize them into a lower bit format using this simple approach that's double quantization so we are indeed quantizing the quantization constants while it might be an unfortunate name it is a pretty straightforward process so ingredient three is a paged Optimizer all we're doing here is looping in your CPU into the training process so let's say we have a small model like 51 which has 1.3 billion parameters which which based on those same calculations we saw earlier would require about 21 GB of memory for full fine-tuning the dilemma here is that although we have enough memory across the GPU and CPU for all 21 GB needed to fully fine-tune 51 this isn't something that necessarily just works out of the box these are independent modules on your machine and typically the training process will just be restricted to your GPU and so this paged Optimizer what that means is instead of just restricting training to only fitting on your GPU you can move memory as needed from the GPU to the CPU and then bring it back onto the GPU as needed what that might look like is you'll start model training and you'll have one page of memory and a page of memory is like a fundamental unit or block of memory on the GPU or CPU the pages will start accumulating during the training process until your memory gets full and then at which point if you have this paged Optimizer approach you can start moving pages of memory over to the CPU to make room for new memory for training and then if you need a page of memory that was moved to the CPU back onto the GPU you can make room for it there and then you can just move it back over using this paged Optimizer this is the basic idea honestly I don't know exactly how this all works I'm not like a hardware guy I don't know how computer architecture fully works but this is like my highlevel understanding as a data scientist so if you want to learn more check out the cura paper where they talk a little bit more about it and provide some additional references the final ingredient of cura is loraa which stands for low rank adaptation and so I actually talked about Lura in depth in a previous video on fine-tuning so here I'm just going to give a brief highlevel description of how it works if you want more details you can check out that previous video or check out the low R paper Linked In the description below what Laura does is that it fine-tunes a model by adding a small number of trainable parameters so we can see how this works by contrasting it with the standard full fine-tuning approach so let's say this is our model here this is our neural network and we have this input layer we have some hidden layer and then we have the output layer here full fine tuning consists of retraining every single parameter in this model we're just considering one layer at a time we'll have this weight Matrix corresponding to all these lines in this picture here we'll have this Matrix W KN consisting of all the parameters for that particular layer and all of these are trainable while that's probably not going to be a big deal about these six parameters in this shallow Network here if you have a large language model these matrices will get pretty big and you'll have a lot of them because you'll have a lot of layers Lura on the other hand instead of fine-tuning every every single parameter in your model it'll actually freeze every parameter in the model and it works by adding a small set of trainable parameters which you'll then fine-tune the way this works is you'll have your same hidden layer and then you'll add a small set of trainable parameters through this Delta W Matrix so if you're looking at this you might think well how does this help us because Delta W is going to be the same size as W KN so how is this adding a smaller set of trainable parameters and so the trick with Laura is that this Delta W will actually be the product of two smaller matrices b and a which have the appropriate Dimensions to make all the math work out so visually what that looks like is you have your W KN here but then you have BNA a which have far fewer parameters than W KN but when you multiply it together their product it'll have the proper shape to make all the Matrix operations work here so you'll actually freeze W KN so you won't train these parameters and then these parameters housed in BNA will be the trainable ones the result of training the model this way is that you can get 100 to even 1,000x savings and model parameters so instead of having to train 10 billion parameters you're only having to train like 100 million parameters or 50 million parameters so let's bring these four ingredients together let's first look at the standard fine tuning approach as a baseline so here let's say we have our base model represented in fp16 so we'll have this memory footprint from the base model and then we'll have this larger memory footprint from the optimizer States and then we won't have any adapters because adapters only come in when doing lowra or another parameter efficient fine-tuning method and so we'll do like the forward pass on the model it'll go to the optimizer and then the optimizer will do the backward pass and will update the model parameters this is the same standard fine-tuning approach we talked about earlier so a 10 billion parameter model will require about 160 gbes of memory another thing we could do is use lowra so we can get that 100 to 1,000x Savings in the number of trainable parameters we still have our model represented in 16bit but now instead of fine-tuning every single parameter in the model we only have a small number of trainable parameters and then each of those parameters will have an Associated op Optimizer state which significantly reduces the memory footprint so that a 10 billion parameter model would only require about 40 GB of memory while this is a tremendous savings like a 4X Savings in memory 40 GB is still a lot to ask for from consumer Hardware so let's see how Cur helps the situation even further the key thing here is that instead of using the 16bit representation we can use ingredient one and encode the base model as 4bit normal float and then we'll have the same number of trainable parameters from Lura so that'll be exactly the same and then we can use ingredient 3 with the paged optimizers to avoid any out of memory errors that might come up during the training process with that and including the double quantization here we can use Cur to fine-tune a 10 billion parameter model with just about 12 gigabyt of memory which is something that can easily fit in consumer Hardware and can even run using the free resources available on a Google collab so let's see a concrete example of that here we're going to do fine-tuning using mistol 7B instruct to respond to YouTube comments this example is available on the Google collab associated with this video the model and data set are freely available on hugging face and then additionally there is a GitHub repo that has all the resources put together as well as the code to generate the training data set here first thing we need to do is import some libraries everything here is coming from hugging face their Transformers Library their PFT Library which is parameter efficient fine-tuning this is what's going to allow us to do Q lur and then we're using hugging fa's data sets library because I uploaded the training data set onto hugging faes Hub and then finally we just import the Transformers Library these are kind of like sub dependencies to ensure some of these modules work I think it's mainly this one prepare mod for kbit training you don't need to import these but you need to make sure they're installed in your environment and this was actually a paino because bits and bytes only works on Linux and windows and on Nvidia hardware and then gptq this format for encoding models it doesn't run on Mac so as a Mac User this was kind of frustrating lots of trial and error to try to get it to work on my machine locally but I wasn't able to get it to work so if anyone was able to get it to run on a M1 or a M2 or or even M3 send me your example code or send me any resources you found helpful I would love to get a version working on my personal machine but since collab they have a Linux environment using Nvidia Hardware the code here works fine next we can load the quantized model and so here we're going to grab a version of mistol 7B instruct from the bloke and so if you're not familiar with the bloke he's actually quantized and shared thousands of these large language models completely for free on the hugging face Hub and then we can just import this model using this from pre-trained method so we just need to specify the model name on The Hub device map set to Auto just has the Transformers Library kind of figure out the optimal way to spread the load between the GPU and CPU to load in the model trust remote code basically it's not going to allow like a custom model file to run on your local machine so this is just a way to protect your machine when downloading code from The Hub and then revision main is just saying we want the main version of the model available at this repo here then again gptq which is the format used here does not run on Mac there are some other options with Mac but I wasn't able to get it working on my machine once we have the quantized model loaded we can load the tokenizer so we can do this actually pretty easily using this from pre-train method so we just specify the model name and then specify this use fast argument as true with just those two simple blocks of code we can use the base model one thing we do here is we put the model into evaluation mode which apparently deactivates the Dropout modules next we can craft our prompt so let's say we have a command from YouTube that says great content thank you and then we put it into the proper prompt format so mistal 7B instruct is an instruction tuned model so it's actually Expecting The Prompt in a very particular format and namely it's just expecting this instruction start and instruction end special tokens in the prompt so we set that up very easily what this is doing is it's just going to dynamically take this comment variable and stick it into this prompt here and then once we have that we can pass the prompt to the tokenizer so basically we're taking this prompt and We're translating it from a string into an array of numbers and then we can take that array of numbers and we can pass it into our model to generate more text once we do that we can get the outputs and then pass them back into our tokenizer and have the tokenizer decode the vector back into English the output of this great content thank you comment is I'm glad you found the content helpful if you have any specific questions or topics you'd like me to cover in the future feel free to ask I'm here to help in the meantime I'd be happy to answer any questions you might have about the content I've already provided just just let me know which article or blog post you're referring to and I'll do my best to provide you with accurate and up-to-date information thanks for reading and I look forward to helping you with any questions you may have so while this is a fine response there are a few issues with it one it's very long I would never respond to a YouTube comment like this second is it kind of just like repeats itself it's like glad you found it helpful feel free to ask and then it says happy to answer questions that you have happy to provide you with acurate update information and like look forward to helping you with questions so saying the same thing in different words like a few different times and then finally it says thanks for reading and if this is for YouTube comments people aren't reading this stuff they're watching videos so one thing we can do to improve model performance is by doing so-called prompt engineering I actually have a in-depth guide on prompt engineering where I talk about seven tricks to kind of improve your prompts in a previous video of the series so feel free to check that out if you're interested The Prompt that I ended up using here is something that I generated through trial and error and the way I did that is using a website called together. which I can link in the description below essentially together. a they have a chat interface kind of like chat GPT but for a lot of open- source models including mistl 7B instruct version 0.2 so I was able to test a lot of prompt ideas and get feedback and just kind of eyeball which gave the best performance and I ended up using that one so I have this set of instructions here sha gbt functioning as a virtual data science consultant on YouTube communicates in clear accessible language escalating to technical depth upon request it reacts to feedback aply and ends responses with its signatur sha GPT sha gbt will tailor the length of its responses to match the viewers comment providing concise acknowledgements to brief expressions of gratitude or feedback thus keeping the interaction natural and engaging then I have this instruction please respond to the following comment and then I have this Lambda function where given a comment I'll piece together this instruction string and comment together within the instruction special tokens that the model is expecting with that I can just pass the comment into the prompt template and generate a new prompt what that looks like is this so you see we have the instruction special tokens you see it's well formatted this is the instructions please respond to the following comment and says great comment thank you Now using this new prompt instead of just passing the comment directly to the model we have this set of instructions with the comment this is the response thank you for your kind words I'm glad you found the content helpful sha GPT so this is really good this is actually already pretty close to how I typically respond to YouTube comments and a lot of them tend to be something like this and it appropriately signed off as Sha GPT so people know that it came from an AI and not from me personally well maybe we could just call it here it's like okay this is good enough let's just start using this as the comment responder let's see how we can use Q Laura to improve this model even further using fine tuning so the way to do that is we need to prepare the model for training so we'll put it from eval mode into training mode we're going to enable gradient checkpointing which isn't something I talked about and it's not necessarily part of the qora technique because it's actually pretty standard it's just a memory saving technique that clears specific activations and then recomputes them during the backward path of the model and then we need to enable quantized training the base model is going to be in 4 bit and we're going to freeze them but we still want to do training in higher Precision with Lowa we need to make sure we enable this quantize training option next we want to set up lowra so we can use that using this low ra config file I talk more about low RA in the fine-tuning video so just briefly going through this we're going to set the rank as 8 set the alpha s32 we're going to Target the query modules in the model we're going to set drop out to 0.5 we're not going to have any bias values and then we're going to set the task as causal language modeling with the config file we can pass the model and the config into this method get PFT model so this will just create a lowr trainable version of the model and then we can print the number of trainable parameters so doing that we see that we actually have a significant saving so less than 1% of the original number of trainable parameters just one point of confusion for me personally is it's showing that mistol 7B instruct has 264 million parameters here based on the quick research I did seemed like when you do quantization there could be some terms that you can drop but honestly I don't fully understand why we went from 7 billion parameters to just 264 million parameters so if anyone knows that please drop it in the comments I'm very curious but the main point here is that we're only using 0.8% of the original number of train parameters so huge memory savings using lowon next we're going to load the data set which is freely available on the hugging face Hub it's called shot GPT YouTube comments also the code to generate this data set is available at the GitHub repo if you're curious on how to do the formatting and stuff and then here's an example from this data set you'll see we have the special token the start string and the end string we have the start instruction and end instruction and then we have the same set of instructions as before and then we have the comment here which is a real comment from the YouTube channel then after the instruction string we have the actual response I left to this comment and then I just appended this Shaw GPT sign off so the model learns the appropriate format in style that it should respond to we've got a data set of 59 of these examples so not a huge data set at all and then next we need to pre-process the text so this is very similar to how I did it in the previous find tuning video basically we Define this tokenized function which if the example is too long so if it's longer than 52 tokens it's going to truncate it so it's not more than this max length and then we'll return it as numpy values and then we can apply this tokenized function to every single example in the data set using this method here the map method where we have our data set and then we just pass in the tokenized function and set batched equal to true so it doesn't batches I guess instead of doing it one by one the other thing we need to do is this handles if the examples are too long but when you're training the model each example in a batch they actually need to be the same size so you can actually do matrix multiplication so for that we can create a data cator what that does is if you have multiple examples of different lengths so let's say you have like four examples in a batch and they're all of different lengths the data cator will dynamically pad each example example so they have the same length for that we need to define a pad token which I set as the end of string token and then I create the data collator using this method here and then I think this is masked language modeling set equal to false and that's because we're doing so-called causal language modeling not masked language modeling now we're ready to start setting up the training process so here we're setting hyperparameters we have the learning rate batch size number of epoch we're setting the output directory of the model the learning rate the batch size goes here number epochs goes here weight Decay we set it as 0.01 for logging evaluation and save strategy we set it to every Epoch that means every Epoch will print the training loss we'll evaluate at every Epoch we'll also print the validation data set loss and then save strategy so we'll save the model every Epoch in case something goes wrong we're going to load the best model at the end because maybe the best model was actually at the eighth Epoch and it got worse on the ninth Epoch or something like that gradient accumulation is equal to four warm-up steps equal to two so I actually talk a lot about gradient accumulation and weight decay in the previous video on training a large language model from scratch so if you're curious about what's going on there you can check out those videos next we'll set fp16 equal to true so here we're going to use 16bit values for training and then we'll enable the paged Optimizer by setting this optim equal to paged atom W 8bit so this is ingredient three from before lots of hyper parameters and of course you can spend your whole life tuning and tweaking this but once we have that we can run the training job so we initialize our trainer we give it the model give it our training data set our validation data set training arguments we defined on the previous slide and then the data collator we're going to silence warnings this is what I saw on an example from hugging face when they were introducing bits and bites so I just did it again here and then we can run the training process this took about 10 minutes to run on Google collab so it's actually pretty quick and this is this is what will get printed the training loss and validation loss so we can see a smooth monotonic decrease of both implying stable training which is good and then once it's all said and done we have our model and we can use it so if we pass in that same test comment great content thank you we get the response glad you enjoyed it shot GPT and then it even adds this disclaimer that note I am an AI language model I don't have the ability to feel emotions or watch videos I'm here to answer questions and provide explanations so this is good I feel like this is exactly how I would respond to this comment if I wanted to remove the disclaimer I could easily do that with some like string manipulation just keeping all the text before the sign off or something like that but the point is that the fine-tuning process at least from this one example seemed to work pretty nicely let's try a different comment something more technical like what is fat tailedness the response of the model is actually similar to what we saw in the previous video when we fine-tuned open AI model and then we asked it the same question where where it gives a good concise explanation of fat tailedness the only issue is it doesn't explain fat tailedness the same way that I explained it in my video series on the topic so this brings up one of the limitations of fine-tuning which is that it's great for capturing style but it's not always an optimal way to incorporate specialized knowledge into model responses which brings us to what's next instead of trying to give the model even more examples trying to include this specialized knowledge a simpler approach is that we can improve the model's responses to these types of technical questions by providing it specialized domain knowledge the way we can do that is using a so-called rag system which stands for retrieval augmented generation right now we just get the comment and we pass it into the model with the appropriate prompt and it spits out a response the difference with a rag system is that we take the comment we use the comment to extract ra relevant information from a knowledge base and then we incorporate that into the prompt that we pass into the model so that it can generate a response so that's going to be the focus of the next video in this series we're going to see how we can improve shot GPT using specialized knowledge coming from my medium blog articles and speaking of medium blog articles if you enjoy this video but you want to learn more check out the article published in towards data science on Cur there I cover details that I might have missed in this video here and even though this is a member only story you can access it completely for free using the friend Link in the description below other things I'll point out is that the code example is available for free on collab there's more code available on the GitHub and then again the model and the data set are available on hugging face and as always thank you so much for your time and thanks for watching"
qPrVqTIkobg,2024-02-27T00:59:53.000000,Difference Between #AI Chatbots and Assistants,n/a
LqOJCPonUQU,2024-02-19T14:51:09.000000,The Best Way to Think About Goals #goalsetting,goals are essentially an excuse for us to grow this is actually something that you know I first heard from Ray Doo who has a book called like principles I think that's what it's called oh he said that I don't think it's a new idea I think people have known for a while that the point of a goal isn't necessarily the goal if you've watched the David Beckham documentary and there was something that he said in there I don't know what episode but he was like the game of football is just an excuse for them to keep playing yeah and I was like whoa okay something clicked it has nothing to do with the outcome it has nothing to do with the output it's about the process right levels I think should be more as like a bonus than the the focus and that's so different than how I feel like how we learned about goals yeah you need to get to the goal and that's the point and the person that you have to become yeah in the process that's the real reward take the focus off the goal put it on the person that you want to be the dream life that you want to have
r5qk3uIdkks,2024-02-05T14:59:54.000000,What is #ai? — Simply Explained,n/a
4RAvJt3fWoI,2024-02-05T00:44:32.000000,"3 Ways to Make a Custom AI Assistant | RAG, Tools, & Fine-tuning",n/a
ytmK_ErTWss,2024-01-29T14:53:40.000000,LLMs EXPLAINED in 60 seconds #ai,I'm going to explain large language models in 60 seconds if you've heard of chat GPT then you've heard of large language models or llms for short while this might make you think that llms are just chat Bots that's not necessarily the case an llm is essentially a word predictor meaning given a sequence of words it produces the most likely next word this is just like the autocomplete function on your smartphone the way it gets good at this is by reading trillions of words from the internet and learning which word should come next what makes llms different than pre-existing Technologies like autocomplete is that they can take this ability to Simply predict the next word and generate humanlike and helpful responses to prompts one can then take an llm tweak it a bit and turn it into a powerful chat bot like chat GPT
mtu_v335bQo,2024-01-22T15:06:15.000000,3 Lessons from AI & Data Consultants #freelancing,recently sat down with 10 Ai and data science consultants and asked them how they get clients here are three key takeaways from those conversations first don't skip the discovery phase this avoids rushing into building something and inadvertently solving the wrong problem second is to always ask why why is this important to your business why do this now why me this simultaneously uncovers context for the project and the core problem the client is trying to solve third and finally is to find your lead Source while there are many ways you can get clients like referrals outbound marketing freelancing platforms content creation everyone that I talk to runs their business through one single lead source
8z-WPpP1_-8,2024-01-20T00:48:38.000000,AI for Business: A (non-technical) introduction,these days it seems like everyone is talking about AI with new Innovations seemingly coming out every single week however if you're a professional an entrepreneur or business operator you might be thinking to yourself what is this AI thing and how can I use it to drive value in my business so in this video I'm going to share a non-technical introduction to Ai and machine learning and share how it can fit into how we do business and if you're new here welcome I'm Shaw I'm a data scientist turned entrepreneur and if you enjoy this content please consider subscribing as a great noost way you can support me in all the videos that I make so in this video I'm going to be talking about two main things in part one I'm going to be answering the question what is AI and along the way defining some key terms such as artificial intelligence which is what AI stands for we'll be talking about models and why those are important for AI and then finally we'll be talking about machine learning so once we have a good understanding of what all these terms mean we can turn to part two and figure figure out how we can actually use these Technologies I'll give a concrete example of what AI might look like in practice then I'll share five rules of thumb that I like to use when thinking about how and when to use AI in practice starting from the top what is AI so when you hear the term AI you might think chat GPT or AI generated art or you might think of the Terminator or something similar but if we just take a step back AI stands for artificial intelligence so we've got two words here and one of them is a bit more problematic than the other the first word artificial is not the issue artificial simply means something that is made by humans however the second term intelligence isn't so well defined and even today there's not really a consensus of what this word actually means however a definition that I like to use and one that I think is relevant in a business context is intelligence is the ability to solve problems and make decisions based on this operational definition AI or artificial intelligence is simply a computer's ability to solve problems and make decisions and so to get a better sense of what we mean by intelligence let's see it in action so let's say we wake up Saturday morning and we're trying to figure out what we want to do today and we look out the window and see this so if we see this and we're trying to decide between a pool day or a Netflix day I think most people would pick the Netflix day because the dark clouds in the sky is probably a good indication that the weather's not going to be so great another example is if we see this sales data with this peak in November and someone asks us what caused the peak we might reasonably say that oh was probably because of Black Friday which is one of the biggest retail days of the entire year and then finally if we see this text exchange of someone saying fine do what you want the other person responding are you okay and then that original person saying yeah whatever and if we ask are they really fine most people probably say no know even though the person is saying that they're fine their choice of words like fine do what you want and their use of whatever is probably indicating that they are actually not fine each of these scenarios is a situation where we used our intelligence to make a decision or to solve a problem so even though each of these examples is very different there's a Common Thread that runs through each of them which is intelligence requires knowing how the world works but of course the world is a mass place and it's very complicated so the way we make sense of this huge and complicated world is through models and a model is simply a approximation of a real world thing that can fit into our heads and more specifically models allow us to make predictions for example when we saw the dark cloudy sky that information went into our mental model of the world and allowed us to make the prediction that it's probably going to rain later however models aren't only restricted to the ones that we have in our heads we can also have computer models and in fact essentially all weather forecasts are done by computer models instead of your weatherman standing outside for 5 minutes and making a forast for the day so models be they mental models or computer models are an essential part of intelligence but a natural question here is where do these models come from so there are two types of models that I'm going to talk about the first I'll call principle driven models which are based on a set of rules these are things you might read in a book or learn from your grandma the other kind of model is based on past examples a principle driven model would say that if we see dark clouds in the sky then it's probably going to rain later while a datadriven model would say the sky is similar to other times when it rained and so each of these models comes to the same conclusion that it's going to rain but they are built on top of a different different Foundation but of course each of these models isn't restricted to something we hold up in our heads but these are things we can program into computers so principle driven models we can explicitly program computers to execute using standard programming techniques but more recently we've seen the rise of datadriven techniques to derive models the most popular of which is called machine learning machine learning is potentially another one of those buzzwords you may have heard around but it's a really simple concept machine learning is just a computer's ability to learn by example so the way this works is we have a set of training data which consists of predictors and targets where targets are the things that we're trying to predict like if it's going to rain or not and predictors are all the information that we're going to use in order to estimate the target the key Point here is instead of explicitly telling the computer how to take predictor to estimate the target machine learning allows the computer to figure out the relationship between predictors and targets simply by seeing many different examples of the two so the way that works is we pass this training data into a machine learning algorithm and out poops our machine learning model with this machine learning model in hand what we can do is get new data pass it into the model and obtain a prediction which is exactly what we did when we saw the dark cloudy Sky we looked out the window we received some information and we were able to make a prediction that it's going to rain later and a machine learning model Works in exactly the same way so up until this point we've talked about three things we talked about artificial intelligence which we Define simply as a computer's ability to solve problems and make decisions we also talked about models which were a essential part of intelligence because they allow us to understand how the world works works and then finally here we talked about machine learning which is a way a computer can generate a model based on past examples with these three terms defined we can move on to the second part of the talk which is how do we use these things how do we use these Technologies like Ai and machine learning to drive value in our businesses so I'll start with a concrete example and talk about credit decisioning which is something I have some real world experience with so I can talk about it some somewhat intelligibly so when we're talking about credit decisioning what we're talking about is people applying for a loan and financial service providers evaluating that application and making a decision of whether to approve the loan or deny the loan so the way that works is someone submits an application for a loan and the financial services company makes a decision of whether to approve deny in the terms of the agreement so the traditional way of doing this is that the application goes to an underwriter which is a person who makes the decision and defines the terms of the contract however now that we've learned about Ai and machine learning we might think oh we can just replace the human underwriter with an AI underwriter right and the answer to this question is yes and no while it might be easy to imagine replacing a human job with an AI the reality is a bit more complicated so what this looks like in practice is something like this with all these steps within the blackbox being our AI underwriter and really what it is is not just a machine learning model but rather a large number of business rules data and it processes all working together to take the application and finally make a credit granting decision so although machine learning is a critical part of this AI underwriter here it is only a component in a much broader solution and so this is often the reality of what AI looks like in practice although from an outside view it might look as simple as we have an AI underwriter in reality what's going on under the hood is a bit more complicated which is an important point to keep in mind when trying to implement AI Solutions in your business so while this might be an illustrative example it may not give us a good idea of how and when to use AI to solve business problems so for that I'm going to talk about five rules of thumb that I like to use when thinking about how and when to use AI in practice so the first one is to focus on problems not Technologies next is to apply AI to problems that you solve repeatedly next we have look for problems in which a 70% solution is good enough to generate value the fourth is pick situations in which one failure of your AI solution doesn't erase nine successes and then finally start simple fast and easy and build sophistication as needed so I'm going to talk through each of these rules of THB one by one and share concrete examples of each first focus on problems not Technologies so this brings up what in data science we might call the hammer problem which is when you have a really nice Hammer everything looks like a nail so let's say you have some problem in your business something is broken if you take a technology first approach you might grab your hammer and say I got this which obviously is not going to solve the problem and is probably going to make things a lot worse so an example of this from my personal experience is something I saw over and over again which was last year I had a lot of clients reaching out to me asking for help in building a custom chatbot or fine-tuning a chatbot for a particular use case and this was a classic example of the hammer problem because often what had happened was the client had seen the power of chat GPT and saw all the incredible innovations that have been happening in the space of natural language processing and large language models and was probably thinking something like I need one of these for my business however the Trap that you fall into with the technology first approach is that you can spend a lot of time and money building a solution for a problem that isn't very critical to your business and essentially this time and money is wasted however let's flip things around instead of starting with the technology what was starting with the problem look like so let's say we have a problem where our customer support line is overwhelmed well from here instead of jumping into building something you jump into problem solving because when you have a tool your instinct is to build but when you have a problem your instinct is to solve the problem so you might ask why are people calling if people are calling for some specific piece of information you can update your FAQs and if that doesn't cut it you can improve call routing to make sure that callers are getting sent to the right person and there isn't time wasted where customer support representatives are on the phone with someone just to transfer them to someone else and then maybe after exploring a few Solutions then you start thinking about building a chatot for your website but the key Point here is that when you start with a problem you don't jump to building a solution you jump to finding the root cause of the problem so you can find the best solution and ultimately When comparing these two approaches the technology first approach approach on the left and the problem first approach on the right you almost always want to go with the problem first approach because that is almost guaranteed to generate value in your business while the technology first approach might be intellectually stimulating and exciting is often something that doesn't drive any real value the next rule of thumb is to apply AI to problems you solve repeatedly and the reasoning behind this is that AI is just the continuation of the story of Technology since the beginning of time it's simply a tool to help make our lives easier so the problems that you're solving over and over again are great candidates to apply AI to for a few reason one if you can automate it with AI you no longer have to spend a lot of your time solving that problem or even if you reduce the amount of effort it takes you to solve that problem by some marginal amount it can still translate to some big gains other reasons are if you're solving a problem repeatedly you likely have a deep understanding of that problem which puts you in a good position to build good solutions to solve it and finally if you're already solving a problem that means you have an existing solution which is a fantastic starting place for building an AI solution so an example of this is something that I use in my own work which is a literature review assistant so I read a lot of papers about Ai and machine learning for both my content and my Consulting business and often when reading research articles I discover gaps in my my understanding and so this is a problem that I face over and over again I'm reading the paper and then I stumble across a sentence that seems obvious to the authors but is completely not obvious to me so for that I will turn to chat GPT I'll upload the PDF of the paper ask chat gbt what the paper is about then ask chat gbt specific questions until I have a clear understanding of what's going on so using Chad gbt in this way has significantly sped up how quickly I can read articles because now instead of spending a 30 minute tangent on Google trying to figure out what a particular term means or putting an idea in a larger context Chach PD does a pretty good job of explaining things and adding additional context where needed the next rule of thumb is find situations where the 70% solution is good enough where this is coming from is a model is simply an approximation of the real world thing no model is ever going to be perfect and there's a famous quote from statis George box which goes all models are wrong some are useful so the key thing is to accept that your model is not going to be perfect but pick the ones that are actually useful to you in whatever problem you're trying to solve and so a good example of this is Spam filtering the way that works is you have a bunch of spam emails flooding your inbox in this situation even an imperfect model is very valuable because even a 70% reduction in spam emails is very helpful that'll give a thumbs up for from any user another important rule of thumb is ensure that one failure doesn't erase nine successes and essentially what we're talking about here is find the low stakes or low exposure situations an example of this might be using chat gbt as a writing assistant is pretty low stakes if it gives nine good recommendations followed by one bad recommendation for writing it's no big deal you can just ignore that recommendation and move on with your life however if you're using using Chachi BT to make cancer diagnosis it doesn't matter if it's right nine out of 10 times that one time when it's wrong can have a tremendous negative impact so that is a situation where you probably don't want to use Ai and if you do you have to be very thoughtful about how it's implemented and then the final rule of thumb is to start simple fast and easy and each of these words simple fast and easy is important so starting simple is important because sophisticated Solutions are fragile and costly they'll cost you a lot of money to build and they have a high likelihood of failure because they are well complicated next you want to build fast because to build good Solutions you'll need to iterate so that means you'll need to try out a lot of different things and if it takes you a long time to do one iteration it's going to take you a long time before you implement a good solution and then finally you want to make it easy so you want the solution to kind of be on the way and not something way out of the the way for people because if it's hard to access no one's going to use it including you even if you're the one that's implementing the solution so let's look at a specific example let's say we're trying to implement a sales email sequence in our business the way this start simple fast and easy approach would play out is you'll start by writing all these emails by hand so what that looks like in my business is I'll have someone book a discovery call with me and I'll send them a follow-up email asking them a couple of follow-up questions based on their specific use case that's me doing the process by hand however after doing that for a bit I've naturally developed email templates for responding to someone booking a discovery call and like a post Discovery call email and then maybe another template for following up with people after 40 days or following up with people after 90 days and so on and so forth so over time instead of just writing emails by hand you start to develop templates and then over time those templates can get loaded into a CRM so you use a CRM tool to automatically fill in these emails with some bit of personalization like including people's names and maybe some other information that they provide but then you can take this one step further and use some kind of large language model or NLP solution to make the emails a bit more personalized so instead of just using a template and just filling in a name you can make the email sound more like a person so all that to say it's good to start here you know start by just doing things by hand and build toward that sophisticated solution often times when you're a small business you'll find that just doing it by hand or having some templates are more than suitable so for me I have a small Consulting business so I'm spending most of my time here I don't have a CRM but let's say you have like a 10p person business then you might want to be looking at a CRM then let's say you have a larger Enterprise and let's say you're working with hundreds or thousands of clients then maybe building the AI solution makes sense but the value in taking this simple fast and easy approach is that you don't artific officially just jump to the end you take it step by step and you only move on to the next level of sophistication if the value is there if it makes sense for your business we talked about a lot of different things so just to recap a few key terms we talked about Ai and how it's a computer's ability to solve problems and make decisions we also talked about models and how they help us make predictions and that they're a necessary part of any AI system and then finally we talked about machine learning which is a datadriven way computers can generate models from past examples as opposed to being programmed explicitly and then we talked about how we can use AI through five different rules of thumb focusing on problems not Technologies applying AI to problems you solve repeatedly seeking problems where the 70% solution is good enough identifying problems where one failure doesn't erase nine successes and taking this simple fast and easy approach to iteratively develop AI Solutions so while this was a pretty highlevel introduction I hope it gave you some clarity about what AI is and how you can start to use it in your business this is the first video in a larger series on how to use AI in business in future videos I'm going to dive into more the project management side of machine learning and model development so if you have any specific questions or anything specific you'd like to see in future videos of this series please drop those in the comment section below and as always thank you so much for your time and thanks for watching
jGn95KDWZMU,2024-01-11T22:12:29.000000,5 Questions Every Data Scientist Should Hardcode into Their Brain,data science is more than just building fancy machine learning models when you boil it down the key objective of data science is to solve problems the trouble however is at the outset of most data science projects we rarely have a well-defined problem in these situations the role of a data scientist isn't to have all the answers but rather to ask the right questions in this video I'll share five questions that every data scientist should hardcode into their brain to make ident identifying and defining business problems second nature and if you're new here I'm Shaw I make content about data science and Entrepreneurship and if you enjoyed this video please consider subscribing that's a great no cost way to support me in all the content that I make before diving into the questions I want to give some context for where they are coming from like many others when I started my data science journey I was hyperfocused on learning tools and Technologies while this technical Foundation is necessary to be a a successful data scientist focusing too much on tools creates the hammer problem which is when you have a really nice Hammer everything looks like a nail this often leads to projects that are intellectually stimulating yet practically useless I finally outgrew this approach when I joined a data science team at a large Enterprise the key lesson from that experience was the importance of focusing on problems rather than Technologies this means that one should gain a sufficiently deep understanding of the business problem before writing a single line of code and since as data scientists we don't typically solve our own problems we gain this understanding through conversations with stakeholders and clients getting this right is important because if you don't you can spend a lot of time and money solving the wrong problem this is where problem Discovery questions come in over the past 6 months I've developed a bit of an obsession with cracking these early stage discover y conversations with stakeholders and clients my approach to getting better at this has been twofold first I interviewed 10 seasoned data Freelancers about their best practices and how they approach these conversations and second I took as many Discovery calls as possible which ended up being around 25 the five questions I share here are the culmination of all these efforts while this is by no means a complete list these are questions that seem to come up over and over again so the first question here is what problem are you trying to solve while in theory this should be the only question you need to ask in practice things don't typically work out that way in most instances clients aren't super clear on the problem that they need to solve and even if they are I typically will need to do some catching up to better understand the business context either way this question is helpful because it ideally brings up follow-up questions which allow me to dig deeper into the client's world for example if a client says we tried creating a custom chapot with open AI but it didn't provide good results I might ask what was the chapot used for or what makes you say the results weren't good and a lot of times if a follow-up question doesn't come to mind I find a really helpful practice is just to rephrase and summarize what the client tells me most times this is another way to keep the conversation going and keep digging into the challenges that the client is facing a natural way to follow up the what question is why this is one of the most powerful questions you can ask a client because it can unlock the floodgates to the client's motivations goals assumptions and Beyond however why questions have a tendency to make people defensive which is why having multiple ways of phrasing this question can be helpful some examples of this are as follows why is this important to your business why do you want to solve this now what does solving this mean for your business how does this fit into the larger goals of your business why do you want to use AI to solve this problem the key benefit of asking the why question or any of its variants is that they allow you to dig more deeply into the client's problem and ultimately identify the root cause this is reminiscent of Toyota's five wise approach which teaches to get to the root cause of any problem one should ask why five times these first two questions of what are we doing and why are we doing it are two of the most fundamental questions in business so getting really good at asking what and why in many different ways can take you very far the next question is what's your dream outcome I like this question because it essentially combines the what and why questions and it tends to get people to speak to their vision of the project in a way that may not come through when asked directly having multiple ways of asking what and why is important because it often takes a few passes to really get to the root cause of a client's problem two related questions here are what does success look like and how would we measure it these are a bit more pragmatic than a dream outcome but are helpful for transitioning from asking what and why to how the next question is what have you tried so far this helps narrow down potential Solutions in two ways one it helps avoid wasting time on things that didn't work and two any new project should build upon existing work this latter point is based on the philosophy that data science projects should seek incremental Innovation therefore they should be simple and iterative for situations where the client hasn't built anything so far one can ask any of the following questions what is the existing solution how do you solve this problem now what have others done to solve a similar problem in either case these questions help set the stage for the project and help you avoid Reinventing the wheel the final question is one I got from Master negotiator Chris Voss which is why me asking this question is an effective way to reveal people's motivations for talking to you often this Sparks additional context of what led them to to you and how they see you fitting into the project which is helpful for next steps sometimes however people don't have good answers to this question which may indicate they don't actually want to work with you and they're holding back some deeper motive such as they're looking for free consulting or they're looking for a competing bid to take to the person they actually want to work with a key lesson for me these past 6 months was to learn these questions I.E hardcode them into my brain but then forget about them the point isn't to mindlessly go down a list of questions when talking to clients but rather get to the point where these questions naturally form in your mind during the flow of conversation this intuition is something that can only develop through practice toward that end here are three key takeaways that have been helpful to me in developing this skill set first don't just study these questions use them while this may result in a fair share of awkward moments it's all part of the learning process and don't worry I'm still learning to second is to stay curious the goal of these early stage conversations isn't to look smart or sell but rather to learn which brings up the final takeaway listen more than you talk my rule of thumb is to wait until the last 5 to 10 minutes of a 30 minute call to start offering recommendations and next steps prior to that my challenge is to ask questions rephrase and summarize client answers and to ask follow-up questions following my Natural Curiosity if you enjoyed this content please consider subscribing that's a great no cost way you can support me in all the videos that I make to read more about this topic check out the blog in TS data science which you can access using the friend Link in the description below like I said earlier this is by no means a complete list so if you have anything to add please drop those in the comments section below and as always thank you so much for your time and thanks for watching
scAxgeGadv4,2024-01-10T14:38:49.000000,2 Types of Data You Should Know #datascience,are two types of data that every data scientist should know about they are what we can call thin tailed data and fat tailed data thin tail data are gaussian like things they have the key property that no single observation will significantly impact the aggregate statistics of the data some examples of thin tail data include Heights weights and test scores fat tail data on the other hand are more parade like these data have the key property that a single observ can and often will significantly impact the aggregate statistics some examples of fat tail data include sales wealth Wars pandemics and so many other things that we care about so before you do any kind of analysis or build any kind of model ask yourself whatat kind of data am I working with
GvRPKPCg5no,2023-12-26T23:07:01.000000,How to learn anything #learning,looking dumb makes you smart this was one of my biggest takeaways from grad school and the point is once I realized that by not being afraid to look dumb not being afraid to ask questions I actually learned so much more so much faster than I did before and this is something that has served me very well even after grad school into the corporate world working as a data scientist it's natural to want to look smart or want to give off this Persona that you know what you're talking about and you're professional you kind of get everything that everyone's talking about but more often than not this holds you back
BGZu6WxevoM,2023-12-19T22:05:55.000000,How Much YouTube Paid Me in My First 6 Months of Monetization (as a Data Science Creator),n/a
sS10PXKqm7o,2023-12-19T15:03:57.000000,Why You Need to “Play” #getpaidtolive #podcast,think of play I think explore you know I think mistakes I think no rules essentially and if you're not living your dream life already then I would assert necessarily you need to play because your current life is not your dream life right so how do you expect to Live Your Dream Life by just continuing to live your current life you know continuing to live by the same rules not exploring new possibil not willing to make mes you know and if you kind of constrain yourself into your current life and your current situation and the only way out of that is to play is to explore and so you at least have a chance of achieving your dream life yeah but the only way to do that is through play
0iFEtnHyzE0,2023-12-18T14:55:36.000000,Fine-tuning EXPLAINED in 40 sec #generativeai,n/a
15Kd9OPn7tw,2023-12-11T18:29:09.000000,4 Ways to Measure Fat Tails with Python (+ Example Code),n/a
TyhlSNB5Ko0,2023-12-11T15:23:39.000000,How I’d learn data analytics (if I had to start over in 2024) #dataanalytics,I was going to start from scratch my dad has a business he has a car dealership if I was starting over I would go to him and be like hey is there any data that I can use to solve a problem for you so if I was trying to get into Data analysis I would take his sales data and make a dashboard and this is actually something I did in grad school but you know maybe your your dad doesn't have a business but if you know anyone that has a business or you know anyone that has data that's one Avenue another Avenue there's so many public data sources out there in the US we have the US Census so like working with that API to get that data and then to do some kind of analysis on US Census Data that could be another Avenue
rtUpRMWFu7k,2023-12-06T14:51:18.000000,How to Move Toward Your DREAM LIFE #getpaidtolive,wherever you are is a starting Place yeah maybe you're getting the money at your job but maybe it's not in the right industry or maybe you're not learning the right skills um or maybe it's not the right role that you want to be working in what you can do is like okay I checked one box but I have like three boxes unchecked is it possible for me to find another job maybe in just in the right industry or at the right company or in the right direction so I can just check one more box and then let me do that for some period of time and then let me re-evaluate and see if I can check another box you know this approach to it is just a checklist that you're ultimately just going down and you know you're not going to do it overnight but like over the course of years you can check off everything on your list and you this is your dream list this is your dream life exactly and just slowly just keep going because what else are you going to do in your life if not check everything off this list oh it's it's like your bucket list
8uwHRVaRsmY,2023-12-04T14:56:41.000000,PCA explained in 60 seconds #datascience,n/a
x5-IW1m3zPo,2023-11-30T23:54:09.000000,Detecting Power Laws in Real-world Data | w/ Python Code,"this is the second video in a larger series on power laws and fat taals in the previous video I gave a beginner-friendly guide to power laws and presented three problems with using our standard statistical tools and analyzing them while in awareness of these problems can help us avoid them in practice it's not always clear whether some data follows a power law or not in this video I will describe how we can detect power laws from Real World data and share example python code for how you can do this in analyzing real world data from my social media accounts and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way you can support me and all the videos that I make the title here is detecting power laws in data a maximum likelihood based approach with python if that doesn't make any sense hopefully it will in just a few minutes just a quick recap of what we talked about in the previous video the central issue that we discussed was that these things called Power laws break many of our favorite statistical tools in the previous video we talked about two general types of distributions the gaan distribution shown on the left hand side here and the power LW distribution shown on the right hand side qualitatively these distributions look very different and it turns out this qualitative difference raised three major problems with using our standard statistical tools in analyzing power laws namely we saw that the mean was meaningless as well as many other standard statistical tools we saw that regression doesn't work so well and then we also saw that payoffs can diverge from probabilities when working with power laws and this all boils down to a single core property of power laws which is they are driven by rare events and this rare event driven property of power laws is more generally described by so-called fat tail Tails if you're unfamiliar with power laws or fat tails check out the previous video because that's going to be a good primer for everything that we discuss here so this rare event driven property of power laws giving rise to these three problems here motivates us to want to detect power laws from Real World data and so one popular way of doing this is what I'll call the log log approach which essentially boils down to plotting a histogram of your data on a log log plot I'm going to walk through the math of where this is coming from from in the previous video we defined the power laws probability density function according to this expression here where P of Z is the probability density function Z is some random variable F of Z is a slowly varying function of our variable and then we have this Alpha index which is the so-called tail index which defines the shape of the long tail of the power law we also saw that power laws are defined according to a minimum value so everything here here has Z greater than some minimum what this log log approach does is you start with this probability density function and then you take the log of both sides so what happens is you can rewrite the log of the PDF as this and it turns out this is a linear model so we can see that more clearly here where the natural log of our probability density function is r y natural log of our random variable is our X and then this term here will be the slope and then this term here will be the Y intercept since the PDF is linear when you take the log and rearrange these things this implies that the histogram when plotted on a log log plot will look something like this you'll have this line essentially with a negative slope as we can see here because Alpha is always going to be positive and then you can take this one step further and actually do a linear regression and estimate values for the slope and Y intercept and then that'll allow you to estimate the Alpha parameter here so while this is a simple way to try to assess out whether your data follows a power law distribution or not there are a handful of limitations with this approach for one the slope that we estimate from the linear regression is prone to systematic errors another is that when fitting a linear model to data that doesn't follow a power law you can sometimes still get a good fit and then one thing that you'll notice here the data that I'm plotting here is truly a power LW distribution that I artificially generate however this line isn't exactly straight which also seems a bit problematic and so there are a few other issues that I won't get into but if you want to learn more check out reference number one where in the appendix they break down a handful of issues with this log log approach and both these references are available in the description below so an alternative to this and the main topic of this video is a maximum likelihood based approach maximum likelihood is a very popular technique in statistics statistics it's a method for inferring the best parameters for a model given some data and this consists of two main steps the first step is to obtain a likelihood function and then the second step is to maximize that likelihood function with respect to our model parameters hence the name maximum likelihood and so I'm going to walk through step by step what this looks like step one is to write the likelihood function and we do this as follows so we have the likelihood of function denoted by L here the arguments of the likelihood function are our model parameters so for a power law we have two parameters the tail index Alpha and then the minimum value for the distribution and the likelihood is defined as the product of the probability density function over all our observations given our model parameters so what this likelihood function will generate is a number which will tell us how likely some choice of model parameters are given our data next we can use a Paro distribution for our probability density function and you'll notice that this takes the form of our power laws probability density function that we defined previously here's the power law term and then l ofx in this case is just a constant which depends on the model parameters plugging this into our likelihood function we get something like this and then doing a little bit of algebra we get this expression here so again for some choice of model parameters we can plug those values in and then take the product over all of our observations to obtain a likelihood of that particular choice of model parameters next what I call step 1B often people will work with the log likelihood function because one it's a little easier to work with and two the log likelihood function and the likelihood function are maximized by the same choice of model parameters so what this looks like is we'll Define the log likelihood function as lowercase L same inputs as the likelihood function and it's just the natural log of the likelihood of function here and then from the previous slide this is the likelihood function here for a Paro distribution and we'll just take the natural log of that using the property of logs we can translate the product within a log function to the sum of log functions and then again using another property of logs we can bring down the exponents to the front which brings us to this expression here and then in step two we maximize the log likelihood function and so the standard way of maximizing or optimizing a function is we take its derivative and set that derivative equal to zero so what that looks like is we'll take the derivative of the log likelihood function with respect to our tail index which gives us an expression like this and then setting this equal to zero implies that Alpha is equal to this expression here and then rearranging this a little bit we get this expression this expression for Alpha it's called the maximum likelihood estimator for that parameter what this allows us to do is given some data and some value for XMen we can derive the optimal value for Alpha that's all the theory stuff let's see what this looks like in code I'll start with an example using artificial data just so we can get a sense of what to expect when we apply it to real world data first we're going to import some helpful libraries namely numpy matplot lib this power Law Library which implements this maximum likelihood based approach that we just reviewed we'll also import pandas and then we'll fix the random seed for numpy just so the results are repeatable to generate the artificial data we can do it like this here I'm generating data following a Paro distribution with Alpha value 2 X-Men value equal 1 generating 1,000 observations this is just an array x with 1,1 values in it and then we can use this function from the numpy library to generate our random sample from a Paro distribution and then similarly we can generate data following a log normal distribution with mean equal to 10 and sigma equal to 1 using again a function from the numpy library we briefly touched on log normal distributions in the previous video what we saw there is that log normal distributions are a little tricky because they can at times appear more gaussian like and then other times appear more like a power law depending on the value of Sigma just to show this graphically we see that a log normal distribution that is thin tailed can look like a gaussian so this is Sigma equal to 0.2 while at the same time a log normal distribution can also appear fat tailed so this is a sigma value equal to two this presents some more difficulties when working with data in the real world it might be hard to discern whether data follows a power law or maybe it just follows a fat tailed log normal distribution and so lucky for us the power Law Library that we're going to be using here has this functionality built into it so that when we do our fit it'll generate parameter estimates for both a power law and a log normal distribution as well as some other distributions it's always a good idea to look at your data whatever data set you're working with so here it's an artificial data set so we have a pretty good idea of what to expect nevertheless here's what the histogram looks like this is just our data plotted in a regular histogram you can see most of the data are here and then there are so few values in the tail you can't even see them in the histogram however when you take the log the tail becomes a bit more visible so you can see that these values start to pop up more and then we can do a similar thing for the log normal distribution qualitatively it can be kind of easy to misinterpret a log normal distribution as a power law distribution because the regular histogram looks very similar however when you take the log of data following a log normal distribution the histogram looks very different than that of the power Lot distribution so this almost looks like a gaussian distribution a lot of times in practice just doing something as simple as plotting histograms and plotting the log of a histogram gives you a pretty good idea of how fat tailed your data are and whether or not it follows a power law distribution however let's see how we can make this a bit more objective and quantitative using the power Law Library we can use the fit method to fit our data generated from a par distribution to a power log with just one line of code and then we can print the results so Alpha is our estimated tail index Xmen is the estimated X-Men value and then p is a quality score it's a number between zero and one that represents how good of a fit the power law distribution is the closer the value is to one the better the power law fit so that's telling us that this is a pretty good fit before comparing to the ground truth values it's important to point out that the power law libraries Alpha definition is different than the standard definition that I've been using throughout this video series so namely the library's Alpha value is equal to the standard Alpha Value Plus one so all we need to do is subtract one from the alpha value generated from the power law fit to make the comparison to the true Alpha value here the true Alpha value we used was two and so subtracting one from the estimate gives us 1.9 so that's a pretty good fit it got pretty close and then the true value of Xmen is 1 and the fit was 1.27 so overall the library did a good job of estimating these parameter values even though we don't have a whole lot of data here okay we can do the same exact thing for the log normal distribution and again we get an alpha value an xmin value an quality score so this should raise some eyebrows again we're getting a very good quality score but this is a log normal distribution why is the log normal distribution described very well by a power law distribution so the thing to point out here is that the X Min value is actually pretty far into the tail looking at this visually here's our histogram of the log normal data and the X-Men value starts about right here so another thing we can do is we can manually fix the xmin value to force the library to fit the distribution to all of the data not just the tail which best fits a power LW so what that looks like is this we can just set this Xmen argument in the fit method and then it'll generate parameter estimates for us of Alpha and Xmen another cool thing about the fit method is that it'll automatically generate estimates of the log normal distribution parameters so without any extra steps we can just plot the MU and sigma estimates for a log normal distribution and we can see that it does a pretty good job at estimating these parameters so comparing to the ground truth of 10 and one the fit method does a pretty good job but of course in practice we don't know what the true parameter values are so just from these results here we wouldn't be able to tell which is a better fit is it going to be the power law fit with these parameters or is it going to be the log normal fit with these parameters so for that we can actually go one step further and compute likelihood ratios between the power law fit and a list of other candidate distributions to give us a sense of which distribution best explains the data so what that looks like is this here I'm just defining a list of different candidate distributions so here we have a log normal exponential truncated power laws stretch exponential log normal positive then I just go through in a for Loop here and use this distribution compare method to compare the power law distribution to each candidate distribution in this list here and then I just print the results what the results look like is an R value and a P value this is different than the P value we saw earlier and this is a P value that we're more used to it is quantifying the significance level of the likelihood ratio R is denoting the likelihood ratio and the way to interpret this is that a positive r value means that the power law distribution is a better fit a negative r value implies that the second distribution is a better fit and then a likelihood ratio of zero means that there's really no difference between the two here in every single case we can see that all the likelihood ratios are negative meaning that all the other candidate distributions are preferred over the power law and we see that the P values are very low they're basically zero in reference number one they use the rule of thumb cut off of 0.1 and this is below that rule of thumb threshold here we see that the likelihood ratio for the log normal distributions has the largest magnitude so we can conclude that the log normal distribution is probably the best fit in this situation and indeed since we know the data was generated from a log normal distribution that it is the correct fit now that we've gone through this whole process with artificial data let's see what this looks like with Messy data from The Real World first we'll just grab this chunk of code cuz we'll use it in a bit here here we're going to be looking at three data sets all coming from my social media accounts we'll be looking at medium followers gained on a month-to-month basis we'll be looking at YouTube earning on a video by video basis and then we'll be looking at LinkedIn Impressions on a day-to-day basis here we just have histograms like we saw before with the artificial data we have the regular histogram on the left and then on the right we have the histogram of the log values it's pretty clear that all of these histograms have pretty fat tails most of the data are sitting in this first bin of the histogram here with a small minority of data in the tals however when we take the log of each value it's more reminiscent of what we saw with the log normal distribution where there is kind of like a typical value that the log of the data tends to be clumped around so based on this we might be thinking the data may be more like a log normal distribution than a power Lot distribution but let's keep going another thing we can do is look at the top five records by percentage what we can see here is that I gained 42% of all of my medium followers in just a single month and so that's the most followers gained in a single month the second place month accounted for about 18% of my medium followers so kind of bringing this together I gained 60% of my medium followers in Just 2 months even though I've been writing on medium for about 3 years now we see a similar thing on YouTube where my number one video by earnings generated 50% of my earnings across all my videos even though I have about 50 videos on YouTube these are very fat tailed distributions which might pull us in the other direction maybe medium followers and YouTube earnings are better explained by the power law and not the log normal distribution but on LinkedIn it's a different story we don't have such a huge distance between the top record and the second and third and so on so this is less fat tailed and I wouldn't be surprised if it's well fit by a log normal distribution based on these numbers and the histogram we saw in the previous slide but let's keep going let's see what the fit looks like the code for this I actually do this all in a for Loop and the full code and notebook is available at the GitHub repository linked here but this is just like a chunk of code from that notebook and it's essentially what we did before on the artificial data set where we're just using this fit method applying it to our data set so I'm doing it in a for Loop so this will kind of iterate between each of the three data sets and then I am manually setting the minimum value the xmin value equal to the smallest value in each data set to forc the fit to use all the data and then from there we can print the power law distribution parameters and the log normal distribution parameters looking at these numbers we see that these Alpha values are very small and again this Alpha value generated from the power law library is one greater than the standard definition of alpha so we actually would need to subtract one from all these values to translate it into the more standard Alpha definition so this would mean that the alpha value is 0.29 0.79 and 0.15 which are super small Alpha values you know typically Alpha values are going to be between 2 and three and if it's really fat tailed it's going to be greater than one but below Alpha equal to 1 as we're seeing in this case here it's what author Nim taleb called the forget about it domain where the mean for the power law distribution is not defined so it becomes very difficult to do any kind of analysis on that data and and just as easily we can get the log normal parameter values one thing that stands out is that the mean for the log normal fit is negative for YouTube earnings so that might be like a red flag that maybe the log normal fit isn't going to be so good for YouTube earnings and just like before we can compute the likelihood ratios to compare the power law fit to that list of other candidate distributions so this is just taking that same chunk of code we saw a couple slides ago and these are the results going through one by one we can see from medium followers the log normal fit is preferred over the power law and it has a significant P value and the magnitude of the likelihood ratio is largest for the log normal fit from this data we would say that the medium followers best follow a log normal distribution for YouTube earnings only one of the ratios is statistically significant and then for that one it's saying the power law distribution is a better fit than the exponential distribution while the other comparisons are inconclusive but kind of based on everything we saw with 50% of earnings coming from one video the negative mean value of the log normal fit I would say that the power law is the best candidate of all the distributions that we're looking at here for the LinkedIn Impressions we actually see something very similar as to what we saw with the artificial log normal data set where all the P values are significant and they're all preferring the non power law distribution So based on this I would say that the LinkedIn Impressions best follow a log normal distribution there's a small caveat here that the medium followers and YouTube earnings data that we're looking at here are a relatively small data set there are less than 100 observations in each and when you're talking about fat tails when you're talking about power laws where you have data driven by rare events small data is a killer because all it takes is one additional data point to completely skew the fits that we're looking at here so all it takes is maybe a couple more observations of like extreme values of medium followers to completely change this from best being described as a log normal fit to a power law fit and conversely maybe as I put more videos out and get more data on earnings I find that the data better follows a log normal fit than a power law fit so we should just take these results with a grain of salt which should always be your mindset as a data scientist skepticism is the default mindset for for a scientist okay so looking ahead to what's next while it is helpful to have a idea of whether some data follows a power law distribution or not this idea of fat tailedness that we described in the previous video is something more General than data following a power law or not following a power law and as we saw before fat tailedness is really on a spectrum from not fat tailed at all to very fat tailed this is why it can be handy to forget about this idea aidea of fitting data to particular distributions and just try to focus on quantifying the fat tailedness of some data set that's what we're going to talk about in the next video of this series through four different heris for quantifying fat tailed in this and then a couple other things I want to call out if you enjoyed this video and you want to dig a Little Deeper check out the blog published in towards data science this kind of goes into a bit more details that I may not have covered here even though this is a member only story you can access it completely for free even if you're not a medium member using the friend Link in the description below and this is the case for any one of my YouTube videos they will all have friend links in them so everyone who is trying to learn this stuff can access the blogs and then finally the code is available on the GitHub repository shown here and Linked In the description below so if you enjoyed this content please consider subscribing that's a great no cost way to support me in all the content that I generate if you have any questions questions or suggestions for future content please drop those in the comments section below and as always thank you so much for your time and thanks for watching"
0oBoHwwJYJQ,2023-11-27T17:29:19.000000,Do NOT become an entrepreneur #entrepreneurship,n/a
2Axas1OvafQ,2023-11-20T19:35:35.000000,DON’T study Gen AI #generativeai,don't study generative AI those early in their career will often ask me what's the best degree to get if I want to get into Ai and while I don't think there's a best path that's true for everyone I do think that there are some paths that are more fragile than others rule of thumb that I buy into to is that when it comes to developing a foundational knowledge about something the older the subject the better and this is based on something called the Lindy principle which B basically says that the life expectancy of a subject is proportional to its current age for example math has been around for thousands of years so we can probably expect math to be around for a few more Thousand Years conversely generative AI has been around for about a year so I expect it to stick around for about another year so the moral of the story is if you want to study something that's going to last study something that's been around for a while
Wcqt49dXtm8,2023-11-15T16:06:40.000000,"Pareto, Power Laws, and Fat Tails","statistics is the Bedrock of Science and data analysis this is why we all learn about it in some form or fashion in school however many of our favorite statistical techniques are completely useless when applied to a certain type of data this specific type of data are called Power laws in this video I'll be giving a beginner friendly introduction to power laws and describe three problems that come up when trying to apply our standard statistical tools to analyze them if you're new to the channel I'm Shaw I make content about data science and Entrepreneurship and if you enjoyed this video please consider subscribing that's a great no cost way to support me in all the videos that I make and with that let's get into it so the official title of this talk is Paro power laws and fat tales what they don't teach you in statistics we'll start with the background information I'll talk about the gaussian distribution Fredo's 8020 rule introduce the power lock class class and describe the difference between weight and wealth then I'll move on to three big problems when trying to use traditional statistical approaches to analyze data following a power law distribution and then finally I will introduce the idea of fat tails which generalizes a key property of these power law distributions so many quantities in nature tend to Clump around a typical value one example of this is if you go to a busy coffee shop and measure the weights of all the customers coming in and out of the coffee shop you would eventually observe a pattern like the one shown here so in other words the weights would tend to Clump around some typical value and then Decay rapidly toward these Tails this is a distribution that most people are familiar with it's called a gaussian distribution also called a bell curve and the great thing about data that follows a gausian distribution is that we can capture a lot of the essential information of the underlying data with just a single number which is the mean and you can go even further and capture how spread out this distribution is via measures like the standard deviation and so these concepts of a gaussian the mean the standard deviation variance Etc these are all Concepts that people will learn in an introductory statistics course or a business statistics course and indeed these are powerful techniques for analyzing ing data solving problems and making decisions however not all data that we care about follows a distribution like a gaussian and a great example of this comes from the work of vredo paredo and so many people have probably heard of paro's principle or the 8020 Rule and typically how this is quoted is that 80% of sales come from 20% of customers however this idea did not originate from the business world or sales and marketing it actually originated from the work of an Italian economist IST and mathematician vredo paredo in his study of Italian land ownership where he found that about 80% of the land in Italy was owned by about 20% of the citizens this simple observation is indicative of Statistics that are very different from the gaussian distribution that we saw in the coffee shop and so what this 8020 rule or Paro principle implies is that the underlying data follows a Paro distribution which looks like this this just qualitatively this looks very different than the gaussian distribution from the previous slide and the biggest difference here is that there's no typical value around which the data is clumped so in the case of a gaussian the mean is very representative of the overall distribution however when looking at a Paro distribution the mean doesn't give you a whole lot of information so in this case the mean is going to be somewhere around here which doesn't tell you much about a lot of the dat data that's living in the so-called tail over here putting this another way while knowing the average weight of an Italian man gives you a good idea of what to expect on your next trip to Rome knowing the average population of an Italian city which is about 7500 is completely useless in grounding your expectations and the reason for this is that weight tends to follow a giian distribution while city populations tend to follow a parade of distribution so the parade distribution is actually part of a broader class of distributions called Power laws and so here are a few different Power laws in red we actually see a power law matching this 8020 rule like we saw in the previous slide making this a bit more General a power law is defined by this equation here so PDF is the probability density function X is a random variable Little X is some specific value of that random variable L of X is some slowly varying function and then Alpha is is just some number which defines the shape of the distribution here and another important note is that power laws are only defined Beyond a minimum value so in these plots here the minimum value is one but this value could be anything these two types of distributions the gaussian like distributions and now these like parol like power law distributions they give us these two conceptual anchors by which we can qualitatively categorize data that we observe in the the real world author Nim Nicholas TB in his book The Black Swan defines these two categories as mediocre Stan and extremist where mediocris are the gaussian like data while extremist are the Paro like data and so the key property of data from mediocris is that no single observation will significantly impact the aggregate statistics to see an example of this suppose on your trip to Rome you go visit the Coliseum and then again you have your scale with you and you decide to start weighing random strangers at the Coliseum so let's say you weigh a th000 people at the Coliseum and compute the average and it turns out to be 175 lb then suppose you add to this 1,000 person sample the heaviest Italian that you can find and so if you do this this will have very little impact on the mean the average might go from 175 lb to 175.2 lb and this is the key property of data from mediocre Stan which is again that no single observation will significantly impact the aggregate statistics there's going to be no person on Earth that you can add to this sample that will dramatically change the mean of the weight distribution however data from extremist on is different in this case a single observation can and often will drive the aggregate statistics so let's say instead of weighing people at the Coliseum you ask them what their net worth is again you get that same sample of 1,000 people and you compute their mean net worth and you find it to be about $300,000 and then let's say you add the richest Italian to the sample what's going to happen here is that the average net worth is going to go from about $300,000 to $7.5 million so about a 25x increase in the average from just a single observation and so that's the key property of data from extremist on and data following a Paro like distribution to get a bit more intuition about this here are some more examples from mediocris Stan and extremist respectively gaussian like data will be things like IQ weight height calorie consumption test scores car accidents mortality rates blood pressure on the other side data from extremist on will be things like wealth as we saw at the Coliseum sales as people talk about with the 8020 rule in business city populations which we mentioned earlier pandemics deaths in wars and terrorist attacks word occurrences and text a very small number of words will be used the most amount of times academic citations a very small number of researchers get the bulk of the citations and Company sizes there are very few number of companies that employ most of the world's Workforce as you can see the things that live an extremists on isn't some trivial set of things in fact you could argue that most of the things that we care about as a society and civilization are not gaussian likee at all while this may seem just like splitting hairs some like technical exercise of categorizing data as gaussian like or par like it turns out there are major limitations to our standard statistical Tools in analyzing data from extremist on and so here I'll highlight three such problems with using our so-called stat 101 techniques to try to analyze these quantities that we care about and so this all boils down to one thing the law of large numbers which basically says if we take n random samples the sample mean will approach the true mean as the number of samples goes to Infinity put another way if we start collecting data generated from a gaussian distribution as we collect more and more samples more and more observations the average that we compute from our sample will approach the true average of the underlying distribution this is also true for the Poo distribution and a uniform distribution and a log normal distribution any distribution that has a finite mean the law of large numbers is true however in practice we never have infinite data we can only have a certain number of observations and this results in some complications with the law of large numbers assumption if we take 10 observations we'll get a pretty accurate sample mean of a gaussian distribution however if we take 10 observations of something generated from a Paro distribution the sample mean is going to be biased this is all because the the law of large numbers Works more slowly for power laws than gaussian distributions which brings us to our first problem the mean is meaningless as well as many other metrics when it comes to working with finite sample sizes of data that follows a power law distribution is that it takes much longer for the mean to converge to the true value compared to a gaussian so we can see this from the plots shown here so on the left we have the number of samples on the x-axis and then on the y- AIS we have the sample mean so this black line here is the true mean and then the blue line is the mean that we compute when the data is generated from a gaussian while this orange line here is the mean that we compute when the data is generated from a Paro distribution as you can see the gussian is never too far off from the True Value you know maybe in the super small sample sizes you have a biased mean but pretty quickly it starts to get really close to the True Value however for the power lot we can see the sample mean is not only much more biased than the gaussian but it's also much more erratic and this extends to not just small sample sizes like 100 observations but to a th000 observations and even 10,000 observations this whole time the Paro sample me is much more erratic than the gaussian and much more biased this even extends to when we 10x the sample size even more to a 100,000 observations at this point the gaussian is right on the money the mean isn't changing at all with additional observations however with the power law the mean is still wiggling around and not quite the True Value and so we're seeing bias at 100,000 observations for the power law similar to what we were seeing at about 10 observations for the gaussian but this isn't limited to just the mean we see this for many other standard statistical quantities that's what's being shown here on the left hand side of these plots we have the respective quantities so we have the median the standard deviation the variance the mean the max first percentile the 99th percentile ptosis and entropy and then horizontally oriented we have 100 samples the th sample case and the 10,000 sample case so while some of these quantities are relatively stable like the median once you get to sufficient sample size it tends to level out the minimum value even in small sample size it's pretty accurate and the first percentile in small sample size is pretty accurate and stable some of these other quantities can't seem to land on a particular value so namely standard deviation variance the maximum the 99th percentile to some extent curtosis and then entropy seems to continually be changing Without End so the one quantity I want to highlight here is the maximum and that's because given this property that rare events Drive the statistics of power LW distributions as sample size increases we see a order of magnitude increase in the maximum value when we go from a th000 samples to 10,000 samples the danger here is that you could have a maximum value that seems stable in a relatively small sample size let's say you have 7,000 observations and the max value seems to have plateaued and it seems pretty stable but then as you collect more data you have this huge jump in the max value and so the danger here is that you can be in this period where it seems like things are stable and predictable but then all of a sudden you have this huge change in the data that you're observing so to connect this to the real world if this data were say deaths from a pandemic what this might look like is the deadliest pandemic in a 100-year time span will be in order of magnitude less severe than a pandemic in a Thousand-Year time span the deadliest pandemic in the past 100 years was the Spanish Flu which killed about 50 million people and we might think okay that was the deadliest pandemic it's not going to get any worse than that if the data is following a power law we can't be surprised if over a Thousand-Year time period the deadliest pandemic claims 500 million victims so this is highlighting this key property of data from extremist on which is that rare events Drive the underlying statistics however this doesn't stop with the mean and all the other standard statistical quantities that we see here it also impacts our ability to make predictions effectively which brings us to problem two regression doesn't work so what regression boils down to is predicting future events from past data and intuitively if your data is driven by rare events you may simply just not have enough past observations to make good predictions about the future and this problem is exacerbated when working with power law distributions so let's look at a particular example let's suppose that we want to do linear regression between the variable X and Y here x is a normally distributed random variable m and b are the parameters that we're trying to learn and e is a noise term that follows a power law distribution so one case where regression just completely breaks down is when this noise term has an alpha value that tail index we saw earlier when we defined power laws is less than or equal to two because in this case the power law has infinite variance so the variance of this noise term is going to be infinity and it turns out if the variance of this noise term is infinite then the variance of this whole equation will be infinite which makes the R 2 value go to zero there's a quick derivation of this in citation number two Linked In the description below in chapter 6.7 but of course you can't observe infinite variance in practice because your data is necessarily finite so what's going to happen when doing regression in practice is going to going be similar to what we saw before with the max value where the results might seem stable in small sample size but then break down as more data are collected we can see this through an example taking our normally distributed random variable with the added power law noise term and doing a linear regression with a 100 samples the results of our regression might look like this which looks pretty good you know maybe there's some outliers here but overall we get a pretty good fit and the r squ isn't bad however this is incorrect correct because the noise term has infinite variance which means r s should actually be zero in this case and indeed as we collect more and more data we can see the R squ value quickly deteriorating so we go from 100 samples to a th000 to 10,000 to 100,000 to a million to 10 million to 100 million and so on this is the danger of doing regression with data that follows a power LW your results might look deceivingly well in small sample size but then as you collect more data your model performance quickly deteriorates but at this point you might say Shaw what's the big deal you know so what if our model can't predict some super rare events like these like 1 in a th000 one in 10,000 Etc events the model can predict 99% of things pretty well why do we care about these super rare events and I agree with you when data are generated from a power law it's not hard to be right most of the time because most of the data do not live in this long t of the power law however when solving problems and making decisions in the real world probabilities are only half of the story the other half of the story are payoffs which brings us to problem number three payoffs diverge from probabilities in other words it's not just about how often you are right or wrong but also what happens when you're right or wrong so let's see what this might look like in a business context consider a software company with three key offerings offer one is they have a free software that has ads they have a premium offer which it's no ads with some monthly subscription and then they have a third offer which is a Enterprise level software with different customizations and add-ons and whatever those clients need and let's say that the 8020 rule is in play so 80% of sales comes from 20% of customers what this might look like is that 80% of customers go with offer one they just use the free version 16% of customers use the premium version and then 4% of clients are the Enterprise clients what this means for revenue is that 20% of the revenue comes from the free users 16% of the revenue comes from the premium users and 64% of the revenue most of the revenue comes from the Enterprise customers so let's say the software company wants to optimize the core service making it run 25% more efficiently and as any good company might do they're not just going to roll this out blindly they're going to ask the customers first they're going to ask their customers you like this update is this something that you need so they do a survey and they find that 95% of the customers like the update 4% of the customers don't really care and 1% of the customers said the update was bad seeing that the overwhelming majority of the customers like the update the company decides to move forward with the update but now fast forward 6 weeks and the company notices a 50% drop in Revenue so what happened it turns out that the company's three biggest clients dropped the service because the software update killed some Legacy data Integrations that were critical to their business while this is just like a madeup artificial example it's meant to illustrate the point that in extremist being wrong one time can erase the gains of being right 99 times and even Beyond if 1% of your customers are driving 50% of your Revenue that means that you can do something that 99% of your customers love and 1% of your customers hate and be much worse off and so now we're going to talk about about fat tales there has been a bit of controversy in extremist on an example of this is Illustrated around wealth going back to Paro this idea that 80% of the land is owned by 20% of the citizens has kind of been applied throughout economics with the prevailing sentiment being that wealth follows a parol like distribution so maybe you've heard something like this when it comes to income inequality where it's like the top 1% has like a third of the wealth or something like that but there's a bit of contr I around whether wealth truly follows a Paro distribution or power law distribution or not so the story goes something like this I'll summarize wealth distribution via the mean and standard deviation but of course if wealth is following this power law the mean and standard deviation are going to be useless because these are parameters for a gaussian distribution not so much helpful for a power Lot distribution so someone will say that's useless because wealth follows a power law but then you have someone else that's saying actually wealth fits a log normal distribtion tion better and then you'll have someone else that says Well Log normal behaves like a power log distribution for high Sigma so this kind of summarizes the controversy here and to just avoid this altogether instead of trying to say does some particular data set follow some particular distribution we can instead focus on fat tails this idea of fat tailedness we can Define as the degree to Which rare events Drive the aggregate statistics of the distribution so this Maps directly onto what we were talking about before with mediocris Stan and extremist where in mediocris Stan rare events do not drive the aggregate statistics while in extremist they do to kind of connect this to different distributions we have a sort of map of mediocris and extremist here so on the far left we have the Gan distribution that we all know and love and then more generally we can call these like student te distributions on the right hand side in extremist da we have the power law distributions that we've been discussing but then we have this land in between and we can Define this as the subexponential domain so an example subexponential distribution is the log normal distribution so we can see for low Sigma it kind of looks like a gaussian but for high Sigma it kind of looks like the Paro distribution and we can kind of index different Power lot distributions according to this Alpha parameter so if Alpha is greater than or equal to two the distribution has finite mean and variance which allows us to do some productive statistics with it if the alpha value is between 1 and two it has finite mean but infinite variance so now regression blows up but at least we have a mean we can work with however when the alpha value is below one the mean is infinite and this is what author Nim TB calls the forget about it domain you can't really do much when the power law has a tail as fat as this as you can see the space between mediocris and extremist ston between gaussian distributions and power law distributions is really a spectrum so instead of thinking of this as like a binary thing as like fat tailed or not this is really a quantity that lives on a spectrum from not very fat tailed to very fat tailed while there's no like true way to quantify fat tailedness there are a few heris that we can employ and so here's some ideas the first one is power Latin and we kind of saw this on the right hand side of that image in the previous slide where as the alpha parameter of the power law got smaller and smaller the tail got fatter and fatter so we can use this tail index to kind of quantify how fat the tails are in other words the lower the alpha value the fatter of the Tails and this is kind of demonstrated in this plot here on the other side instead of thinking of it as like power law we can think of it as like non gaussian there are measures for non-gaussianity the most popular being curtosis however the problem with curtosis is that it breaks down when the alpha value is less than or equal to four because it has infinite curtosis another idea is to use the variance of the log normal distribution and this kind of goes from what we saw in the previous slide where for low Sigma log normal distribution looks gaussian but for high Sigma it looks like a power LW so if you have a log normal distribution you can look at the variance to quantify the fat tailedness and then finally TB defines this Kappa metric which generalizes to any type of distribution where lower values have thin Tails or don't have fat tails and large values have fat tails and Kappa has a max value of one so if you want to learn more about that he talks about in reference number six Linked In the description below so that was a ton of information but to try to boil everything down when it comes to data that follows a power law distribution to Fat tailed data the central problem that comes up in practice is insufficient sample size essentially we don't have enough data to truly capture the underlying statistics to cope with this fact I want to leave the data practitioner with a few key takeaways that I like to think about when navigating these types of problems so first and foremost is to plot distributions plot histograms plot PDFs plot cdfs to get an impression of how fat tailed the data might be just kind of visually another takeaway is to ask yourself is this data from mediocris or extremist or somewhere in between maybe turning to some of those heris in the previous slide to try to quantify the fat tailedness another key take away is ask yourself what's the value of a correct prediction but just as importantly what is the cost of an incorrect prediction and then finally if working with fat tailed data don't ignore rare events don't just chop off outliers if 50% of your Revenue comes from 1% of your clients instead of this being something detrimental to your analytics figure out how you can come up with efficient interventions in that 1% to drive even more business and then a couple things I want to call out is if you enjoyed this video and you want to learn more check out the blog published in towards data science Linked In the description below there I cover a bit more details that I may not have covered in the video here all the code to generate the plots that I showed here are available on the GitHub repository linked here and if you enjoyed this content please consider subscribing to the channel that's a great no cost way to support me and the content that I generate and as always thank you so much for your time and thanks for watching"
JgWV1skSpEc,2023-10-25T16:44:10.000000,I Spent $716.46 Talking to Data Scientists on Upwork—Here’s what I learned.,n/a
Xn_Zw6KSxYU,2023-10-18T16:04:24.000000,I Have 90 Days to Make $10k/mo—Here's my plan,"3 months ago I left my sixf fig data science job to pursue entrepreneurship full-time since I have a video all about why I left that role I won't get into that however here I will talk about how it's going so far and my plan for the next 3 months for how I'm going to get back to my full-time salary but now as an entrepreneur this journey started for me on July 14th 2023 which was my last day as a data scientist at Toyota when I walked out the door I had my SES on three main goals which mapped to the three key pillars of my business Consulting my community called the data entrepreneurs and content my first goal for Consulting was to land one single client that came through an inbound lead so essentially a client found me through a Blog that I wrote or a YouTube video that I made and reached out to me because they wanted to work together on some data science project the second was to get 100 people in attendance across all Q3 events in the data entrepreneurs Community third and finally was to generate $11,000 in one month from my medium blogs and of these three goals I only managed to hit one of them which was Land one client that came through as an inbound lead ironically this was the one that I was most worried about because the relationship between content and Consulting clients is a very unpredictable one while I had always known that content was a great way to generate leads in land clients it was all theoretical until I finally landed that first contract and what surprised me about many of my inbound leads is that they did not come from my most popular content in fact the client that I'm working with currently found me through a Blog that I wrote about topological data analysis which is not only an esoteric data science technique but is not even an approach for using in the project that we're working on together however the most significant takeaway of this for me is seeing this idea of cont content turning into contracts become a reality which gave me a boost of confidence and a signal that I'm on the right path as for the second goal of getting 100 people in attendance across all Q3 events I fell short of this goal while we only had 62 people in attendance I still see this past quarter for the community to be a success one reason for that is this goal of 100 attendees was very much a stretch goal and A good rule of thumb when you're dealing with these types of stretch goals is that even getting 60 to 70% completion can be considered a success this goal accomplished a higher level objective of growing the community a few metrics that reflect that are the Discord Community almost doubled in membership our email and newsletter list almost quadrupled and then our YouTube subscribers increased by 42x so we went from five subscribers on YouTube to about 210 and at this point we're even past that so this failure or success however you want to look at it really inspired ired a new direction for the goals in Q4 which I'll touch on a little later in the video and the final goal of generating $1,000 in earnings in one month for my medium blog turned out to be a bit of a roller coaster ride which I feel is very representative of my life as an entrepreneur medium is a blogging website which I first discovered through technical articles coming from towards data science I've been writing on medium for almost 3 years now putting out content about data science entrepreneurship and other things that I find interesting and if you've been following me on YouTube you probably notice that most of my YouTube videos have an Associated blog with them in June 2023 I made almost $500 writing on medium and the way this works is that anytime a medium subscriber reads one of my blogs medium pays me a portion of their membership fee coming into July 2023 my rationale was since I won't be committing 40 hours of my week to my full-time job I'll have more time to make more blogs on medium so if I double the amount of blogs that I write right I should be able to double my earnings this sounded like a good idea until in August something unexpected happened medium changed their monetization structure which took my monthly earnings from $500 all the way down to $200 even though I was putting out more content on medium and so by the end of August I had basically given up hope that I was going to hit this $11,000 goal but I still move forward with the plan of making twice as much content on medium then something unexpected happened yet again my blog series all about large language models started getting a lot of traction in the final 2 weeks of September and generated about $800 in earnings which brought me $20 Within Reach of my $1,000 goal but of course $980 is less than 1,000 so I technically did not hit this goal my main takeaway from this experience is that entrepreneurship is a wild ride and what I find helpful in the face of all this uncertainty and volatility is to find the work intrinsically rewarding and for me making content isn't about making money while that is a nice upside to it the main benefit that I get from it is that it gives me a way to structure and continue my learning which is super important in a space that is as rapidly evolving as data science and AI That's How The First 3 months went as a full-time entrepreneur now here's my plan for the next 3 months I again set three main goals for the quarter corresponding to each key pillar of my business and if I manage to hit all these goals I'll be in a place where my income will match what I made working full-time as a data scientist which was a little more than $10,000 a month I'll go through these goals one by one and talk more about why that goal is important and how I'm going to achieve it so starting with the first landing 20 hours a week of client work going into q1 the reason that this goal is important is that the overwhelming majority of my Revenue comes through Consulting this is about 90% or more of my business's revenue is from Consulting engagements My Hope Is I can hit this goal of 20 hours a week of work relying solely on inbound leads coming from my YouTube videos blog posts and other content that I put out there however given the unpredictable nature of content this cannot be my only strategy that's why if things aren't looking great halfway through the quarter I'll start an outbound campaign to try to get more clients what this could look like is reaching out to other Freelancers in my network to see if they know of any prospective clients this could be cold emailing or cold dming local businesses and finally applying to contracts on upwork the next goal is to put on 9 to 12 community events and this is about double the number of events that we did in Q3 and the idea is if we double the number of events we'll also double the number of attendees and continue to grow all the different Community channels the reason growing the community is important is because the value of the community scales with the number of members so the more people in the community the more people you can learn from the more people you can collaborate with the more jobs are listed in our job board the more projects are listed in our project board the more people showing up to networking events and all the other amazing things that can happen when you're interacting with like-minded people who are trying to go to the same place as you and while the community doesn't generate any Revenue directly it actually costs me some money and a lot of time it provides me with tremendous value in other ways and hopefully it does the same for other people too three of the biggest benefits I found through running this community are as follows first and foremost is learning you can learn a tremendous amount from someone that's just a few steps ahead of you I've learned a tremendous amount about freelancing and Consulting from data scientists who have been doing this a bit longer than me and on the flip side passing along lessons to those who are a few steps behind you in a sense I find is intrinsically rewarding and helps me solidify my my understanding of these Concepts because a lot of the lessons of Entrepreneurship you're not going to find in the textbook anywhere and I find this type of information is best learned by talking directly to practitioners the second big benefit for me of this community is support in other words sometimes it's great to just have someone to tell you you're not crazy the entrepreneurs journey is definitely not the norm which often leads to entrepreneurs being constantly misunderstood by friends family and others however that one stimulating ation with someone that gets it with someone that is on a similar journey to you makes up for the 10 conversations with people that don't understand what you're doing and the third big benefit is alignment and collaboration in other words when you and the person next to you are going to the same place that alignment naturally generates opportunities for collaboration that takes you both further even faster than you could have gone alone and the third big goal for this quarter is to keep up the content more specifically what that means is posting two blogs a week or two videos a week or one blog in one video a week posting three to five times a week on LinkedIn and posting one to two times a week on Instagram and Tik Tok while content generates some revenue for me so about $1,000 a month at this point the Main Financial upside of making content as a data scientist comes from the inbound leads that it generates for me but of course the leads aren't the main benefit of making content like I said before the key benefit of making content for me is that it gives me a way to structure my learning and keep up with the rapidly evolving space of data science and AI not only does content force me to read articles watch YouTube videos write example code do projects using whatever new technique that I'm learning but it also forces me to synthesize these learnings into a narrative which gives me a tremendous amount of clarity and understanding of these topics there's one last thing that has given me a lot more clarity and a good perspective for looking at my goals for this quarter often times with the uncertainty and volatility of Entrepreneurship things can get hard and it can be easy for me to lose perspective of why am I doing this why don't I just go back to a full-time job where I don't have to worry as much I don't have to stress as much the income is guaranteed why put myself through all this trouble and so what helps me in keeping a positive mindset is looking at all these goals through the lens of learning and giving these two aspects of learning and giving makes it easier to maintain that positive mindset which could be elusive in the ups and downs of Entrepreneurship and so when things get hard and when things get uncomfortable I can just remind myself that it's hard because I'm learning it's hard because it's new and then when I'm putting in a bunch of work for the community or I'm putting in a lot of work for content and maybe it feels like it's not worth it reminding myself that you're helping someone you're helping someone understand a complicated subject you're helping someone level up their data science skills you're giving someone the opportunity to speak to the community about their passions about their expertise you're giving a freelance client access to AI giving them the opportunity to leverage these new technologies in their business and I found that to be kind of like a superpower you know if you're just doing something for yourself it's easy to give up but if you're doing something for other people it makes it easier to stay motivated and engaged in the work that you're doing and so I'm leveraging both of these mindsets learning and giving to help me stay engaged stay productive and stay motivated this quarter if you have any specific questions about my decision- making for transitioning into entrepreneurship or how it's been so far or any questions on why I set my Q4 goals as I did please drop those in the comment section below I'm happy to share anything I've picked up along the way and as always thank you so much for your time and thanks for watching"
ZLbVdvOoTKM,2023-10-05T19:01:23.000000,How to Build an LLM from Scratch | An Overview,"hey everyone I'm Shaw and this is the sixth video in the larger series on how to use large language models in practice in this video I'm going to review key aspects and considerations for building a large language model from scratch if you Googled this topic even just one year ago you'd probably see something very different than we see today building large language models was a very esoteric and specialized activity reserved mainly for Cutting Edge AI research but today if you Google how to build an llm from scratch or should I build a large language model you'll see a much different story with all the excitement surrounding large language models post chat GPT we now have an environment where a lot of businesses and Enterprises and other organizations have an interest in building these models perhaps one of the most notable examples comes from Bloomberg in Bloomberg GPT which is a large language model that was specifically built to handle tasks in the space of Finance however the way I see it building a large language model from scratch is often not necessary for the vast majority of llm use cases using something like prompt engineering or fine-tuning in existing model is going to be much better suited than building a large language model from scratch with that being said it is valuable to better understand what it takes to build one of these models from scratch and when it might make sense to do it before diving into the technical aspects of building a large language model let's do some back the napkin math to get a sense of the financial costs that we're talking about here taking as a baseline llama 2 the relatively recent large language model put out by meta these were the computational costs associated with the 7 billion parameter version and 70 billion parameter versions of the model so you can see for llama 27b it took about 180,000 th000 GPU hours to train that model while for 70b a model 10 times as large it required 10 times as much compute so this required 1.7 million GPU hours so if we just do what physicists love to do we can just take orders of magnitude and based on the Llama 2 numbers we'll say a 10 billion parameter model takes on the order of 100,000 GPU hours to train while 100 billion parameter model takes about a million GPU hours to train so how can we trans at this into a dollar amount here we have two options option one is we can rent the gpus and compute that we need to train our model via any of the big cloud providers out there a Nvidia a100 what was used to train llama 2 is going to be on the order of $1 to $2 per GPU per hour so just doing some simple multiplication here that means the 10 billion parameter model is going to be on the order of1 15 $50,000 just to train and the 100 billion parameter model will be on the order of $1.5 million to train alternatively instead of renting the compute you can always buy the hardware in that case we just have to take into consideration the price of these gpus so let's say an a100 is about $110,000 and you want to form a GPU cluster which is about 1,000 gpus the hardware costs alone are going to be on the order of like $10 million but that's not the only cost when you're running a cluster like this for weeks it consumes a tremendous amount of energy and so you also have to take into account the energy cost so let's say training a 100 billion parameter model consumes about 1,000 megawatt hours of energy and let's just say the price of energy is about $100 per megawatt hour then that means the marginal cost of training a 100 billion parameter model is going to be on the order of $100,000 okay so now that you've realized you probably won't be training a large language model anytime soon or maybe you are I don't know let's dive into the technical aspects of building one of these models I'm going to break the process down into four steps one is data curation two is the model architecture three is training the model at scale and four is evaluating the model okay so starting with data curation I would assert that this is the most important and perhaps most time consuming part of the process and this comes from the basic principle of machine learning of garbage in garbage out put another way the quality of your model is driven by the quality of your data so it's super important that you get the training data right especially if you're going to be investing millions of dollars in this model but this presents a problem large language models require large training data sets and so just to get a sense of this gpt3 was trained on half a trillion tokens llama 2 was trained on two trillion tokens and the more recent Falcon 180b was trained on 3.5 trillion tokens and if you're not familiar with tokens you can check out the previous video in the series where I talk more about what tokens are and why they're important but here we can say that as far as training data go we're talking about a trillion words of text or in other words about a million novels or a billion news articles so we're talking about a tremendous amount of data going through a trillion words of text and ensuring data quality is a tremendous effort and undertaking and so a natural question is where do we even get all this text the most common place is the internet the internet consist of web pages Wikipedia forums books scientific articles code bases you name it post J GPT there's a lot more controversy around this and copyright laws the risk with web scraping yourself is that you might grab data that you're not supposed to grab or you don't have the rights to grab and then using it in a model for potentially commercial use could come back and cause some trouble down the line alternatively there are many public data sets out there one of the most popular is common crawl which is a huge Corpus of text from the internet and then there are some more refined versions such as colossal clean crawled Corpus also called C4 there's also Falcon refined web which was used to train Falcon 180b mentioned on the previous slide another popular data set is the pile which tries to bring together a wide variety of diverse data sources into the training data set which we'll talk a bit more about in the next slide and then we have hugging face which has really emerged as a big player in the generative Ai and large language model space who houses a ton of Open Access Data sources on their platform another place are private data sources so a great example of this is fin pile which was used to train Bloomberg GPD and the key upside of private data sources is you own the rights to it and and it's data that no one else has which can give you a strategic Advantage if you're trying to build a model for some business application or for some other application where there's some competition or environment of other players that are also making their own large language models finally and perhaps the most interesting is using an llm to generate the training data a notable example of this comes from the alpaca model put out by researchers at Stanford and what they did was they trained an llm alpaca using structured text generated by gpt3 this is my cartoon version of it you pass on the prompt make me training data into your large language model and it spits out the training data for you turning to the point of data set diversity that I mentioned briefly with the pile one aspect of a good training data set seems to be data set diversity and the idea here is that a diverse data set translates to to a model that can perform well in a wide variety of tasks essentially it translates into a good general purpose model here I've listed out a few different models and the composition of their training data sets so you can see gpt3 is mainly web pages but also some books you see gopher is also mainly web pages but they got more books and then they also have some code in there llama is mainly web pages but they also have books code and scientific articles and then Palm is mainly built on conversational data but then you see it's trained on web pages books and code how you curate your training data set is going to drive the types of tasks the large language model will be good at and while we're far away from an exact science or theory of this particular data set composition translates to this type of model or like adding an additional 3% code in your trading data set will have this quantifiable outcome in the downstream model while we're far away from that diversity does seem to be an important consideration when making your training data sets another thing that's important to ask ourselves is how do we prepare the data again the quality of our model is driven by the quality of our data so one needs to be thoughtful with the text that they use to generate a large language model and here I'm going to talk about four key data preparation steps the first is quality filtering this is removing text which is not helpful to the large language model this could be just a bunch of random gibberish from some corner of the internet this could be toxic language or hate speech found on some Forum this could be things that are objectively false like 2 + 2al 5 which you'll see in the book 1984 while that text exists out there it is not a true statement there's a really nice paper it's called survey of large language models I think and in that paper they distinguish two types of quality filtering the first is classifier based and this this is where you take a small highquality data set and use it to train a text classification model that allows you to automatically score text as either good or bad low quality or high quality so that precludes the need for a human to read a trillion words of text to assess its quality it can kind of be offloaded to this classifier the other type of approach they Define is heuristic based this is using various rules of thumb to filter the text text this could be removing specific words like explicit text this could be if a word repeats more than two times in a sentence you remove it or using various statistical properties of the text to do the filtering and of course you can do a combination of the two you can use the classifier based method to distill down your data set and then on top of that you can do some heuristics or vice versa you can use heuristics to distill down the data set and then apply your classifier there's no one- siiz fits-all recipe for doing quality filter in rather there's a menu of many different options and approaches that one can take next is D duplication this is removing several instances of the same or very similar text and the reason this is important is that duplicate texts can bias the model and disrupt training namely if you have some web page that exists on two different domains one ends up in the training data set one ends up in the testing data set this causes some trouble trying to get a fair assessment of model performance during training another key step is privacy redaction especially for text grab from the internet it might include sensitive or confidential information it's important to remove this text because if sensitive information makes its way into the training data set it could be inadvertently learned by the language model and be exposed in unexpected ways finally we have the tokenization step which is essentially translating text into numbers and the reason this is important is because neural networks do not understand text directly they understand numbers so anytime you feed something into a neural network it needs to come in numerical form while there are many ways to do this mapping one of the most popular ways is via the bite pair encoding algorithm which essentially takes a corpus of text and deres from it an efficient subword vocabulary it figures out the best choice of subwords or character sequences to define a vocabulary from which the entire Corpus can be represented for example maybe the word efficient gets mapped to a integer and exists in the vocabulary maybe sub with a dash gets mapped to its own integer word gets mapped to its own integer vocab gets mapped to its own integer and UL gets mapped to its own integer so this string of text here efficient subword vocabulary might be translated into five tokens each with their own numerical representation so one two three four five there are python libraries out there that implement this algorithm so you don't have to do it from scratch namely there's the sentence piece python Library there's also the tokenizer library coming from hugging face here the citation numbers and I provide the link in the description and comment section below moving on to step two model architecture so in this step we need to define the architecture of the language model and as far as large language models go Transformers have emerged merged as the state-of-the-art architecture and a Transformer is a neural network architecture that strictly uses attention mechanisms to map inputs to outputs so you might ask what is an attention mechanism and here I Define it as something that learns dependencies between different elements of a sequence based on position and content this is based on the intuition that when you're talking about language the context matters and so let's look at a couple examples so if we see the sentence I hit the base baseball with a bat the appearance of baseball implies that bat is probably a baseball bat and not a nocturnal mammal this is the picture that we have in our minds this is an example of the content of the context of the word bat so bat exists in this larger context of this sentence and the content is the words making up this context the the content of the context drives what word is going to come next and the meaning of this word here but content isn't enough the positioning of these words is also important so to see that consider another example I hit the bat with a baseball now there's a bit more ambiguity of what bat means it could still mean a baseball bat but people don't really hit baseball bats with baseballs they hit baseballs with baseball bats one might reasonably think bad here means the nocturnal mammal and so an attention mechanism captures both these aspects of language more specifically it will use both the content of the sequence and the positions of each element in the sequence to help infer what the next word should be well at first it might seem that Transformers are a constrained in particular architecture we actually have an incredible amount of freedom and choices we can make as developers making a Transformer model so at a high level there are actually three types of Transformers which follows from the two modules that exist in the Transformer architecture namely we have the encoder and decoder so we can have an encoder by itself that can be the architecture we can have a decoder by itself that's another architecture and then we can have the encoder and decoder working together and that's the third type of Transformer so let's take a look at these One By One The encoder only Transformer translates tokens into a semantically mean meaningful representation and these are typically good for Tech classification tasks or if you're just trying to generate a embedding for some text next we have the decoder only Transformer which is similar to an encoder because it translates text into a semantically meaningful internal representation but decoders are trying to predict the next word they're trying to predict future tokens and for this decoders do not allow self attention with future elements which makes it great for text generation tasks and so just to get a bit more intuition of the difference between the encoder self attention mechanism and the decoder self attention mechanism the encoder any part of the sequence can interact with any other part of the sequence if we were to zoom in on the weight matrices that are generating these internal representations in the encoder you'll see that none of the weights are zero on the other hand for a decoder it uses so-called masked self attention so any weights that would connect a token to a token in the future is going to be set to zero it doesn't make sense for the decoder to see into the future if it's trying to predict the future that would kind of be like cheating and then finally we can combine the encoder and decoder together to create another choice of model architecture this was actually the original design of the Transformer model kind of what's depicted here and so what you can do with the encoder decoder model that you can't do with the others is the so-called cross attention so instead of just being restricted to self attention with the encoder or mask self attention with the decoder the encoder decoder model allows for cross attention where the embeddings from the encoder so this will generate a sequence and the internal embeddings of the decoder which will be another sequence will have this attention weight Matrix so that the encoders representations can communicate with the decoder representations and this tends to be good for tasks such as translation which was the original application of this Transformers model while we do have three options to choose from when it comes to making a Transformer the most popular by far is this decoder only architecture where you're only using this part of the Transformer to do the language modeling and this is also called causal language modeling which basically means given a sequence of text you want to predict future text Beyond just this highlevel choice of model architecture there are actually a lot of other design choices and details that one needs to take into consideration first is the use of residual connections which are just Connections in your model architecture that allow intermediate training values to bypass various hidden layers and so to make this more concrete this is from reference number 18 Linked In the description and comment section below what this looks like is you have some input and instead of strictly feeding the input into your hidden layer which is this stack of things here you allow it to go to both the hidden layer and to bypass the hidden layer then you can aggregate the original input and the output of the Hidden layer in some way to generate the input for the next layer and of course there are many different ways one can do this with all the different details that can go into a hidden layer you can have the input and the output of the Hidden layer be added together and then have an activation applied to the addition you can have the input and the output of the Hidden layer be added and then you can do some kind of normalization and then you can add the activation or you can have the original input and the output of the Hidden layer just be added together you really have a tremendous amount of flexibility and design Choice when it comes to these residual Connections in the original Transformers architecture the way they did it was something similar to this where the input bypasses this multiheaded attention layer and is added and normalized with the output of this multi attention layer and then the same thing happens for this layer same thing happens for this layer same thing happens for this layer and same thing happens for this layer next is layer normalization which is rescaling values between layers based on their mean and standard deviation and so when it comes to layer normalization there are two considerations that we can make one is where you normalize so there are generally two options here you can normalize before the layer also called pre-layer normalization or you can normalize after the layer also called post layer normalization another consideration is how you normalize one of the most common ways is via layer norm and this is the equation here this is your input X you subtract the mean of the input and then you divide it by the variance plus some noise term then you multiply it by some gain factor and then you can have some bias term as well an alternative to this is the root mean Square Norm or RMS Norm which is very similar it just doesn't have the mean term in the numerator and then it replaces this denominator with just the RMS while you have a few different options on how you do layer normalization the most common based on that survey of large language models I mentioned earlier reference number eight pre-layer normalization seems to be most common combined with this vanilla layer Norm approach next we have activation functions and these are non-linear functions that we can include in the model which in principle allow it to capture comp Lex mappings between inputs and outputs here there are several common choices for large language models namely gelu relo swish swish Glu G Glu and I'm sure there are more but glus seem to be the most common for large language models another design Choice Is How We Do position embeddings position embeddings capture information about token positions the way that this was done in the original Transformers paper was using these sign and cosine basic functions which added a unique value to each token position to represent its position and you can see in the original Transformers architecture you had your tokenized input and the positional encodings were just added to the tokenized input for both the encoder input and the decoder input more recently there's this idea of relative positional encodings so instead of just adding some fixed positional encoding before the input is passed into the model the idea with relative positional encodings is to bake positional encodings into the attention mechanism and so I won't dive into the details of that here but I will provide this reference self attention with relative position representations also citation number 20 the last consideration that I'll talk about when it comes to model architecture is how big do I make it and the reason this is important is because if a model is too big or train too long it can overfit on the other hand if a model is too small or not trained long enough it can underperform and these are both in the context of the training data and so there's this relationship between the number of parameters the number of computations or training time and the size of the training data set there's a nice paper by Hoffman at all where they do an analysis of optimal compute considerations when it comes to large language models I've just grabbed a table from that paper that summarizes their key findings what this is saying is that a 400 million parameter model should undergo on the order of let's say like 2 to the 19 floating Point operations and have a training data consisting of 8 billion tokens and then a parameter with 1 billion models should have 10 times as many floating Point operations and be trained on 20 billion parameters and so on and so forth my kind of summarization takeaway from this is that you should have about 20 tokens per model mod parameter it's not going to be very precise but might be a good rule of thumb and then we have for every 10x increase in model parameters there's about a 100x increase in floating Point operations so if you're curious about this check out the paper Linked In the description below even if this isn't an optimal approach in all cases it may be a good starting place and rule of thumb for training these models so now we come to step three which is training these models at scale so again the central challenge of these large language models is is their scale when you're training on trillions of tokens and you're talking about billions tens of billions hundreds of billions of parameters there's a lot of computational cost associated with these things and it is basically impossible to train one of these models without employing some computational tricks and techniques to speed up the training process here I'm going to talk about three popular training techniques the first is mixed Precision training which is essentially when you use both 32bit and 16 bit floating Point numbers during model training such that you use the 16bit floating Point numbers whenever possible and 32bit numbers only when you have to more on mixed Precision training in that survey of large language models and then there's also a nice documentation by Nvidia linked below next is this approach of 3D parallelism which is actually the combination of three different parallelization strategies which are all listed here and I'll just go through them one by one first is pipeline parallelism which is Distributing the Transformer layers across multiple gpus and it actually does an additional optimization where it puts adjacent layers on the same GPU to reduce the amount of cross GPU communication that has to take place the next is model parallelism which basically decomposes The Matrix multiplications that make up the model into smaller Matrix multiplies and then distributes those Matrix multiplies across multiple gpus and then and then finally there's data parallelism which distributes training data across multiple gpus but one of the challenges with parallelization is that redundancies start to emerge because model parameters and Optimizer States need to be copied across multiple gpus so you're having some portion of the gpu's precious memory devoted to storing information that's copied in multiple places this is where zero redundancy Optimizer or zero is helpful which essentially reduces data redundancy regarding the optimizer State the gradient and parameter partitioning and so this was just like a surface level survey of these three training techniques these techniques and many more are implemented by the deepe speed python library and of course deep speed isn't the only Library out there there are a few other ones such as colossal AI Alpa and some more which I talk about in the blog associated with this video another consideration when training these massive models is training stability and it turns out there are a few things that we can do to help ensure that the training process goes smoothly the first is checkpointing which takes a snapshot of model artifacts so training can resume from that point this is helpful because let's say you're training loss is going down it's great but then you just have this spike in loss after training for a week and it just blows up training and you don't know what happened checkpointing allows you to go back to when everything was okay and debug what could have gone wrong and maybe make some adjustments to the learning rate or other hyperparameters so that you can try to avoid that spike in the loss function that came up later another strategy is weight Decay which is essentially a regularization strategy that penalizes large parameter values I've seen two ways of doing this one is either by adding a term to the objective function which is like regular regularization regular regularization or changing the parameter update Rule and then finally we have gradient clipping which rescales the gradient of the objective function if it exceeds a pre-specified value so this helps avoid the exploding gradient problem which may blow up your training process and then the last thing I want to talk about when it comes to training are hyperparameters while these aren't specific to large language models my goal here is to just lay out some common choices when it comes to these values so first we have batch size which can be either static or dynamic and if it's static batch sizes are usually pretty big so on the order of like 16 million tokens but it can also be dynamic for example in GPT 3 what they did is they gradually increased the batch size from 32,000 tokens to 3.2 million tokens next we have the learning rate and so this can also be static or dynamic but it seems that Dynamic learning rates are much more common for these models a common strategy seems to go as follows you have a learning rate that increases linearly until reaching some specified maximum value and then it'll reduce via a cosine Decay until the learning rate is about 10% % of its max value next we have the optimizer atom or atom based optimizers are most commonly used for large language models and then finally we have Dropout typical values for Dropout are between 0.2 and 0.5 from the original Dropout paper by Hinton at all finally step four is model evaluation so just cuz you've trained your model and you've spent millions of dollars and weeks of your time if not more it's still not over typically when you have a model in hand that's really just the starting place in many ways next you got to see what this thing actually does how it works in the context of the desired use case the desired application of it this is where model evaluation becomes important for this there are many Benchmark data sets out there here I'm going to restrict the discussion to the open llm leaderboard which is a public llm Benchmark that is continually updated with new models un hugging faces models platform and the four benchmarks that is used in the open El M leaderboard are Arc H swag MML and truthful QA while these are only four of many possible Benchmark data sets the evaluation strategies that we can use for these Benchmark data sets can easily port to other benchmarks so first I want to start with just Arc helis swagen MML U which are multiple choice tasks so a bit more about these Ark and MML U are essentially great school questions on subjects like math math history common knowledge you know whatever and it'll be like a question with a multiple choice response A B C or D so an example is which technology was developed most recently a a cell phone B a microwave c a refrigerator and D an airplane H swag is a little bit different these are specifically questions that computers tend to struggle with so an example of this is in the blog associated with this video which goes like this a woman is outside with a bucket ET and a dog the dog is running around trying to avoid a bath she dot dot dot a rinses the bucket off with soap and blow dries the dog's head B uses a hose to keep it from getting soapy C gets the dog wet then it runs away again D gets into a bathtub with a dog and so this is a very strange question but intuitively humans tend to do very well on these tasks and computers do not so while these are multiple choice tasks and we might think it should be pretty straight forward to evaluate model performance on them there is one hiccup namely these large language models are typically text generation models so they'll take some input text and they'll output more text they're not classifiers they don't generate responses like ABC or D or class one class 2 class 3 class 4 they just generate text completions and so you have to do a little trick to get these large language models to perform multiple choice tasks and this is essentially through prompt templates for example if you have the question which technology was developed most recently instead of just passing in this question and the choices to the large language model and hopefully it figures out to do a BC or D you can use a prompt template like this and additionally prend the prompt template with a few shot examples so the language model will pick up that I should return just a single token that is one of these four tokens here so if you pass this into to the model you'll get a distribution of probabilities for each possible token and what you can do then is just evaluate of all the tens of thousands of tokens that are possible you just pick the four tokens associated with a B C or D and see which one is most likely and you take that to be the predicted answer from the large language model while there is this like extra step of creating a prompt template you can still evaluate a large language model on these multiple choice tasks and in a relatively straightforward way however this is a bit more tricky when you have open-ended tasks such as for truthful QA for truthful QA or other open-ended tasks where there isn't a specific one right answer but rather a wide range of possible right answers there are a few different evaluation strategies we can take the first is human evaluation so a person scores the completion based on some ground truth some guidelines or both while this is the most labor int ensive this may provide the highest quality assessment of model completions another strategy is we could use NLP metrics so this is trying to quantify the completion quality using metrics such as perplexity blue score row score Etc so just using the statistical properties of the completion as a way to quantify its quality while this is a lot less labor intensive it's not always clear what the mapping between a completions statistical properties is to the quality of that that completion and then the third approach which might capture The Best of Both Worlds is to use an auxiliary fine-tuned model to rate the quality of the completions and this was actually used in the truthful QA paper should be reference 30 where they created an auxiliary model called GPT judge which would take model completions and classify it as either truthful or not truthful and then that would help reduce the burden of human evaluation when evaluating model outputs okay so what's next so you've created your large language model from scratch what do you do next often this isn't the end of the story as the name base models might suggest base models are typically a starting point not the final solution they are really just a starting place for you to build something more practical on top of and there are generally two directions here one is via prompt engineering and prompt engineering is just feeding things into the language model and harvesting their completions for some particular use case another Direction one can go is via model fine-tuning which is where you take the pre-trained model and you adapt it for a particular use case prompt engineering and model fine tuning both have their pros and cons to them if you want to learn more check out the previous two videos of this series where I do a deep dive into each of these approaches if you enjoyed this content please consider liking subscribing and sharing it with others if you have any questions or suggestions for future content please drop those in the comment section below and as always thank you so much for your time and thanks for watching"
eC6Hd1hFvos,2023-10-01T20:19:38.000000,Fine-tuning Large Language Models (LLMs) | w/ Example Code,n/a
FEEvnpfD16c,2023-09-30T13:03:33.000000,The REALITY of entrepreneurship. #entrepreneurship #startup #smallbusiness,n/a
0cf7vzM_dZ0,2023-09-26T22:38:42.000000,Prompt Engineering: How to Trick AI into Solving Your Problems,hey everyone I'm Shaw and this is the fourth video in the larger series on using large language models in practice today I'm going to be talking about prompt engineering and now before all the technical folks come after me with their pitchforks let's just address the elephant in the room so if you're a technical person like Tony Stark here you might be rolling your eyes at the idea of prompt engineering you might say prompt engineering is not engineering or prompt engineering is way overhyped or even prompt engineering is just a complete waste of time and when I first heard about the concept I had a similar attitude it didn't seem like something worth my time I was more concerned with the model development side of things like how can I fine-tune a large language model but after spending more time with it my perspective on prompt engineering has changed my goal with this dog is to give a sober and practical overview of prompt engineering and the technical people out there who are rolling their eyes like this version of Tony Stark maybe by the end of this you'll be more like this version of Tony Stark oh wow imprompt engineering will be another tool in your AI data science and software development Arsenal so since this is kind of a long session I apologize in advance first I'll talk about what is prompt engineering then I'll talk about two different levels of problem engineering what I call the easy way and the less easy way next we're going to talk about how you can build AI apps with prompt engineering then I'll talk about seven tricks for prompt engineering and then finally we will walk through a concrete example of how to create an automatic grader using Python and Lang chain what is prompt engineering the way I like to Define it is it's any use of an llm out of the box but there's a lot more that can be said about prompt engineering here are a few comments on prompt engineering that have stood out to me the first comes from the paper by white at all which defines prompt engineering as the means by which llms are programmed with prompts and this raises this idea that prompt engineering is a new way to program computers and this was something that was really eye-opening for me when I first saw tragedy PT and heard this idea of prompt engineering it felt like oh this is just like a chat bot kind of thing but as I dove deeper into it and I read this paper and consumed other resources out there the deeper picture here is that large language models provide a path to making programming and computation as easy as asking a computer what you want in natural language another definition comes from the paper by Hugh at all which defines prompt engineering as an empirical art of composing and formatting the prompt to maximize a model's performance on a desired task the reason this one stood out to me is because it highlights this aspect of prompt engineering then it's at this point it's not really a science it's a collection of heuristics and people throwing things against the wall and accidentally stumbling across techniques and then through that messy process it seems like some tricks and heuristics are starting to emerge and this might be part of the reason why people are so put off by prompt engineering because it doesn't seem like a serious science and that's because it's not a serious science it's still way too early in this new paradigm of large language models that we're operating in it's going to take a while for us to understand what these models are actually doing why they actually work and I think with that we'll have a better understanding of how to manipulate them how to throw stuff at them and get desired results out and the final comment that I really liked about prompt engineering comes from Andre carpathy in his state of GPT talk from Microsoft build 2023 where he said language models want to complete documents and so you can trick them into performing tasks just by arranging fake documents I feel like this captures the essence of prompt engineering language models are not explicitly trained to do the vast majority of tasks we ask them to do all these language model want to do is to predict the next token and then predict the next one and the next one and the next one and so I love this concept of tricking the AI into solving your problems and that's essentially all prompt engineering is constructing some text that generates the desired outcome from the large language model and so the way I like to think about it is that there are two levels of prompt engineering the first level is what I call the easy way which is essentially chat GPT or something similar so now Google has barred out there Microsoft has Bing chat all these different applications provide a very user-friendly and intuitive interface for interacting with these large language models and so while this is the easiest and cheapest way to interact with large language models it is a bit restrictive in that you can't really use chat GPT to build an app maybe it'll help you write some code but you can't integrate chat gbt into some piece of software or some larger application that you want to build out that's where the less easy way come comes in the less easy way is to interact with these large language models programmatically and so you could use Python for this you could use JavaScript or whatever programming language the key upside of the less easy way here is that you can fully customize how a large language model fits into a larger piece of software this in many ways unlocks a new paradigm for programming and software development and that brings us to building AI apps with prompt engineering like I just said the less easy way unlocks a new paradigm of software development and to demonstrate this let's just look at a specific use case suppose we wanted to make an automatic grader for a high school history class and while this might be easy enough if the questions are multiple choice or true false this becomes a bit more difficult when the answers are short form or even long form text responses and so an example of this is as follows consider the question who was the 35th president of of the United States well you might think that there's only one answer John F Kennedy there are many answers that are reasonable and could be considered correct and so here's a list of a few examples so there's John F Kennedy but JFK a very common abbreviation of his name could also be considered correct there's also Jack Kennedy which is a common nickname used for JFK there's John Fitzgerald Kennedy which is his full name and someone probably trying to get extra credit and then there's John F Kennedy where the student may have just forgotten to put one of the ends in his last name let's see how we can go about making a piece of software that can do this grading process automatically first we have the traditional Paradigm this is how programming has always been done and here it's on the developer to figure out the logic to handle all the variations and all the edge cases this is the hard part of programming it's like writing a robust piece of software that can handle all the different edge cases so this might require the user to input a list of all possible correct answers and then that might be hard you know with homework with a bunch of questions you can't anticipate every possible answer that a student is going to write down and traditionally if you're trying to evaluate texts against some like Target text you probably would be using some kind of like exact or fuzzy string matching algorithm but now let's look at this new paradigm where we can incorporate large language models into the logic of our software and here you can use an olm to handle all the logic of this automatic grading task using prompt engineering instead of coming in with some code that does exact matching or fuzzy matching and figuring out the logic that gives you the desired outcome you could just write a prompt and so what this might look like is you write the prompt you are a high school history teacher grading homework assignments based on the homework question indicated by q and the correct answer indicated by a your task is to determine whether the student's answer is correct grading is binary therefore student answers can be correct or wrong simple misspellings are okay then we have this template here where we have q and a as indicated by The Prompt and these curly brackets are indicating where a question is going to be placed in and where the single correct answer is going to be placed in and then we also have a place for the student answer all this can be fed to a large language model and the language model will generate a completion that says the student answer is correct or the student answer is wrong and maybe it'll give some reasoning behind why this student answer is wrong taking a step back and comparing these two approaches to this problem approach one was to manually sit down think and write out a string matching algorithm that tried to handle all the different edge cases and variations of potentially correct answers I'm an okay programmer at best so it would probably take me a week or so to get a piece of software that did an okay job at doing that comparing that to how long it took me to write this prompt which is about two minutes think of the time saving here I could have spent a week trying to use string matching to solve this problem or I could have spent a couple minutes writing a prop this is just like the core logic of the application this isn't including all the peripherals the user interfaces the boilerplate code and stuff like that but that's the cost savings we're talking about here we're talking about minutes versus days or weeks of software development and so that's the power of prompt engineering and this kind of new way of thinking of programming and software development so now let's talk about best practices for prompt engineering here I'm going to talk about seven tricks you can use to write better prompts and this is definitely not a complete or comprehensive list this is just a set of tricks that I've extracted from comparing and contrasting a few resources if you want to dive deeper into any one of these tricks check out the blog published and towards data science where I talk more about these tricks and different resources you can refer to to learn more about any of these so just running through this the first trick is to be descriptive even though so in a lot of writing tasks less is more when doing prompt engineering it's kind of the opposite more is better trick twos give examples and so this is the idea of few shot learning you give a few demonstrations of questions and answers for example in your prompt and that tends to improve the llm's performance trick three is to use structured text which we'll see what that looks like later trick four is Chain of Thought which is essentially having the llm think step by step trick five is using chatbot personas so basically assigning a role or expertise to the large language model trick six is this flipped approach where instead of you are asking the large language model questions you prompted to ask you questions so it can extract information from you to generate a more helpful completion finally trick 7 is what I summarize as reflect review and refine which is essentially having the large language model reflect on its past responses and refine them either by improving it or or identifying errors in past responses okay so let's see what this looks like via a demo here I'm going to use Chad GPT and it's important to know what large language model you're using because optimal prompting strategies are dependent on the large language model that you're using Chachi PT is a fine-tuned model so you don't really have to break your back too much on the prompt engineering to get reasonable responses but if you're working with a base model like gpt3 you're going to have to do a lot more work on the prompt engineering side to get useful responses and that's because gpg3 is not a fine-tuned model it only does word prediction while chat GPT is a fine-tuned model it was trained to take instructions and then on top of that they did this reinforcement learning with human feedback to refine those responses even further trick one is to be descriptive so let's compare and contrast an example with and without this trick so let's say I want to use chatgpt to help me write a birthday message for my dad the naive thing to do would be to type been to chat GPT the following prompt write me a birthday message for my dad and so it's gonna do that and so while this might be fine for some use cases I don't write messages that are verbose like this and the response is a bit generic you know like Dad you've been my rock my guide my source of inspiration throughout my life your wisdom kindness and unwavering support has shaped me into the person I am today for that I am eternally grateful oh that's very nice I tend to be a bit more cheeky when it comes to these kinds of birthday messages and whatnot another thing we can do is to employ this trick of being descriptive and getting a good response from chat you PT what that might look like is you type in write me a birthday message for my dad no longer than 200 characters okay so now we don't want it to be as verbose this is a big birthday because he's turning 50 so now we're giving more context to celebrate I booked us a boy's trip to Cancun more context and then be sure to include some cheeky humor he loves that so I'm giving jack gbt more to work with to tailor the response to something closer that I would actually write so let's see what this response looks like okay so it's a lot more concise which I like it says happy 50th dad time to Fiesta like you're 21 again in Cancun cheers to endless Adventures ahead hashtag dad and Cancun that's actually pretty funny maybe I want to use this exactly but I could see it as like a starting point for actually writing a birthday message so the second trick is to give examples let's compare prompts without and with this trick without giving examples we might prompt chat gbt as follows given the title of a torch data Science Blog article write a subtitle for it here we're putting in the title as prompt engineering how to trick AI into solving your problems which is the title of the blog associated with this video and then we leave the subtitle area blank so the completion that it spits out is Unleash the Power of clever prompts for more effective AI problem solving yeah pretty nifty let's see what this looks like if we give a few more examples to try to capture the style of the subtitle that we're looking for and so here the prompt is pretty similar but now I'm putting in the title and subtitle for preceding blogs in this larger Series so here put a practical introduction to llms three levels of using llms in practice then we have cracking open the openai python API a complete beginner friendly introduction with example code and then finally we have the same prompt as we saw before so let's see what it spits out now mastering the art of crafting effective prompts for AI driven Solutions well at face value this might not seem much different than the completion that we saw before I kind of prefer this one over this one here and the only reason is because again I don't like verbose text and this is more concise than this previous one here so I think maybe that's what Chad GPT picked up on it's like oh these subtitles here have these number of tokens let's make sure that the next subtitle has about the same number of tokens just speculating but regardless that's how you can incorporate examples into your prompt the next trick is to use structured text let's see what this looks like in action so I suppose this is our prompt for tragedy BT we don't have any structured text here we're just putting in prompt without structured text so we're asking it to write me a recipe for chocolate chip cookies gives a pretty good response gives us ingredients gives us instructions and gives us some tips if Chachi PT was not fine-tuned it may not have spit out this very neat structure for a chocolate chip cookie recipe and so this is another indication of why what large language model you're working with matters because I could be happy with this response here there may not even be a need to use structured text here but still let's see what this could look like if we did use structured text in our prompt here the prompt is a little different create a well organized recipe for chocolate chip cookies use the following formatting elements the key difference here is we're now asking it specifically to follow this specific format and we're giving it kind of of a description of each section that we want so let's see what this looks like so one subtle difference here is that in the completion where we use structured text you notice that it just kind of gives the title and the ingredients and so on this is something that you could easily just copy paste onto like a web page without any alterations well if we go here there's no title which could be fine but you have this certainly here's a classic chocolate chip cookie recipe for you so now it's trying to be more conversational and may have required some extra steps if this is fitting into a larger like automated pipeline but other than that it doesn't seem like there's much difference between the other aspects of the completion one interesting thing is that here the tips are a bit more clear and bolded well here there's just some like quick bullet points next we have trick four which is Chain of Thought and the basic idea with Chain of Thought is to give the llm time to think and this is achieved by breaking down a complex task into smaller pieces so that it's a bit easier for the large language model to give good completions without using Chain of Thought this is what the prompt might look like write me a LinkedIn post based on the following medium blog and then we just copy paste the medium blog text here through some text in here so it does a pretty good job again this feels way too long for LinkedIn post and it feels like it's just summarizing the text that I threw in there but I mean it's not bad this could be a really good starting place but now let's see what this can look like using Chain of Thought instead of just having it write the LinkedIn post based on the text here I'm trying to explicitly list out my personal process for turning a Blog into a LinkedIn post and trying to get the llm to mimic that so here I put write me a LinkedIn post based on the step-by-step process and medium blog given below so here step one come up with a one line hook relevant to the blog step two extract three key points from the article step three compress each point to less than 50 characters step four combine the hook compress key points from step three and add a con to action to generate the final output and then we put the medium text here okay looking at this this seems a lot more reasonable for a LinkedIn post each line is just one sentence it's not way too much text no one likes reading a wall of text or at least I don't like reading a wall of text so this is much more helpful to me in making a LinkedIn post okay trick five is to use these chatbot personas the idea here is to prompt the llm to take on a particular Persona so let's see a concrete example of this without the trick let's just say we want chat gbt to make me a travel itinerary for a weekend in New York city so it spits out something that looks pretty good so now let's see what this could look like with a Persona so here instead of just asking it straight up for an itinerary I say act as an NYC native and cabbie who knows everything about the city please make me a travel itinerary for a weekend in New York City based on your experience don't forget to include your Charming New York accent in your response okay so let's see what this does comparing this response with the other response there seems to be a lot of overlap and maybe there's not a practical difference between these two but it does feel like there are things here that you don't get here start your day with the classic New York breakfast at a local dinner Cafe well this one will just say start with the bagel Central Park stroll Museum and grab a bagel again yeah it's just eat Bagels every single day oh that's funny I like how it injected a bit of humor here yep you guessed it another Bagel fuel of your final day maybe you really have to like read through these to get a sense of the subtle differences but maybe just from this Bagel example this just gives you two different flavors of itineraries and maybe one matches your interests a bit more than the other trick number six the flipped approach and so here instead of you asking all the questions to the chat bot you prompt the chatbot to ask you questions to better help you with whatever you're trying to do so let's see this without the trick let's say you just want an idea for an llm based application you give it that prompt and it's just gonna generate some idea for you here's generating a idea for us edu bot pros and intelligent educational platform that harnesses the power of llms to offer personalized learning and tutoring experience for students of all ages and levels so this could be a great product idea the problem is maybe this isn't something that you're passionate about or that you really care about or this idea is not tailored to your interests and skill set as someone that that wants to build an app let's see how the flipped approach can help us with this so here instead of asking for an idea just straight up we can say I want you to ask me questions to help me come up with an llm based application idea ask me one question at a time to keep things conversational you can see right off the bat what are your areas of expertise and interest that you'd like to incorporate into your llm based application idea I didn't think to say oh yeah maybe I should tell the chat bot what I know and what I'm interested in so we can better serve me and maybe there are a bunch of other questions that are critical to making a good recommendation on an app idea that I just wouldn't think of and that's where the flip approach is helpful because the chatbot will ask you what it needs to know in order to give a good response and those questions may or may not be something that you can think of all up front the seventh and final trick is reflect review and refine and so this is essentially where we prompt the chat bot to look back at previous responses and evaluate them whether we're asking it for improvements or to to identifying potential mistakes so what this might look like is here we have the edu bot Pro response from before let's see what happens when we prompt it to review the previous response so here I'm saying review your previous response pinpoint areas for enhancement and offer an improved version then explain your reasoning for how you improved the response so I haven't tried this so we're both seeing this for the first time it looks pretty similar but since we asked it to explain how it improved the responses it gave us this extra section here so reasoning for enhancements Clarity and conciseness emphasizing personalization enhanced language and then monetization strategies the monetization section provides more detail on viable strategies okay cool well I'm not going to read through this but this prompt or something like it you can basically copy paste this as needed to potentially improve any chat completion so I know that was a ton of content and I flew through that but if you want to dive into any particular trick a bit more check out the Torches data Science Blog where I talk about each of of these a bit more insight resources where you can learn more everything we have just talked about is applicable to both the easy way and the less easy way of prompt engineering but now I want to focus more on the less easy way and I'm going to try to demonstrate the power of prompt engineering the less Easy Way by building out this automatic greater example we were talking about before using the langchain python Library first as always we're going to do some imports so here we're just importing everything from langchain and then here we're going to be using the openai API so that requires a secret key if you haven't worked with the open AI API before check out the previous video that talks all about that there I talk about what an API is talk about open ai's API and give some example python code of how you can use it here we're just importing our secret key which allows us to make API calls here we're going to make our first chain the main utility of Lang chain is that it provides a ton of boilerplate code that makes it easy to incorporate calls to large language models within your python code or some larger piece of software that you're developing and it does this through these things called chains which is essentially a set of steps which you can modularize into these so-called chains so let's see what that looks like the first thing we need is our chat model so here we're going to incorporate open ai's GPT 3.5 turbo the next thing we need is a prompt template so essentially this is going to be a chunk of text that we can actually pass in inputs and dynamically update with new information so for example this is the same prompt we saw from the previous slide for the automatic grader we'll be able to pass in these arguments question correct answer and student answer into our chain and it'll dynamically update this prompt template send it to the chat bot and get back the response to put this chain together it's super simple the syntax looks like this you have llm chain you define what your llm is which is chat model which is the open AI model we instantiated earlier The Prompt is prompt which is the prompt template we created on the previous slide and you combine it all together into this llm chain and we Define it as chain what this looks like in action is as follows we Define the inputs so here we're going to define the question who was the 35th President of the United States of America we Define the correct answer John F Kennedy and we Define the student's answer FDR and so we can pass all these inputs to the chain as a dictionary so we have this questions correct answer student answer keywords and then we plug in these values that we Define up here and then this is what the large language model spits out students answer is wrong so it correctly grades the student answer as wrong because FDR was not the 35th President of the United States however there's a small problem with our chain right now namely the output from this chain is a piece of text which may or may not fit nicely into our larger data pipeline or software pipe line that we're putting together it might make a lot more sense instead of outputting a piece of text the chain will output like a true or false indicating whether the student's answer was correct or not with that numerical or Boolean output it'll be much easier to process that information with some Downstream task maybe you want to sum up all the correct and incorrect answers of the homework and generate the final grade of the entire worksheet we can do this via output parsers so this is another thing we can include in our chains that will take the output text of the large language model we'll format in a certain way extract some piece of information or convert it into some other format as we'll see here here I'm defining our output parser to determine whether the grade was correct or wrong and I just use a simple piece of logic here I have it returned a Boolean of whether or not the word wrong is in the text completion as an example before the completion was the student answer is wrong so this word wrong appears in the text completion this parser here will return false because wrong is in the completion and so this knot will flip that and it'll make it false so as you can see like we haven't automated all the logic out of programming you still need to have some problem solving skills and programming skills here but then once we have our parser defined we can just add it into our chain like this so we have our llm same as before our prompt template same as before and then we add this output parser which is the grade output parser that we defined right here and then we can apply this chain so let's see what this looks like in for Loop so we have the same question and correct answer as before who's the 35th President of the United States and then the correct answer is John F Kennedy and now we're defining a list of student questions that we may have received which are John F Kennedy JFK FDR John F Kennedy only one n John Kennedy Jack Kennedy Jacqueline Kennedy and Robert F Kennedy also with one end we'll run through this list in a for Loop we'll run our chain just like we did before and we'll print the result and so here we can see that John F Kennedy is true indicating a correct response JFK is true FDR is false John F Kennedy spelled incorrectly is true because we specifically said misspellings are okay John Kennedy is true because we're just dropping the middle initial Jack Kennedy's true it's a common nickname Jacqueline Kennedy is false that was his wife and then Robert F Kennedy is false because that's his brother and as always the code is available at the GitHub repo for this video series which is linked down here feel free to take this code adopt it or maybe just give you some ideas of what's possible with prompt Engineering in this way I would be remiss if I did not talk about the limitations of prompt engineering which are as follows like I said in before optimal prompt strategies are model dependent what is the optimal prompt for chat GPT it's going to be completely different than what's a optimal prompt for gpt3 another downside is that not all pertinent information may fit into the context window because only so much information can be passed into a large language model and if you're talking about a significantly large knowledge base that's not something that prompt engineering may be able to do most effectively another limitation is that typically the models we use to do prompt engineering are these like huge general purpose models and if you're talking about a particular use case this might be cost inefficient or even overkill for the problem you're trying to solve and another version of this is that smaller specialized models can outperform a larger general purpose models an example of this was demonstrated by open AI When comparing their smaller instruct GPT model to a much larger version of gpt3 so this brings up the idea of model fine tuning and that's going to be the topic of the next video in this series there we're going to break down some key fine-tuning Concepts and then I'm going to share some concrete example code of how you can fine tune your very own large language model using the hugging face software ecosystem so I hope this video was helpful to you if you enjoyed it please consider liking subscribing and sharing with others if you have any questions or suggestions for future content please feel free to drop those in the comments section below and as always thank you so much for your time and thanks for watching
LRLH_yIxHrI,2023-08-29T19:25:48.000000,"Why I Quit My $150,000 Data Science Job",n/a
jan07gloaRg,2023-08-10T19:48:23.000000,The Hugging Face Transformers Library | Example Code + Chatbot UI with Gradio,hey everyone I'm Shaw and I'm back with the third video in the series on using large language models in practice in this video I'm going to be breaking down a hugging face Transformers Library which is a python library that makes working with open source large language models super easy I'll start by explaining some key Concepts before diving into some concrete example code and then at the end of the video we're going to see how we can spin up our very own chatbot UI using Transformers and gradio and with that let's get into the video so in the previous video of the series we were talking all about the open AI python API and what this API allows you to do is to programmatically interact with open ai's language models so you can build tools or if you want to build some kind of product or service however one obvious downside is that API calls cost money so in some situations where this cost might be too prohibitive we can turn to open source Solutions one way we can do this is via the hugging face Transformers Library which is what I'm going to talk about today so you're probably wondering what is hugging face well it's more than just an emoji on your phone hugging face is actually an AI company and they've become a major hub for open source machine learning in the past few years so there are three key elements to hugging faces ecosystem which is largely contributed to its recent popularity the first one is its models there are hundreds of thousands of pre-trained Open Source machine learning models freely available on hugging face second is their data sets repository so these are open access data sets that developers and practitioners can grab to train their own machine learning models or to fine-tune existing models and finally is hugging face spaces which is a platform that allows users to build and deploy machine learning applications so while these three aspects of hugging faces ecosystem have made developing machine learning models more accessible than ever before or there's still another key element of the ecosystem worth mentioning which is the Transformers Library so Transformers is a python Library developed by hugging face that makes downloading and training machine learning models super easy so while the library was originally developed specifically for natural language processing its current functionality spans all different domains from computer Vision Audio processing multimodal problems and more so just to give you a flavor of how easy it is to get started with the Transformers Library let's look at a concrete example suppose we want to do sentiment analysis you can imagine that there could be a lot of steps involved in doing sentiment analysis so first you need to find the model that is able to do this classification task then you'll need to take some raw text and convert it into a numerical representation that you can pass into the model and then you need to kind of decode the numerical output of the model to get a label for the text input and so while this might sound like many different steps and could be very complicated this can all be done in one line of code in the Transformers library and so this is possible with the pipeline function as you can see the syntax is super simple here so we have this pipeline function we just need to specify the task we want it to do so here we put sentiment analysis and then we pass to the pipeline function a text input so here I put love this and then if we run that the output of this line of code here is a label of positive and a score associated with that label so this makes sentiment analysis super easy but of course sentiment analysis is not the only thing we can do with the pipeline function you can also do summarization translation question answering feature extraction text generation and many many more if you want the full list it's available on hugging faces documentation this is the link down here and I'll also drop it in the description below going back to this one line example here it almost feels like magic because we didn't give it a model we just said hey do sentiment analysis and apply it to this text here we could have been a bit more explicit here and specified the model that we wanted to use to do sentiment analysis and so to do that it's very simple we just specify a model using the syntax here we're using distillber base uncased fine-tuned SST to English so this is actually the default model in the Transformers library that was used before so that's why we have the same exact output but what really makes Transformers powerful is we could have put any one of the thousands of text classification models available on hugging face and so to explore these models we could have gone to huggingface.co models which is a growing repository of pre-trained Open Source machine learning models for things such as natural language processing computer vision and much more so let's see what this looks like so we'll click on here and I'm going to zoom in a little bit we can see here there are currently 200 184 000 models on the platform and if we look on the left here these are for all different types of tasks they're these categories multimodal computer vision natural language processing audio tabular reinforcement learning and so just now we were doing sentiment analysis which is a type of text classification so we can see what other models we could have used here so just zooming in there are over 28 000 models we could have used for text classification but notice we can add additional filters to narrow down the models we want to pick so let's say we want to make sure we can use the model easily with the Transformers library in that case we can just click on this Transformers filter and then it'll narrow down even more so we went from 28 000 models to about 27 000 models so still a lot of different options we can also go further we can specify the data sets we want it to have been trained on so for example if we wanted to be trained on PubMed because we wanted to use this for a medical use case and you know so on languages licenses so I guess this is important if you have a specific use case in mind for example you want to develop a product for commercial use you want to make sure that the license kind of aligns with what you're trying to do and then there's some other filters as well so let's just say that we're fine with any text classification model that is compatible with the Transformers library then we can kind of explore from here right now it's sorted by trending but we can also sort by number of likes number of downloads recently updated so the number one most trending one looks like it's from the bloke and the model name is llama270b guanaco Q Laura fp16 so we can go ahead and click on that and if we click on that we get the model card just to kind of explore this a bit here's the organization or the individual that created the model this is the model name we can easily copy the model name if we want to paste it into our pipeline function we can do that very easily also we have all the different tags for the model here we have the text classification tag it's available in Transformers which is what we filtered on but also this model is compatible with pytorch this is an important point because the models on hugging phase aren't only for the Transformers Library they are also for many other popular machine learning Frameworks so if you're not using the Transformers library but using pi torch hugging faces models repository can still be a very helpful resource so another cool thing about the model card is that there are these like quick start buttons here so we can get a quick start for like fine-tuning the model on Amazon sagemaker this will give you some code to jumpstart your model fine tuning and let's say you don't want to just do text classification you want to do text generation or token classification this will give you some example code to get you started also there's deploy so you can deploy this model using Amazon sagemaker and then also you can use in Transformers so this is the same syntax we saw in the slides where you're just specifying the task you wanted to do and then just specify model and then coming out of that there's just a bunch of general information about the model laid out here now you can imagine that we have these model cards for hundreds of thousands of models on this platform if you're trying to build some kind of machine learning app it's really never been easier to get started you can just use Transformers to load in any of these state-of-the-art open source language models and just start building from there so hopefully you have a little bit of a sense of what you can do with the Transformers Library your next thought might be like how do I get this on my machine and start using it so the standard way to install the library is via PIV and hugging face has a great guide on their website on how to do this and so I'm not going to walk through the PIP installation steps here however I will walk through a conda installation specifically for the example code that we will see in the following slides there's two steps the first step is head over to the GitHub repository and download the HF Dash EnV yaml file so here's the GitHub this is linked in the description below here we have the HF Dash env.yaml file it's a yaml file listing out all the dependencies for the example code and here we of course have the Transformers Library along with all the other dependencies and so once you download that this next step is to execute the following two commands in your terminal or anaconda command prompt depending on the machine you're using two commands here one is you're going to change directories into wherever you have this HF env.yaml file saved and then you can create a new content environment using this command here so conda envcreate dash dash file HF env.yaml it'll probably take a few minutes to download all the dependencies but once that's done you should be good to go and you shouldn't get any kind of Errors running the following example code so while Transformers does more than just NLP these days and the example code here we're just going to focus on NLP tasks first we'll start with sentiment analysis so before we saw something like this where we use the pipeline function to do sentiment analysis and we specified the model but what's different here is instead of just doing it all in one line we use the pipeline function to create this classifier object and then we can take this object and pass in text to it and it'll generate an output like this so if we pass in the text hate this it'll spit out a label negative and a score associated with that label however you're not limited to just passing one input at a time into this classifier object you can actually pass in a list and it'll do a batch prediction for all the elements in that list so for example we have a text list here so this is great thanks for nothing you've got to work on your face your beautiful never change and so we pass these into the classifier and we get the following labels so it says the first one is positive since the second one is positive even though I sent some sarcasm there the third one is negative and then the fourth one is positive but of course there are more models than just this default one on hugging face that we could have used so one example is Roberta bass go emotions by Sam Lowe so the difference with this model and the default model is that the model here has several Target labels that it uses for text classification and so just using the same exact syntax we can create a classifier object using the pipeline function and then we can apply the classifier object to the first element in our text list defined before and so you can see now instead of just positive or negative there are tons of labels so there's admiration approval neutral excitement gratitude Joy curiosity so on and so forth and actually there are even more but I just cropped the image because there's just too many super easy to do sentiment analysis with Transformers and super easy to swap out this model with any other model you like on the hugging face platform another thing we can do of course is summarization so even though this is a completely different task the syntax is very similar so we use the pipeline function to specify the task and the model we want to use and create this summarizer object then we Define the text that we want as input into this object then we pass it in along with some other input parameters so here we're defining a minimum length the maximum length and then we do a couple things to retrieve just the summary text so this is text from the blog associated with this video it's just talking about hugging face and what it is and it takes this couple paragraphs of text and it reduces it to the following two sentences hugging face is an AI company that has become a major hub for open source machine learning they have three major Elements which allow users to access and share Machine learning resources and of course you can chain together multiple objects so for example you can bring together summarization and sentiment analysis by passing the summarize text into our classifier from before and we can generate all these different outputs and so really these become like Lego blocks and you can just piece together very easily very quickly these different NLP tools for whatever particular use case that you're working on another NLP task we can do is conversational text the syntax is slightly different because in a conversation like with a chatbot there's a bit of back and forth but we started with the same exact syntax so we can use the pipeline function to specify the model that we want to use to create this chat bot object and then we can use this conversation object to handle the back and forth between the user and the chat bot how we do this is we pass in the initial user prompt into this conversation object and then we save it as conversation then we can update this conversation object by passing it into this chat bot object and we can print the result and so what this looks like is the user says hi I'm Shaw how are you and then the chatbot says I'm doing well how are you doing this evening I just got home from work then to keep the conversation going we can use this add user input method so here we're adding a follow-up question where do you work and again we pass the conversation into the chat bot and then Auto magically the chat bot will generate a response and it'll update the conversation object and we can print it all out here so follow up where do you work then the chatbot says I work at a grocery store what about you what do you do for a living and all of this is just running locally on my machine no need for API calls no need for cloud resources these models are small enough that it's just running locally okay so while this is giving us very powerful functionality this is a very awkward way to interact with a chat bot so let's see how we can spin up a user interface to make this a bit more intuitive we can actually do this very easily using a library called gradio in just a few lines of code so first I'm initializing these two lists so one list is to store the user inputs and another list to store the chat bot responses and then we Define a function I call it vanilla chat bot here and it has two inputs it has the message which is essentially the user input and the history which is just the history of everything that's happened in this python script it is automatically generated and so in this vanilla chatbot function we use the same exact syntax we used in the previous slide so we have this conversation object we're passing in the message so this is the user input but also we can pass in the context of the conversation so the chat bot knows what the back and forth has been up until this point the way we do this is we pass in the message list and the response list so all this goes into conversation and we pass the conversation to the chatbot who will generate a response appendix response to the conversation and then we will return the latest generated response from the conversation object and then in basically one line of code we can spin up the chat interface with radio they have this chat interface base object that we can readily use so the first input of this object is the function we just defined the vanilla chat bot and then we can Define some other things like the title of the user interface and then a description for the user interface so all this gets stored into this demo chatbot object and then we simply can just launch it if we do that it'll start running locally at this URL here so if it doesn't automatically pop up in your browser you can just copy paste this into your browser also you can create a public link to this chat interface which is pretty cool you know you can spin this up and then you can send the link to someone and they can actually access the application that you made locally so let's see what this demo looks like here's the chat bot running if you run it in Jupiter lab it'll actually spin up the UI in the notebook or you can just click on this which will open a new tab and then you can start talking with the chatbot so let's just talk to the chat bot gotta response pretty quickly hello how are you doing today I just got home from a long day work so this chatbot for some reason always wants to throw it in our face that it had a long day at work and it seems like it wants me to ask what it does for a living so this one might take a bit more time but we'll see oh Works in a warehouse it's pretty boring but pays the bills how about you there's some other things here you can like have it redo its output you can undo you can clear it it gave a different response that's cool I'm gonna cashier to the grocery store isn't the most exciting job in the world so it doesn't really matter what this chatbot does for a living it just doesn't enjoy its work so that's pretty cool however we can take this one step further and instead of Hosting this chat bot locally we can host this chatbot on huggingface Via hugging face spaces essentially spaces are just git repositories that are hosted by hugging face and they allow you to create machine learning applications and so let's see how we can do this go to space's website you know we'll actually find a lot of existing applications so let's see this is a really popular one so this is the open llm leaderboard so we can see them ranked here with all these different metrics and you can filter as you like so that's pretty cool these are just open source machine learning applications that anyone can access so you can actually look at the source code by going over to files here and then you can see how they did it you can also clone the repository you can run with Docker so it makes it really easy to not only deploy your applications but find applications that are already on Spaces to use as a starting point but going back to spaces if we want to create a new space we just go here click create new space you can create a name we'll call it vanilla chat bot license doesn't matter we'll use radio as our SDK we'll make it public since this is hosting the application it needs computational resources so you have a few different options here you can have gpus or just CPUs of course A lot of these cost money so we'll just stick with the free version for this example we'll hit create space next you'll see something like this you it's giving us some nice instructions on how to upload our application to spaces so first it wants us to clone the repository create our app.pi file so this is where our application is going to go and then pushing our code to the repository and so an important note here is you need to add requirements basically what libraries are necessary to run your application when you do to make the push so let's see what this looks like we can clone the repository just copy paste this into terminal and we got it here so I already have the files ready to go in a different folder called my first face and so you can see that they're there if we look in the repo we can see it has a readme file already so all we need to do is add this app.py file and this requirements.txt file so I'll do that on my other screen we do that again we should see those files there okay so we clone the repo we added our app file and requirements file and now we just commit and push so here OBS decided to stop working so I lost audio but you can see me pushing the code to the git repository now I'm waiting around for the image and app to spin up then it finally did spin up and we see that we have this nice interface completely hosted on hugging face spaces the chatbot indeed does work but it was significantly slower here than it was when I was running it locally and since this is publicly available you can actually go and interact with this very same chat bot using the link on the screen and also in the description below so that's basically it I hope this video and demo has been helpful to you and given you a flavor of what's possible with the hugging face ecosystem while it does seem like we covered a ton of information and content we've only really scratched the surface of what's possible with the hugging face Transformers library and broader ecosystem so with that being said in future videos of this series we will explore more sophisticated use cases of the hugging face Transformers Library such as how to fine tune a pre-trained large language model as well as how to train a language model completely from scratch couple other things I'll call out is that there is a Blog associated with this video published in towards data science they're definitely details in there that I probably left out of this video and of course as always all the code that I covered in this video is available on the GitHub repository linked here and in the description below and if you enjoyed this content please consider liking subscribing and sharing with others and as always thank you so much for your time and thanks for watching
czvVibB2lRA,2023-07-28T15:24:51.000000,The OpenAI (Python) API | Introduction & Example Code,n/a
4oUOJ37GKYE,2023-07-24T13:50:51.000000,"The more they hurt you, the stronger you get #antifragile",n/a
tFHeUSJAYbE,2023-07-22T14:45:18.000000,A Practical Introduction to Large Language Models (LLMs),everyone I'm Shah and I'm back with a new data science Series in this new series I'm going to be talking about large language models and how to use them in practice in this video I will give a beginner friendly introduction to large language models and describe three levels of working with them in practice future videos in this series will discuss various practical aspects of large language models things like using open ai's python API using open source Solutions like the hugging face Transformers Library how to fine-tune large language models and of course how to build a large language model from scratch if you enjoyed this content please be sure to like subscribe and share with others and if you have any suggestions for me to include in this series please share those in the comments section below and so with that let's get into the video so to kick off the video series in this video I'm going to be giving a practical introduction to large language models and this is meant to be very beginner friendly and high level and I'll leave more technical details and example code for future videos and blogs in this series so a natural place to start is what is a large language model or llm for short so I'm sure most people are familiar with chat GPT however if you are enlightened enough to not keep up with new cycles and Tech hype and all this kind of stuff chat GPT is essentially a very impressive and advanced chat bot so if you go to the chat GPT website you can ask it questions like what's a large language model and it will generate a response very quickly like the one that we are seeing here and that is really impressive like if you were ever on AOL Instant Messenger also called aim you know back in early 2000s or in the early days of the internet there were chat Bots then there have been chat Bots for a long time but this one feels different like the text is very impressive and it almost feels human-like a question you might have when you hear the term large language model is what makes it large what's the difference between a large language model and a not large language model and this was exactly the question I had when I first heard the term and so one way we can put it is that large language models are a special type of language model but what makes them so special and I'm sure there's a lot that can be said about large language models but to keep things simple I'm going to talk about two distinguishing properties the first quantitative and the second qualitative so first quantitatively large language models are large they have many many more model parameters than past language models and so these days this is anywhere from tens to hundreds of billions of parameters the model parameters are numbers that Define how the model will take an input and generate the output so it's essentially the numbers that Define the model itself okay so that's a quantitative perspective of what distinguishes large language models from not large language models but there's also this qualitative perspective and these so-called emergent properties that start to show up when Lang language models become large and so emergent properties is the language used in this paper cited below a survey of large language models available in the archive really great beginner's guide I recommend it but essentially what this term means is there are properties in large language models that do not appear in smaller language models and so one example of this is zero shot learning one definition of zero shot learning is the capability of a machine learning model to complete a task it was not explicitly trained to do so while this may not sound super impressive to us very smart and sophisticated humans this is actually a major innovation in how these state-of-the-art machine learning models are developed so to see this we can compare the old state-of-the-art Paradigm to this new state-of-the-art paradigm the old way and not too long ago we can say like about five ten years ago the way the high performing best machine learning models were developed was strictly through supervised learning what this would typically look like is you would train a model on thousands if not millions of labeled examples and so what this might have looked like is you have some input text like hello Ola how's it going nastabian so on and so forth and you take all these examples and you manually assign a label to each example here we're labeling the language so English Spanish so on and so you can imagine that this would take a tremendous amount of human effort to get thousands if not millions of high quality examples so let's compare this to the more recent Innovation with large language models who use a different Paradigm they use so-called self-supervised learning so what that looks like in the context of large language models is you train a very large model on a very large Corpus of data and so what this can look like is if you're trying to build a model that can do language classification instead of painstakingly generating this labeled data set you can just take a corpus of English text and a corpus of Spanish text and train a model in a self-supervised way so in contrast to supervised learning self-supervised learning does not require manual labeling of each example in your data set the so-called labels or targets for the model are actually defined from the inherent structure of the data or in this context of the text so you might be thinking to yourself how does this self-supervised learning actually work and so one of the most popular ways that this is done is the next word prediction Paradigm so suppose we have this text listen to your and we want to predict what the next word would be but clearly there's not just one word that can go after the string of words there are actually many words you can put after this text and it would make sense in this next word prediction Paradigm what the language model is trying to do is to predict the probability distribution of the neck next word given the previous words what this might look like is listen to your heart might be the most probable next word but another likely word could be gut or listen to your body or listen to your parents and listen to your grandma and so this is essentially the core task that these large language models are trained to do and the way the large language model will learn these probabilities is that it'll see so many examples in this massive Corpus of text that is trained on and it has a massive number of internal parameters so it can efficiently represent all the different statistical associations with different words and an important Point here is that context matters if we simply added the word don't to the front of this string here and it changed it to don't listen to your then this probability distribution could look entirely different because just by adding one word before this sentence we completely change the meaning of the sentence and so to put this a bit more mathematically and I promise this is the most technical thing in this video this is an example of a auto regression task so Auto meaning self regression meaning you're trying to predict something so what this notation means is what is the probability of the nth text or more technically the nth token given the preceding M token so n minus 1 and minus two and minus three and so on and so forth and so if you really want to boil everything down this is the core task most large language models are doing and somehow through this very simple task of predict the next word we get this incredible performance from tools like chat GPT and other large language models so now with that Foundation said hopefully you have a decent understanding of what large language models are and how they work and a broader context for them now let's talk about how we can use these in practice here I will talk about three levels in which we can use large language models these three levels are ordered by the technical expertise and computational resources required the most accessible way to use large language models is prompt engineering next we have model fine tuning and then finally we have build your own large language model so starting from level one prompt engineering here I have a pretty broad definition of prompt engineering here I Define it as just using an llm out of the box so more specifically not touching any of the model parameters so of these tens of billions or hundreds of billions of parameters that Define the model we're not going to touch any of them we're just going to leave them as is here I'll talk about two ways we can do this one is the easy way and I'm sure is the way that most people in the world have interacted with large language models which is using things like chat GPT these are like intuitive user interfaces they don't require any code and they're completely free anyone can just go to the Chad gbt website type in a prompt and it'll spit out a response so while this is definitely the easiest way to do it it is a bit restrictive in that you have to go to their website this doesn't really scale well if you're trying to build a product or service around it but for a lot of use cases this is actually super helpful so for applications where the easy way doesn't cut it there is the less easy way which is using things like the open AI API or the hugging phase Transformers library and these tools provide ways to interact with large language models programmatically so essentially using python in The Case of the openai API instead of typing your request in the chat GPT user interface you can send it over to openai using Python and their API and then you will get a response back of course their API is not free so you have to pay per API call another way we can do this is via open source Solutions one of which is the hugging phase Transformers Library which gives you easy access to open source large language models so it's free and you can run these models locally so no need to send your potentially proprietary or confidential information to a third party and open AI so future videos of the series we'll dive into all these different aspects I'll talk about the openai API what it is how it works share example code I'll dive into the hugging face Transformers Library same situation what the heck is it how does it work and then sharing some python example code there I'll also do a video talking about prompt engineering more generally how can we create prompts to get good responses from large language models and so while prompt engineering is the most accessible way to work with large language models just working with a model out of the box may give you sub-optimal performance on a specific task or use case or the model has really good performance but it's massive it has like a hundred billion parameters so question might be is there a way we can use a smaller model but kind of tweak it in a way to have good performance on our very narrow and specific use case and so this brings us to level two which is model fine tuning which here I Define as adjusting at least one internal model parameter for a particular task and so here there are just generally two steps one you get a pre-trained large language model maybe from open AI or maybe an open source model from the hugging phase Transformers library and then you update the model parameters given task specific examples kind of going back to the supervised learning versus self-supervised learning the pre-trained model is going to be a self-supervised model so it will be trained on this simple word prediction task but in step two here's where we're going to do supervised learning or even reinforcement learning to tweak the model parameters for a specific use case and so this turns out to work very well models like Chachi BT you're not working with the raw pre-trained model the model that you are interacting with in chat GPT is actually a fine-tuned model developed using reinforcement learning and so a reason why this might work is that in doing this self-supervised task and doing the word prediction the base model this pre-trained large language model is learning useful representations for a wide variety of tasks so in a future video I will dive in more deeply into fine tuning techniques popular one is low rank adaptation or low raw for short and then another popular one is reinforcement learning with human feedback or rlhf and of course there is a third step here you'll deploy your fine-tuned large language model to do some kind of service or you know use it in your day-to-day life and you'll profit somehow and so my sense is between prompt engineering and model fine tuning you can probably handle 99 of large language model use cases and applications however if you're a large organization large Enterprise and security is a big concern so you don't want to use open source models or you don't want to send data to a third party via an API and maybe you want your large language model to be very good at a relatively specific set of tasks you want to customize the training data in a very specific way and you want to own all the rides have it for commercial use all this kind of stuff then it can make sense to go one step further Beyond monofine tuning and build your own large language model and so here I Define it as just coming up with all the model parameters so I'll just talk about how to do this at a very high level here and I'll leave technical details for a future video in the series first we need to get our data and so what this might look like is you'll get a book Corpus a Wikipedia Corpus and a python Corpus and so this is billions of tokens of text and then you will take that and pre-process it refine it into your training data set and then you can take the training data set and do the model training through self-supervised learning and then out of that comes the pre-trained large language model so you can take this as your starting point for level two and go from there and so if you enjoyed this video and you want to read more be sure to check out the blog in towards data science there I share some more details that I may have missed in this video this series is both a video and blog Series so each video will have an Associated blog and there will also be tons of example code on the GitHub repository story The goal of the series is to really just make information about large language models much more accessible I really do think this is the technological innovation of our time and there's so many opportunities for potential use cases applications products services that can come out of large language models and that's something that I want to support I think we'll be better off if more people understand this technology and are applying it to solving problems so with that be sure to hit the Subscribe button to keep up with future videos in the series if you have any questions or suggestions for other topics I should cover in this series please drop those in the comments section below and as always thank you so much for your time and thanks for watching
4-Byoa6BDaQ,2023-07-20T14:24:32.000000,"When you’re robust, your environment can’t hurt you #antifragile #resilience",Second Story is the story of the Phoenix which is a bird who when it dies it rises again from its ashes and this highlights the concept of robustness the Phoenix doesn't care if it lives in a stable tranquil environment or a constantly changing environment worst case scenario if the Phoenix dies it'll Rise Again from its actions the Phoenix is apathetic toward its situation
hB27yAkJLC8,2023-07-19T13:57:49.000000,Being fragile means you have more downside than upside. #antifragile #mythology,first story is the sword of dimicles is the story about a servant who sees the life of a king and wants to be in his position so for one day the servant trades places with the King and what he soon realizes is that it's not so great being King because when you have everything everyone wants to take your stuff everyone wants to take your power and the sword sitting above the King's throne is a symbol for the constant threat you are under when you are in a position of power so being King is an example of being fragile because when you have everything you have nothing to gain and everything to lose so any changes will probably not benefit you
Ty2mi994yfE,2023-07-17T16:02:05.000000,How to learn Causal Inference with #python #dataanalysis #datascience,n/a
WzL3USLPwmY,2023-07-16T18:31:57.000000,A physicist edited this video #physics  #datascience #curiosity,a long tradition of physicists poking around where they shouldn't be you'll see physicists everywhere economics biology psychology statistics wherever I think a lot of people that I've met that have studied physics have this Natural Curiosity and they tend to pull on the threat of curiosity and just see where it takes them and so that's why I think you find physicists in all sorts of disciplines I identify with that classically trained in physics but I've worked at a car dealership I work at Toyota in financial services I'm really into entrepreneurship I love business I'm fascinated by nature and natural systems and even going back to Isaac Newton he was getting into finance and there's a famous quote from him he said he could predict the emotions of the stars but he can't predict the madness of man so I think there's always that Curiosity of the physicist to try to explain the world through some kind of physical model or at least something like that
ejwVSYKo7jk,2023-06-30T20:19:24.000000,Lessons from Spending $675.92 to Talk to Top Data Scientists on Upwork #freelance #datascience,n/a
_Wjn0gm4g20,2023-06-23T18:10:33.000000,I Spent $675.92 Talking to Top Data Scientists on Upwork—Here’s what I learned,n/a
NjMD1bGBNqw,2023-05-26T23:04:47.000000,How to Create a Custom Email Signature in Gmail (2024),hey everyone I'm Shaw and in this video I'm going to walk through how to customize your email signature in Gmail so this is a continuation of a recurring theme in my past few videos which is all about leveling up how you present yourself as a professional in the data space so I talked about how to build a free website portfolio using GitHub pages I broke down the resume I used to get me hired in my current full-time role and here I'm going to talk about how to level up your email signature which may sound like a very small part of the story of you as a professional but it is one that I believe is very important so I did a deep dive into customizing email signatures when I started freelancing back in grad school so as a freelancer you're by yourself and you typically don't have a bigname brand to back you up and give you credibility and this is where I feel a solid professionall looking email signature can make a real difference so having a custom professionall looking email signature is an easy way to give yourself credibility and an easy way to share your online platforms and key links to anyone you send an email with with that let's see how to do this all right so here we have Gmail opened so for those who are not familiar you can make custom email signatures in Gmail and so what that looks like every time you hit this compose button to write a new email your email signature is going to pop up there so you don't have to write it out every single time and when you reply to people you can have this come up by default so anytime you send an email to everyone you know you know they have your name they have a link to your website they can have some call to action and they have links to your social media so the way you set up your email signature in Gmail is you got to go to settings so you got to click on this little gear icon here and you'll click on see all settings and then you scroll all the way down to a section called Signature and then you can create a bunch of email signatures here so I have a few on Deck so I got this master one I have one without my socials one with a few socials I have this test one which we can actually I'll go ahead and delete but what we can do to make a new one is just hit this create new button so we'll call this one example Sig as we saw with the email signature earlier you know was this picture and this format at text and these social media icons so while it's easy to you know add text you know put your name here and it's easy to bold it it's easy to add in links here URLs you can add in images and then you can actually link these images uh with the URL one thing that you cannot do with the default settings in Gmail is have these like two columns or multiple columns in your email signature like what we saw before so if I hit compose again you have the head shot on the left and you have all the info on the right and so there's actually a trick in order to get like these two columns or if you want to add a third column or whatever and it's actually pretty simple so I'm going to go ahead and delete this and and what we're going to do is head over to Google Sheets so if you're using Gmail you should also be familiar with Google Sheets what we'll do is we'll open up a new spreadsheet okay so what we're going to do here is we're going to basically create these two columns create the layout for email signature in Google Sheets and then we'll be able to copy paste it over into our custom email signature the way to do this is you select the cell and then we can go over to insert and insert image and we're going to insert image into cell and so here you're going to want to import your head shot so I already have mine ready to go so I'm just going to go and drag and drop this over and we can see the head shot is appearing here and so an important note here is that you need to make sure the head shot is a proper size so here my head shot is about 100 by 100 pixels and you really don't want it much bigger than that in fact if we go over here and we search Gmail signature size we see that the ideal email signature is 100 70 pixels high and 200 pixels wide so we have to be really cognizant about this when we're customizing our email signature and so here I just did a 100 by 100 head shot so just making sure I'm not too tall I think that's going to be the main concern here so I can clean this cell up a bit to make it nice and tight around the head shot if you want to add a bit more space you can always go over here and do the center line and then this will kind of do the resizing automatically for you next we can add some text here so I don't recommend recommend writing out your whole email signature here cuz when I was playing around with this and I was writing out my whole email signature in the cell here and then I copy pasted it over and then the formatting that got pasted over made the email signature too big and it wasn't able to be saved so I recommend just using Google Sheets to set up the formatting and then actually fill out all the content of your email signature in Gmail itself so here I'll just put test just so we can see that some content goes there and let's see I can now let's left align it but let's make it in the center so I think that looks good and then one last thing is if I just copy paste this over it's going to carry with it these like gray lines and it doesn't look very good so we don't want that what you want to do is highlight the cells and you want to add a border but you want to make sure the Border color is white and you do that and then you can click all borders and then you'll see the borders have disappeared from the cell and then when we copy paste it over we don't have those gray marks anymore Additionally you can add this little accent line in between the head shot and the text so I'll just kind of put that in here as well it just like a light gray have some thickness then you click right border and then it adds that little line there so that's optional you know you can really go to town on how you want to do the formatting of this okay but now we have the head shot we have the text we have this line in between and what we can do is just copy these two cells go back over to Gmail and paste that in and so now you'll notice that this head shot looks terrible we actually don't need to keep this head shot here we really really just wanted to get the formatting get these two columns into this signature customizer here so we can actually go ahead and remove the head shot but now you see we have like this other column here and so with that what we can do is we can reinsert our headshot so I'm going to go to insert image and then I'll do upload and then I'm going to drag over the same exact headshot I use so again you know I use 100 pixels by 100 pixels here so just make sure it's a reasonable size if the size is a little big you can go ahead and just hit small and then that'll make it look nice so now with the headshot in we can start adding text so what I had was something pretty simple I just had my name now we can just have your title I like to have my website here and then some call to action so I'm going to put okay so here now we're running into an issue so going back you saw that the text was kind of jumping over to the next line that's cuz I didn't make this second cell wide enough so we're going to going to go ahead and make that cell a bit wider and then we're going to come back delete all this put this back in and just repeat those steps 100 by 100 pixel head shot comes in we'll make it small okay so now we'll try again so put in my name data scientist so yeah so I like to put my homepage in there and then I'll put a call to action book a call so this is a good checkpoint here cool thing is just like with a lot of text editors these days we can make any text a URL so I'm going to go over to my website and I'm going to get this link as well and I'm just going to copy this and you can highlight the text you want to turn into a link you just highlight it and then click this link button and then you're just going to paste in the link you wanted to go to and so now you should see it pop up as a link and then we'll just do that again for book a call there we go and so already this looks like a pretty nice email signature maybe I'll I'll add a space here just to give it a bit more distance from that line but this looks pretty professional so kind of go in the extra mile we can import the social media icons like we saw so kind of going back adding in these social media icons and what's cool about these is that these are actually clickable links when you send the email and so to do that we'll do a similar thing as we did before and we'll insert an image and so if we click this insert image button here and there are actually two ways to do this so the first way is you can import an image as a URL and so you know easy way to do this is just go to like any website so I already have it loaded up in the search bar social media icons for email signature I I found this one was pretty good and what you can do is just go to any website that these images exist and you can just right click on it and copy the image address and if you do that you can come back over here paste it in and you'll see the image pop up and there you go so now the image is in your email signature the other way to do it is to insert the image like we did before which is by uploading it from your computer so I have these like already on my desktop so I'll just kind of go drag and drop them over so there we go and now the image is there and so again whenever we're inserting images into the email signature we have to be cognizant about the size of the image so the head shot I did is 100 pixels x 100 pixels and this LinkedIn icon I made sure it was 24x 24 pixels and that's actually the same as this website over here I thought this was a good size 24x 24 pixels but this website has more 32 64 128 so on also it's pretty easy to resize images if you're on Mac you can use preview and so it's pretty easy typically to scale down images just find it on the web somewhere download it and just make sure it's 24x 24 or 30 X3 whatever you want to make it okay so we'll just repeat this process so I'll go ahead and insert the other links added a space and I'm going to hit insert image again I'm going to upload it again and then I'm going to bring in a few more so here's Youtube upload okay and you know obviously what social media icons you put here will depend on your industry and whatever platforms you're on but you can kind of customize it get the spacing right I just added white spaces in between the icons for the formatting and then what you can do to actually make these clickable links is you can highlight them just like we did with the text hit this link button and then paste in whatever link you want so I'll go back my homepage and grab all these links so grab my LinkedIn grab YouTube grab medium highlight and then finally get sometimes it'll hide behind this thing but you can find that all right there you go and so now we have our custom email signature and when we're done we got to make sure we save changes so we'll click that I'll make a new email I'll send it to myself make the subject line check out my email Sig and so this is actually a different one you can actually choose whichever email signature you want to use so I'll go to the one we just made which is called email Sig and it'll actually automatically change to whatever email signature you want to use so maybe you want to have different signatures for different clientele or different audiences and then I'll just hit send all right so that's it so I hope that was helpful if you were painstakingly trying to figure the out for hours like I was at one point hopefully this made your life a little easier if you enjoyed this video be sure to like subscribe and share it with others if you run into any issues or you have any questions please feel free to leave a comment down below I do read all the comments and try to answer all the questions that I receive and as always thank you for your time and thanks for watching
gp29_P3_lgo,2023-05-06T16:03:12.000000,"My $100,000+ Data Science Resume (what got me hired)",everyone I'm sure and in this video I'm going to walk through the data science resume that actually got me hired and so I've worked as a data scientist in three different Arenas in research in freelance and now in corporate and through this experience I've gotten a good sense of what the day-to-day of a typical data scientist looks like and what it takes to be successful in a data science role so the whole motivation for this video is that I know a lot of people these days are trying to get that first data science job that first job in any industry is always the hardest to get you know it's that old story of you need experience to get a job but you need a job to get experience and so the goal of this video is to talk through the resume that actually worked and got me hired and share the principles and tips that help me craft the resume as well as things that I did outside of the resume to help me land the job and so be sure to stick around to the end because I'm going to talk about five specific things that you can do to increase your value as a candidate Beyond on the essentials of a resume so I want to start by asking what is the goal of a resume and so it might sound obvious you might say the goal of our resumes to get the job or the goal of her resumes to describe my experience the goal of her resume is to say why I'm a great fit for this role so while there are many goals of a resume the one that I found the most clear and helpful is that the goal of a resume is to get the interview and the reason why I think this way of thinking about it is important is because people get so wrapped up in trying to jam-pack every little detail into their resume with the attention of trying to come across as a more attractive candidate but ironically when you put way too much information especially information that's not relevant to the role it actually makes you a less compelling candidate which brings me to my foundational principle of crafting a resume Which is less is more the way I see it the person reading your resume has a finite amount of attention that they're going to give to your resume so you've got about seven seconds to convey not so much who you are as a person and every little detail of your experience but rather your goal in that seven seconds is to be someone interested be someone that they want to reach out to be someone that they want to set up an interview with and so as we walk through my old resume we're going to see this principle of less is more come up through the design and the content of the resume so let's see what this looks like okay so here is the resume so this was for my current role which is a data scientist at Toyota financial services and so you can see this is a very simple resume no pictures no colors just to the point so I'm going to talk through four things that I think were good here so the first one isn't something I included but something I did not include which was this objectives section that you might see on other resumes and so just thinking back to this principle of less is more and that you have a finite amount of the reader's attention do you want to spend that fine finite attention for them to read a paragraph of objectives or do you want to save that attention for some impressive projects you may have done further down in your resume so for me I don't see any utility in having an objective section for data science roles the second thing I think was good here is I added this technical skills section and not only do I have this section I have it at the top of the resume and so I think one thing that's happening is if a human is reading your resume they are looking for specific skills especially in data science they want to know can this person code in Python do they have SQL experience do they have cloud computing experience you know they're looking for these types of things and so don't make them hunt for these keywords because they're going to be so focused on trying to find python in this wall of text here that they're gonna miss everything else so just give it to them at the beginning so they can relax their mind they're like okay great this person can code in Python that's all we use on our team so now with that sense of relief they can actually read your resume with an open mind the third thing is pretty standard which is these bullet points and how you craft these bullet points and so the format that I've been taught and that I found helpful is to start with a strong action verb and then there are resources online that give you like a strong action verb ideas for these types of bullet points you start with the strong action verb what you did and have some kind of quantifiable impact associated with that thing so here I put conducted data collection processing and Analysis for novel study evaluating the impact of over 300 biometric variables on human performance and Hyper realistic live fire training scenarios so that sounds really interesting but I will say one thing that's missing here is the quantifiable impact so what was the result of this work just stop and ask yourself that question of any bullet point you have on here it's like why is this important in a larger context and I would even say if you can't think of a wider impact of any bullet point consider just dropping it and picking up something else that has a higher impact and so if I was to do this over again I would think a bit more carefully about the impact of this bullet point here and here's a corollary to that anytime you can put numbers in these bullet points it just stands out like you might see clickbaity blogs out there like five ways to lose weight fast or seven secrets that will make you a more attractive mate or something like that there's a reason they put numbers in the titles is because our minds are attracted to that when you see a bunch of letters the numbers stand out and it gets your attention and so use numbers especially if you can have a quantifiable impact like here this is a better example analyze marketing and sales reports to inform an inventory acquisition which resulted in a 50 decrease in average inventory age that's a quantifiable impact and that's the type of bullet point you want to have for everything and then the fourth thing is a really small thing but I think it does make a difference in addition to listing your technical skills here bold them in each of these bullet points so this was actually something I picked up from a Google hiring manager when I was in the looking for a job phase of my life in grad school he was actually the one that also told me to put technical skills at the top and then he additionally said both these things because again if the person reading your resume is looking for something specific don't make it hard for them to find it those are the four things that I liked as far as things that I would do differently or improve upon one I already mentioned which is a lot of these bullet points don't have any kind of quantifiable impact they have the strong action verb they have the technical task that I'm trying to convey but there's no impact the second thing that I would improve on this resume is that notice that it's a two-pager and so there's debate on whether a resume should ever be two page should always be one page I'm not going to get into that but specifically here I might consider dropping the talks and Outreach maybe even the awards and honors here and then just have an Abridged version of the Publications just to bring it down to one page and just try to make it a bit more concise just to recap the four things that I thought were good here were no objectives section putting the technical skills at the top so the reader doesn't have to hunt for it having these strong action verb thing then quantifiable impact on the end and then finally building these key skills again so the reader doesn't have to hunt for them and then as far as the two things that I would improve upon is to reevaluate these two bullet points so they do have quantifiable impact and two try to remove some of these sections so this could just be a one-page resume instead of a two-pager another key point with the whole job application process and hiring process is to always ask for feedback no matter what even if you don't get the role ask for feedback in the interviews even if you don't get the interview ask for feedback if you can get in contact with a person and so toward that end when I had that first interview with the hiring manager like that initial interview before you get to the technical interviews and interviews and all that kind of stuff I asked a simple question to the hiring manager which was why me what stood out about my resume what made you think I was a good fit for this role and so I find that a very good question to ask in any kind of hiring process because now you're getting a little bit of insight into their world you're seeing what their problems are and how they see you fitting into that world and so the hiring manager mentioned two things that stood out about my resume that he liked the first was PhD and if you don't have a PhD or you're not pursuing a PhD you might think okay wow that's not helpful at all but I will say in my experience it seems that graduate degrees do open a lot of doors and opportunities so even if we're talking about a master's degree and not a PhD having these Advanced degrees definitely help differentiate you from other candidates and then the second thing that stood out wasn't so much the resume itself because again really all that seemed to stand out was the PHD okay he knows python let's talk to this person but the other thing that that stood out was I had this website here and so we can click on this and it takes you to an actual website having a website a home page an online portfolio is a great way to just stand out as a candidate and so this is being hosted completely for free using GitHub pages and actually in the last video on my channel I have a whole walkthrough guide on how you can spit up a website completely for free using GitHub pages so just having a portfolio website can help differentiate you from other candidates and it doesn't even have to be as sophisticated as this so this took me about a weekend to spend together using this open source drag and drop website builder called Moby rise so I use this Moby rise it's like a drag and drop interface to build the website it's completely free but you don't have to go through all this trouble so GitHub Pages gives you these great themes to spin up a website from a markdown file so still no coding involved whatsoever but the result in like 15 minutes of filling out a readme file you can spin up a good looking website that can also help differentiate you from other candidates and so that was the resume and as we saw based on the hiring manager feedback it wasn't just the resume that stood out it was also this portfolio so this is a great point because you have to realize the resumes and everything there's so much more beyond the resume that goes into the hiring process and everyone has the resume but when you're trying to stand out when you're trying to get the interview a lot of times it pays not so much to be a better candidate but to be a different candidate and what I mean by that is it pays to stand out to distinguish yourself as a candidate for a role and so we already talked about one way to do this which is a website portfolio but even beyond the resume and Beyond the portfolio there's still more that can be done and so what I'm talking about here is networking so a great way to do this is LinkedIn if you have a role that you're interested in go on LinkedIn and try to find people that work at that company try to find people that work on that team try to find people working in HR are at that company and just try to make contact with a human even though it's not going to work out every time that 20 of the time that you actually get connected with someone and start talking to someone that's in the company this can have a very significant impact on your probability of getting the job so if you're sitting there and you're like Shaw you're not telling me anything I don't know I have the resume I have the portfolio I reached out to 200 people in the last three months on LinkedIn and still I'm not getting anywhere so if that's the case a lot of times What's Happening Here is you need more experience you need more development as a data scientist which brings us back to the dilemma I mentioned at the beginning of this video I need experience to get a job but I need a job to get experience but I don't think that's the case necessarily I can think of five things you can do to get more experience as a data scientist and make yourself a more attractive candidate for these data science roles and so the first one that might sound obvious which is education and so this is a lot of go-to's for people you're applying to data science jobs with a bachelor's degree and you're not getting anywhere maybe get that master's degree maybe get that certification if you have the certification master's degree and it's still not working out for you maybe go for another certification or maybe go for a PhD but I'll just caveat this if you're getting a degree just to get a job I don't think that's going to be the right answer most of the time the second thing you can do is independent projects if you don't have enough experiences as data scientists get the experience by coming up with projects go find some data on the web build a web scraper and gather data and start building interesting data sets so you can do interesting data science projects and put that on your resume put that on your portfolio the third thing that you can do that I found very helpful is to make content write blogs on medium publish them in towards data science make YouTube videos like this where you're explaining data science topics where you're explaining your data science projects and you're displaying your competency and people have something tangible they can click on on your portfolio or you can refer them to with a link and Beyond just being something you shared with people sometimes these blogs and videos they take on a life of their own and people start coming to you asking you questions about this stuff and maybe even offering you jobs the fourth thing you can do that seems to be coming more and more popular these days isn't so much doing more data science projects or going back to school but just hosting on LinkedIn and so I've seen a lot of people working in data roles or still in school building a following and building an audience on LinkedIn and through that growth they're getting a lot of visibility and this leads them to getting job offers and so this is interesting because you don't necessarily need to be developing your skills further you just need to be showing off the Knowledge and Skills that you already have and then the fifth option is you can start freelancing and so I really like this option because if your goal is to get a full-time data science role when freelancing on a website like upwork you're getting a lot of interview reps and the time it takes you to apply to 10 full-time roles you can apply to a hundred upwork roles so you're getting 10 times the experience writing resumes writing cover letters doing interviews than you do otherwise and so you can really develop that skill set quickly and another benefit of freelancing you get experience by working on real world projects that aren't just for education but are making an impact for your clients and is putting money in your pocket and so I hope this video was helpful for you and trying to land that first data science role and break into the field if you enjoyed this content please consider liking subscribing and sharing with others if you have any questions about data science or career stuff please feel free to drop those in the comment section below I do read all the comments and try to answer to all the questions that I received and as always thank you so much for your time and thanks for watching
D9CLhQdLp8w,2023-04-24T21:30:03.000000,How to Make a Data Science Portfolio With GitHub Pages (2024),everyone I'm sure and in this video I'm going to walk through a super easy way to make a portfolio website without any coding using GitHub pages so it's about that time of year people are graduating graduating college maybe you're graduating grad school and now it's time to enter the workforce and if you're like a lot of people you've probably heard of data science and how data scientists are doing all these cool things you know they're building chat gpts they're building models they're using machine learning and AI to solve business problems and create impacts and this sounds like a really fun and exciting field to be a part of as someone that works in data science a big problem for a lot of data scientists is not so much knowing your stuff like the technical side of things but it's the ability to show your stuff and sell yourself and this is just something that you never really do When developing the skill set to become a data scientist but if you're trying to get a job whether that's full-time at a company or you want to go into freelancing or Consulting being able to sell yourself is a critical part of the process so one thing that I found super helpful in getting data science work is having a website portfolio that employers can go to to see my experience my projects and just get to know me but the problem is I'm a data scientist I can build you a machine learning model but don't ask me to build you a website because it's not going to be great so I'm totally incompetent with HTML CSS any kind of like web dev type of stuff and if you're a data scientist it's likely that you're in a similar boat but lucky for us our friend GitHub has this built-in functionality for generating and hosting websites completely for free without requiring any kind of web development experience whatsoever so GitHub Pages makes this spinning up a website super simple and I use this functionality a lot in grad school in spinning up websites for my own portfolio and for projects that I was working on quickly running through here what the steps are you have these two options you can build a website from scratch and just host it on GitHub or you can go a much easier route where GitHub will generate you a website automatically from your readme file using a package called Jekyll And so here all you gotta do is fill out your readme file with what you want in your portfolio and then Jekyll will take that text and generate a website based on the theme that you choose not too long ago like one two years ago this was like really really stupid simple just looking at this website it's a pages.github.com just a few steps so you go to the repository settings and then it used to have this Theme Chooser built in so you just say like which branch you have your readme file in and then you just like choose a theme and then you have this user interface this GUI where you could just click which theme you wanted and it would just automatically generate the website so when I first wanted to make this video I'm like okay yeah it's gonna be super simple show people how they can make a website completely for free free without any web development experience whatsoever but when I came to spin up my website this button was not there anymore this kind of let me down a rabbit hole trying to figure out what happened couldn't find anything anywhere no one's made a YouTube video about this no one's written a Blog about this github's documentation was not very helpful to me however after messing around with it for a couple hours I finally figured it out and now I'm gonna walk through step by step how you can build your website using GitHub pages and this built-in functionality so this is what the final product looks like so it's a really clean design cool looking website you have your picture here you think about your name like whatever job you're looking for I didn't do any kind of coding whatsoever I just typed up a markdown file and this was automatically generated so I was able to throw this together in less than 30 minutes because a lot of the stuff it's just like copy pasting from your LinkedIn or your resume or something similar and so this is the final product now I'll walk through the steps to build this so first step go to your GitHub Pro profile if you don't have a GitHub profile go ahead and make one it should be straightforward and a good idea if you're trying to get a job in data science here at our GitHub profile we're going to go ahead and click repositories and you can see this is the example portfolio I made this past week but we're not going to go there we're going to start over from scratch and click new and then we should get a screen like this so create a new repository repository name so we have two options here option one is if you want this portfolio website name to just be your GitHub username.github.io you just type that in right here so Shaheen T is my username GitHub dot IO this will appear in the search bar and this will be your website name I already created a website portfolio using this website name so it's not going to work for me but if you haven't done this already then it should work for you the second option is you can give it any repository name you like so it could just be portfolio well it could be data science it could be data scientist you know whatever you want and what's going to happen in the search bar for example if you make your repository name portfolio what's going to appear in the search bar is going to be your GitHub username.github dot IO slash whatever you make this repository name so if I make a portfolio it'll be Shaheen T github.io portfolio Okay so we'll go ahead and name it portfolio we don't need a description here and it's probably best that we don't have one but we're going to initialize our repository with a readme file so we'll click that this is gonna make it really quick and easy to spin up our website I'm not going to have a git ignore we're not going to add a license that's not necessary so just to recap we make our repository name either our GitHub username.github.io or whatever other name that we like so just go in portfolio here don't need a description and just be sure to click read me and then we're gonna create repository and there we go so now we have the super Bare Bones repository so we already have a readme file here but we're going to need to add one more file called a config file so to do that we can just go to add file create new file and we'll call it config.yml and so now that GitHub got rid of this choose theme button in the interface so let me go back now that they got rid of this choose theme button here and you can't have this super easy user interface for picking your theme you have to do it using this config.yaml file and so this is actually super simple and it's not much more difficult than this interface so I'm going to kind of jump ahead and this is the config file for the example that I've already put together we can just go ahead and like copy paste this so title would be at the top left corner of the portfolio website so you can just put your name here logo we can actually comment this out for now because this is going to be the relative path of the image you want on that left hand side so let me just go back so this is where the title appears this logo relative path is where this image is located in your GitHub repo but since we're starting fresh I'm going to comment that out if you want to add text below here so if we added a description to our repo it would appear here or we can overwrite that by manually putting a description here so show downloads true it's just giving you these options to download the zip the tar or go to the GitHub so I put true for my example portfolio if people want to steal the code but for yours I mean this is optional if you want people to be able to download your code you can make it false or just comment it out and then finally the key thing is this theme so instead of being able to click on the theme we have to manually kind of type it out using this syntax here so I went with this Jekyll theme minimal so that's what we're seeing here it's super Sleek super nice I think it makes a lot of sense for a portfolio but if you don't like that for every reason there are a bunch of other supported themes for example let's click on architect and we can see what that looks like so that looks like this a little different different design but again this will be generated automatically based on your readme file in the GitHub repository so I'll share this link it's pages.github.com themes and I'll share it below if you want to explore the other theme options okay so we have our title we're going to have show downloads we're going to just comment this out we don't really need that and then we're just left here so really right now all you need is the title and the theme that you want to choose you go ahead and hit commit changes and so now you have two files in your repository you have your readme file and your config file and now you can just start adding stuff in here so like data scientist education work experience uh data scientists full of points here we go big impact project one big impact project two let's see what else is good to have here education your work experience what else do I have projects yes that's important so projects see uh and then so on and so forth so you can start just building some stuff out I'm just doing this so when we spin up the website something appears so just through something really simple together here and so still we just have a readme file in this simple config.yaml file and then the last step is to go to settings here we're in settings then we're going to scroll down to pages and then we have this section here build and deployment so here we're going to leave the source as is we're going to deploy from a branch and then under Branch we're just going to hit Main and then we're going to keep it as root because we want GitHub pages to look at the readme file and the config file in our root directory and we're going to hit save so now notice that we didn't have that super simple GUI to select our theme and then if you hit add a jackal theme you get some instructions here but I got tripped up on theme theme name because this isn't the right syntax here so this uh typo here it's uh made this like really straightforward task like a two-hour task for me so you can put just Minima or minimal here you have to put Jekyll Dash theme Dash Minima so if you were in a similar boat and you were struggling for hours like me it's just a simple syntax issue and that's why the website's not working anyway going back so if we go here to actions we see it's already been built but if we came back earlier we would have seen this is like processing and something's happening but we built Pages was built and deployed so if we click over here to deployment we can hit this view deployment option here and it'll take us to our website so oh look at that then you can look in your search bar what the website name is so for me it's shaheent.github.io portfolio and then if we look over here we see our readme file built out we have data scientists which was in the title so I'll go back so we can compare so we have our readme file here so data scientist education work experience projects EEG band discovery that looks a little different but looks better here on the website and then we just build it out and so we can kind of do it on the cooking shows where I don't walk through the process here and just kind of jump to the final product and so going to this example portfolio repo here are a few simple things to do one I'll definitely add a assets slash image path and then dump all the images you want to use in your portfolio here going back next big thing is so we'll go ahead and edit this readme and so configuring a readme file is a lot easier than building a website with HTML you can just copy paste this or clone the repo and use this as a starting point you can customize the structure you can start completely from scratch whatever makes sense for you just a couple things you know you can add links here so this is the standard GitHub syntax you just say what you want the text to appear as and then this is what the link is that people click on if you want to add images you put the title of whatever image and then you put the relative path to the image so again it's in this assets image subdirectory and then that's the file name and then so on and so forth so really go to town on this you know you can spend probably hours just tweaking and fine-tuning it but this is hopefully a nice jump start and it'll get you something pretty close to a final product okay so I rambled on a bit at the end there but hopefully this is helpful you know it's hard to get that first job and bring to the data science field so I hope this kind of accelerates that process for you and if you enjoyed this video please consider liking subscribing and sharing the video with others if you have any questions about like building a portfolio or like getting a job data science you know feel free to drop those in the comment section below I do read all the comments and I try to respond to all the questions that I receive and as always thank you for your time and thanks for watching
4vvoIA0MalQ,2023-03-18T23:34:38.000000,Dimensionality Reduction & Segmentation with Decision Trees | Python Code,hey everyone welcome back I'm Shaw and in this video I'm going to continue the series on decision trees and talk about a couple of applications so in the last two videos of the series we talked about how we can train predictive models using decision trees so in the first video we just talked about models employing a single decision tree then in the second video we expanded this idea to tree ensembles so if you haven't already be sure to check those out because we're going to build upon those ideas in this video the whole point of the discussion today is that we can use machine learning models specifically decision trees for more than just making predictions so this is what I'll call Next Level uses of decision trees not because they're anything profound or groundbreaking but because they go beyond this obvious task of just using a machine learning model to make a prediction and I think for those who are just getting started in data science it may be easy to think that all there is to data science is getting some data training a model and making predictions and that somehow through this process there's going to be immediate real world impact in value and the reality is it's not so straightforward so what I really like about data science is the critical thinking and the creativity that's required to use these tools and techniques to solve real world problems and provide value and so that's all I mean by Next Level so toward that end in this video I'll talk about two ways we can use decision trees for more than just making predictions so the first is reducing predictor count and the second is called predictor segmentation starting with the first one reduce predictor count so this goes back to the previous video of the series where we talked about tree ensembles where we stitch together a bunch of decision trees which made our machine learning model more robust while decision tree ensembles give us so many great things like I was talking about in the previous video all these great things come at a cost which which is that tree ensembles are a bit of a black box so we know what we put into the tree Ensemble and we can see what comes out of it but what happens in between is a bit of a mystery and this is a problem for a lot of machine learning models we may get this great predictive performance but a lot of times it's not so easy to interpret what's happening under the hood however we don't have to use our tree Ensemble to make our final predictions rather what we can do is take the feature importance ranking from our tree Ensemble model and use that to inform a simpler set of predictor variables and so this is all motivated by Occam's razor which is a very popular idea and in this context what it implies is that simpler models are better so let's say that we have our true Ensemble model and it has 30 predictor variables to it but following the logic here what we want to do is take these 30 variables and just keep the handful of variables that are most important so not only does this help us in interpreting what the model is doing but in a lot of cases it can actually lead to an improvement in predictive performance so walking through what this might look like we take our tree Ensemble and we spit out our feature importance ranking then what we can do is take the top predictor train a machine learning model from it so this could be a decision tree could be a logistic regression model linear model neural network it really doesn't matter what kind of model we use it for we use the one predictor to develop a model and then we assess that model's performance then we can do the same exact thing for the top two predictors and now we have a model with two variables we grab its performance metrics for three models four models and so on and so forth so we just keep doing this until we have all the predictors in our original tree Ensemble model once we go through this process we can plot a chart that looks something like this we're on the x-axis we have the number of variables included in the model and then on the y-axis we have some performance measures so here I put AUC just as an example each of these points corresponds to a different model and we can see the gain and predictive performance here so for example we kind of see that we have pretty big gains until we hit about three variables so what this tells us is maybe we don't need all six variables and we can just get away with using three of them without a major loss in predictive performance and so the upside of that like I mentioned earlier is that a model with three input variables is a little easier to interpret than a model with 6 or 60 input variables so now I'm going to walk through some example code of doing this so here I'm going to use the same data set that I used in the previous video this series so there's a bit of overlap between the code here and from the previous video so I'm not going to spend too much time on any recurring details but if you want to learn more check out that video and also the code is available at the GitHub linked down here and Linked In the description below First Step as always is importing modules this is a lot of the same stuff as the example code from the previous video with the only addition of this import here where we're bringing in a logistic regression model from sklearn and then as always we're going to load in our data next we're doing some data prep this is very similar to what we saw in the previous video this is something new where all I'm doing here is since Y is a Boolean variable meaning it can only take values of zero or one all I'm doing is switching the meaning of zero and one so originally zero meant the tumor was malignant and one meant the tumor was benign and all I'm doing with this transformation is switching one means the tumor is malignant and zero means the tumor is benign and the reason I'm doing this is that later down the line when it comes to interpreting what the coefficients of our logistic regression model mean it's just a bit more intuitive to talk about things in terms of risk of breast cancer as opposed to the opposite which would be like safety from breast cancer and then this should also be a review where we're using smoke to balance our imbalance data set so we have way more benign cases than malignant cases so all smote is doing is synthetically over sampling the minority class and then we're using this train test split function to create our training and testing data sets Okay so now we can train our random Forest so this is one of the tree Ensemble methods we saw in the previous video so we can fit that model with just a couple lines of code so next we have something new everything up until this point we basically did in the previous video but now we're kind of going into some novelty so all we're doing here is pulling the feature importances from our random forest model and then we're sorting them in descending order so what this looks like is this where these are all the names of our features and these numbers here quantify their relative importance and so now what we can do is exactly the process I was describing before or retrain a model using the top predictor and assess its performance and we train another model using the top two predictors assess the performance top three sets performance so on and so forth so what this looks like in code could be something like this where we're initializing all these lists to store our classifiers and then to store our different performance measures we can ignore this I equals zero here this is just left over from an earlier version of the code what we're doing here is for I corresponding to the number of elements in this series we're going to go through one by one and do the following block of code so what we're doing here is listing the feature names up until I plus one we train our logistic regression model using this line of code and then we're just appending things to these lists from before so we're pending the classifier to the classifier list we're appending the AUC value for the training data set and then we're appending the AUC value for the testing data set if this is confusing and complicated don't worry what matters is this final result which is just like what we saw before which is our number of variables plotted on the x-axis and then the performance of the models on the y-axis so this this red dashed line is the AUC value for the random forest model we trained originally so this is a tree Ensemble model that uses all 30 predictor variables but what's really remarkable is that once we hit five variables the logistic regression model actually outperforms this more sophisticated model that uses six times as many variables and then you can see after five variables the logistic regression models just keep getting better and better but let's say for our purposes we really value being able to interpret what the model is doing as well as the model's accuracy so let's say once we beat our random force model we're satisfied so that's the model we're going to use and then since logistic regression is a linear model we can easily interpret the relationship between our predictor variables and the target variable by looking at the model coefficients and so the bars here are just showing the coefficient values looking at worst perimeter which is about 0.3 the way to interpret this is a unit increase and worst perimeter translates to a 0.3 increase in the log odds that the tumor is malignant so I know that was a mouthful making that a bit more qualitative as the worst perimeter increases the probability that the tumor is malignant also increases so now we kind of have the concrete quantification of the interaction between our predictor variables and the probability that the tumor is malignant and so there's a small technical detail here that I don't want to spend too much time on but I talk about more in the blog and it has to do with the resampling since we use mode to synthetically oversample the minority class we can't immediately translate our logistic regression model outputs to probabilities and that's just because the y-intercept for our logistic regression model is biased due to the over sampling and so there's a simple fix there we can just adjust our y-intercept to make it not biased and then everything works perfectly and so if you want to learn more about that check out the blog okay next we have predictor variable segmentation and this actually goes back to the first blog in this series where we used a decision tree model for sepsis survival prediction and there the final decision tree we had looked like this and what's interesting here is even though we had three predictor variables the vast majority of these splits are only using age so here the initial split is splitting on ages less than or equal to 58.5 years and then 44.5 78.5 56.5 67 86 so this is really interesting what this is indicating is that when it comes to sepsis survival age is the most important risk factor we have in our data set and so the other ones we had were the sex of the patient and also the number of previous sepsis episodes and so sometimes in cases like this where there's one predictor variable that has this outsized impact on our Target it can make sense to do segmentation on that predictor variable and what that means is all we're doing is taking this continuous variable age and we're going to partition it into discrete sections so kind of looking at this visually let's say we have ages on our data set ranging from zero to a hundred all segmentation does is split these ages into some number of subcategories so let's say we want to split it into five subcategories and then the result looks like this and so what you can do now is instead of training a decision Tree on all of your data you can train separate decision trees for each age group and what this can translate to is better model performance especially if there are systematic differences between these age groups which requires separate model development so the question is how do we come up with these segments so we can definitely do it manually so we just kind of look at the data and say okay let's do this age group and that age group or use some kind of subject matter expertise but another way we can do it is using a decision tree so this picture here is showing how we can come up with these segments using a decision tree you notice that age is actually being split into these different sections based on the sepsis outcome of dead or alive but maybe we wouldn't want to use this decision tree directly because it has this other variable involved in the splits so what we can do is train another decision tree model but now instead of using the three predictors of age sex and number of sepsis episodes we can just use the one variable we care about which is age so now I'm going to walk through what that looks like I'm going to use the same data set from the first video of this series and then as always we're going to start by importing our modules so these shouldn't be anything new next we're going to load in our data just like we did in the first video and then we're going to do some some data prep so here all we're doing is keeping the variables of age and the sepsis survival flag and now we're going to do a little bit of data prep so what we're doing here is we're grouping the data based on age so you can imagine that we have all these different patients and there can be multiple patients of the same age so all we're doing here is reshuffling the data to have only unique age values but then for each of these unique age values we're going to have a percent of patients that are alive and we're going to name this column percent alive and then on the flip side we can take 1 minus the percent alive and create a new column called percent not alive and so the result of that is a data frame that looks like this so now we only have unique age values starting from zero going all the way up to a hundred and then for each age value we have the percentage of them that are live and the percentage of them that are dead next all we're doing here is grabbing the variable names and creating separate data frames for our input and Target variable so here the predicted variable is age the target variable is going to be percent not alive and as a first pass to the relationship we can just plot them against each other it's on the x-axis we have age and on the y-axis we have percent not alive so as percent not alive goes up that's the indication that the risk of sepsis increases so you can see around midlife there's this clear uptrend of the percent of patients that are not surviving their sepsis episode but before that this risk is relatively low and stable and so just looking at this plot we could probably chop up this data into any number of segments based on this risk so maybe we would do like zero to 40 40 to 60 60 80 100 like whatever but this is just us eyeballing it and it'll be interesting to compare this intuition to what the decision tree is going to spit out okay moving forward we can now train in our decision tree model so here we can Define our number of bins by controlling the maximum number of leaf nodes in our decision tree regressor and the reason this works is that as we saw in the first video a fully grown decision Tree on this data is just massive so you can virtually have any number of bins that you like and it'll work and then finally we just fit our data to the decision tree and now with the decision Tree in hand we can go in and grab all the split values in an automated way so this code is a bit involved so I won't spend too much time on it but for those who are curious you can take a look at it here and it's also available at the GitHub repository linked here but the final result looks something like this so here we have the same plot from before where we have age and years plotted against the percent not alive and so this is qualitatively pretty similar to what we were talking about before like maybe we would have put one here and then adjusted the rest but this is kind of a tricky problem because if I shift this border from here to here to make this first bin look a little better now this bin may not look as good because now you're mixing together these lower risk patients with these higher risk patients and then from a treatment standpoint that may not make a whole lot of sense that's one of the upsides of using a decision tree and leveraging that greedy search to Define these bins because it already is doing that tricky optimization for us and then as a final note I'll just say to take all these with a grain of salt just because the decision tree spits out these optimal age buckets this may not translate well to treatment strategies and so as opposed to just taking this as gospel this is more of a starting place and may just serve better than just arbitrarily drawing lines for these different age groups okay that's it so if you want to learn more be sure to check out the blog published on medium and Linked In the description below feel free to steal the code from the GitHub repository and apply it to use cases or projects that you're working on if you if you enjoyed this content please consider liking subscribing and sharing your thoughts in the comments section below I do read all the comments and I find all the questions and feedback that I receive very valuable and as always thank you for your time and thanks for watching
ZaXpMou55lw,2023-03-03T14:22:24.000000,10 Decision Trees are Better Than 1 | Random Forest & AdaBoost,hey everyone I'm Shaw and in this video I'm going to continue the series on decision trees and talk about decision tree ensembles so instead of just using one decision tree a tree Ensemble combines a collection of decision trees into a single model so with that let's get into the video so like I just said a decision tree Ensemble is a collection of decision trees which are combined into a single model so if you recall from the previous video we saw decision trees where a way we can make predictions through a series of yes or no questions and they look something like this you start at the top node here and just follow the arrow heads based on predictor variable values which eventually lead you to your final prediction on the other hand a decision tree Ensemble will look something more like this so now instead of a single decision tree we have multiple decision trees each giving a prediction and then we can combine these predictions together to give us our final estimation and the key benefit of a decision tree Ensemble is that it generally performs better than any single decision tree alone and we'll kind of touch on why this is the case a little later in the video but first i'm going to talk about two different types of decision tree ensembl the first one is bagging which is short for bootstrap aggregation or bootstrap aggregation bootstrapped bootstrapped aggregation first short for bootstrapped aggregation and then the second one is called boosting which is isn't short for anything starting with bagging here the idea is to train a set of decision trees one at a time by randomly sampling with replacement so what the heck does this mean so we'll just walk through this one step at a time say we start with our training data set T KN and each of these blocks here represents a different record or example in our training data set so we have record one we've record two 3 four five what we do here is we create another training data set by randomly sampling T not with replacement what this might look like is this where we randomly pick five records from the original training data set and so notice record number three actually shows up twice in this training data set this just follows from sampling with replacement so basically all that means is every time we pick a record from T for T1 we will replace it before making a second pick so we have this new training data set T1 we can just do the same thing and we get T2 so let's say this this time we really got a lot of twos through this random sample and then so on and so forth and then let's say the nth training data set looks something like this so now notice instead of just a single training data set we have a collection of training data sets which allows us to train a collection of decision trees so that could look something like this and then just like we saw on that first slide we can combine the predictions from each of these decision trees to produce our final prediction so one of the most popular machine learning algorithms that uses bagging is called random forest and I'm not going to get into all the details of that in this video but for those who are interested be sure to check out the blog published in towards data science where I give a few more details about random Forest additionally there's a really nice paper by the creator of the random Forest algorithm Breeman who is very well known for his work on decision trees and decision tree ensembles so I definitely recommend reading that paper if you're into that kind of stuff the second type of decision tree Ensemble we're going to talk about uses something called boosting so boosting is completely different than bagging so here we will recursively train decision trees using an error-based reweighing scheme no worries if this doesn't make any sense we're going to walk through what I mean by this one step at a time so again imagine we start with this training data set T KN but now we're going to introduce this concept of weight and this is exactly what it sounds like essentially we can give different different records in our data set more weight or more importance when it comes to developing our model so we'll just start with T KN in such a way that all the weights are equal so every record every example is equally important and then we can use this training data set to create a decision tree we'll call it hot but now we can create another training data set based on the performance of this decision tree and so that might look something like this so notice the different color here all this is showing is that records 1 and four were correctly classified in this binary classification problem we're trying to solve while records 2 3 and five were incorrectly classified and so what we can do now is we can decrease the weight of Records 1 and four and increase the weights of 2 three and five and then with that we have this new training data set T1 and we can train a new decision tree we'll call it H1 and then we can repeat this process we evaluate the predictions of H1 we see which records were correctly predicted which records were incorrectly predicted update their weights accordingly create another decision tree and so on and so forth and we can do this for however long that we want now notice again we have a collection of decision trees and we can just aggregate the predictions of these decision trees into a single estimate the first technique that really introduced this idea of boosting is called adaboost or adaptive boost and so when people are talking about boosting they're typically talking about a process similar to what we see in adab boost which is essentially what I walk through here just explaining some of the details of it more here so basically all that adaboost does is it combines each of these decision trees into a linear model and weights each of the decision tree predictions based on this Alpha value and then the alpha value is just proportional to the decision tree's performance and here what I've written out is the specific reweighting scheme used in adaboost so notice that incorrectly classified records will get a weight update proportional to this value while correctly predicted records will have their weight updated proportional to this Factor over here ever since adab boost was introduced in the mid 90s there have been two major Innovations around this idea of boosting the first of which is called gradient boosting so instead of talking about the specific reweighing scheme and details of adab boost gradient boosting just provides a more generalized framework where you can take any differentiable loss function and Define its gradient and Define some boosting strategy from it so the second major Innovation comes from a library called XG boost which basically makes the gradient boosting idea much more scalable and computationally efficient through a set of different heuristics and so that's all I'm going to say about those I talk a little bit more about grum boosting and XG boost in the blog associated with this video and I also have the original reference for those ideas in the blog as well so now coming back to this question of why are decision tree ensembl better than just single decision trees and if I were to summarize everything into a single picture it would be something like this we're essentially going away from point estimates toward population estimates so what I mean by this is instead of just having a single number as our prediction from our decision tree we now have a population of predictions from our decision tree Ensemble which has these three main benefits that I'm going to talk about now so the first key benefit is that decision tree ensembles are much more robust to the overfitting problem than single decision trees so if you saw the previous video of the series we saw that overfitting is when your machine learning model essentially over optimizes to a single training data set in such a way that when you try to apply it to new data it doesn't work as well this turns out to be a pretty big problem for just single decision trees but this problem for a lot of cases tends to go away when you start aggregating groups of decision trees together the second key benefit of decision tree ensembles are more robust feature importance rankings so importance rankings are a critical output of any decision tree based method and so these can be based on things like Information Gain or out of bag error if we're talking about random forest or any number of different ways that we want to Define importance some of these quantities that we can use to define importance are only possible through tree Ensemble based approaches so one example is outof bag error defined in the random Forest algorithm so I won't get into all the details of that if you're interested I talk a little bit about it in the blog but all that to say is that Tre Ensemble based approaches not only open up more ways of defining importance but now kind of going back to this idea of population estimates we're not just relying on the importance of our features from one view of the data essentially from one decision tree but through having a wide collection of decision trees our importance rankings can become more robust and then finally the last key benefit of decision tree ensembles is through population estimates we now have a pretty straightforward way to quantify our confidence or uncertainty in our models predictions and anytime you want to use your model in the real world or there are physical consequences for your model it's good to have some measure of confidence or uncertainty just so you know your exposure while if you just have a point estimate you don't really know the confidence of your prediction it could be zero uncertainty or it can be infinite uncertainty so that's another case where population estimates are very beneficial okay now we're going to jump into some example code so here we're going to do breast cancer prediction using decision tree ensembles like always we're going to use the sklearn python Library which is one of the most popular machine learning python libraries there is and then the data that we're going to use for this example comes from the UCI machine learning repository so the first step is we import our python libraries so just kind of running through quickly we have pandas to help Wrangle our data numpy to do some math mat plot lib will help us make some nice visualizations SK learn data sets so this is the data set while it's originally from the UCI machine learning repository sklearn has this data set readily available for us and this is my short apology for using a toy data set and not wrangling a data set from The Real World but the point of this is to focus on the tree ensembles and not the data preparation step so I hope you'll forgive me next I imported smoke so this is optional I have it commented out for the results we're going to see here but if you're interested head over to the GitHub uncomment this code block and you'll be able to see what the results are doing here and if you recall we use SME in the previous example to balance our imbalance data set and then finally we import a whole bunch of other things from sklearn this handy function to create a training and testing data set decision tree classifier and then we import all the different tree Ensemble approaches that we've talked about so Random Forest addab boost and gradient boosting and then finally we import three different evaluation metrics for our decision trees basically what we're going to do in this example is we're going to train four different models using these four different approaches and we're just going to compare their performance sklearn makes it super easy to import the this toy data set just one line of code we have it in a pandis data frame and then it's always a good practice to plot the histograms of your data here are all the predictor variables we have at our disposal and then here is our Target variable so this kind of goes back to the imbalanced data set idea we see that there are a lot more cases where the breast tumor is benign as opposed to malignant and so while we could apply SME here to synthetically over sample the minority class we're not going to do anything here and see how the 4 different models hold up okay next we Define our predictor and Target variables so this is basically grabbing everything but the last variable name in our data frame this is grabbing the very last variable name in the data frame and then this is just creating two data frames based on the variable names and then with that we can easily create our training and testing data sets here we use a 8020 split SK learn makes this super easy a bit of a warning with this next block of code because I just inherently refuse to copy and paste code over and over again I use what I've heard referenced as automatic code generation and basically all that's going to happen here is instead of explicitly writing the python command out and then copy pasting changing one thing copy pasting changing one thing and so on you can Define your python command as a string and then use this handy execute function to execute the command so while this might conceal kind of like what's going on here this is just a much cleaner way and convenient way that I found to write code and I'm sure there are going to be some programmers out there that are going to yell at me for doing this but I haven't run into any major issues writing code this way so be curious to hear other people's thoughts on this while it might conceal kind of what's going on here I have everything printed out so essentially what's being dynamically written here is this single line of code so all that's happening is four different models are being created using the four different things we imported from SK learn so decision tree classifier is our lone decision tree the random Forest classifier uses random Forest add a boost and then gradient boosting so all these different models are initialized in this clf data structure and then each of these are stored in a list so now we have a list of models as you can see here the automatic code generation gets even worse here because we have a lot of combinatorics happening so we have four different models we have two different data sets and we have three different performance metrics we want to Define for each of these cases so this code block may not make a whole lot of sense but I printed everything that's being dynamically written here so let's just look at these first three lines so all that's happening is we're going one model out of time for the models in our list and we're going to apply it to the training data set and get a prediction and then we're going to compute the Precision recall and F1 for this model applied to the training data set so that's what these three lines of code do here and then we do the same exact thing with the same exact model but now for the testing data set so we get a prediction compute the Precision recall F1 score and we just append everything to the same list and so on for each model the results get stored in this performance dict it's just a dictionary that we initialized here where the keys are the different model names and the values are all the performance metrics relevant to that model and then after all that this dictionary gets is all filled up for all four models for all three evaluation metrics and for both data sets and we can just convert it all to a pandas data frame and so if this is all confusing and doesn't make any sense don't worry about it doesn't really matter what matters is this final output here where we can just simply look at all four of our models and all the different performance metrics that we have and just compare them together so we can see that all four models performed perfectly on the training data set so we can see the Precision recall F1 score for the training data set is one but the real test is looking at the performance metrics for the testing data set and so in this context we can use as a rule of thumb the difference in performance between the training and testing data sets is indicative of overfitting put that more simply the smaller these values are the more overfitting that model is showing based on that her istic we can see that the decision tree classifier seems to be overfitting most because it has the worst performance when applied to the testing data set so on the other side of it random forest and gradient boosting seem to have the best performance when looking at the F1 score a close second is adaboost which has F1 score of 0963 and so these results make sense they agree with this story and intuition that tree ensembles are more robust to overfitting than single decision trees alone so two things I did not explore in this example which are obvious next steps are looking at the feature importance rankings for all four models and then addition additionally doing some kind of uncertainty estimate for each of these models okay so that's basically it if you enjoyed this video and you want to learn more be sure to check out the blog published in towards data science Linked In the description below again feel free to steal the code from the GitHub repository referenced here also please consider liking subscribing and sharing your thoughts in the comment section below and as always thank you for your time and thanks for watching
B6a64wdD7Zs,2023-02-18T17:30:14.000000,An Introduction to Decision Trees | Gini Impurity & Python Code,hey everyone I'm Shaw and in this video I'm going to be giving a brief introduction to decision trees so we might ask ourselves what are decision trees and put very simply a decision tree is something we can use to make predictions via a series of yes or no questions so let's look at a concrete example so let's say we want to predict whether I'm going to drink tea or coffee and to make that prediction we can use a decision tree like the one shown here and so the way this works is we start at the top of the decision tree here and we just answered the following yes or no questions so first we ask is it after 4 pm and if yes we follow this Arrow here and if no we follow this Arrow here so let's say yes it is after 4 pm then our answer is I will drink tea but if the answer was no we would follow this arrow and end up at another yes or no question so this one asks if I got more than 6 hours of sleep last night if the answer is yes we go with t once again but if the answer is no we're going to go with coffee so this is a very simple and straightforward way we can make predictions using just two pieces of information namely the time of day and the amount of sleep I got last night and so just to talk a little terminology because it will come up later these rectangles that we're seeing throughout the decision tree are called nodes and we have different types of nodes so for example the node that sits at the top of the decision tree is called the root node over here in green we have what is called a leaf node and these are nodes that I don't ask any yes or no questions and where we can assign our prediction and then we have splitting nodes which ask yes or no questions but are not the root node and so another way to think about decision trees is graphically and I personally find this a much more intuitive way to think about things so here on the right we have the example from before where we have two pieces of information namely time of day which we're plotting on this x-axis and the amount of sleep I got last night which we're plotting on the y-axis so another way we can represent what the decision tree is doing is by partitioning this predictor space meaning the space defined by R2 predictor variables time of day and hours of sleep partitioning this predictor space into different sections and assigning a label to each section so what this will look like for splitting on 4 pm and 6 hours of sleep is something like this so here's 4 pm we draw a line here and then here's six hours of sleep we draw another line here and now we just look at the leaf nodes for each of these splits and assign a label to each section so intuitively this is all a decision tree is doing it's taking the predictor space splitting it into the different sections and then assigning a label to each section so now that we have a basic understanding of what decision trees are and and an intuition for how they work a natural question is how can we bring this into practice namely how can I use a decision tree in the real world and so this is a great question and as it turns out we can use decision trees in practice by developing them from data so put another way we can learn decision tree structure from data so I'm going to walk through an example here just to give you a qualitative sense of how this works and I'll just kind of start with the disclaimer that there are many ways to grow decision trees from data but what I'm going to describe here is a widely used methodology all right so before getting into it I need to introduce the concept of Genie impurity and so I'm just throwing the equation up here for completeness and for those that I think in terms of math and just describing what this is it's saying that the genie impurity of a sample s like this sample right here is equal to 1 minus the sum over p i squared and so Pi corresponds to the probability of the I class looking at this example here we have two possible classes tea or coffee so the genie impurity of this sample here would simply be 1 minus the probability of t squared minus the probability of coffee squared and if that doesn't make any sense no worries we can think of the genie impurity in terms of its extremes namely its minimum and maximum value so visualizing this we have minimum impurity whenever every class in our sample is identical so either every class in the sample is T or every class in the sample is coffee on the other end of the spectrum we have maximum impurity when each class is equally likely and so for those of you familiar with information Theory or the concept of entropy you'll notice that this quantity Genie impurity is actually proportional with information entropy okay so you might be saying Xiao why are you talking about this Genie impurity what does this have to do with decision trees and I'm glad you asked that because we can use the genie impurity to learn decision tree structure from data and so the goal when growing decision trees is to use our predictor variables to split our data such that the overall Genie impurity is minimized so essentially growing a decision tree is an optimization problem and in the following slides I'm going to walk through a popular way of doing this so just putting aside our minimum and maximum impurity just as a reference and then consider this data set on the right where each of the rows or records of this data set are represented in this sample over here so each icon in this box corresponds to a different record in this table so let's say like this icon corresponds to the first row with this T this coffee icon corresponds to the second row and so on and so forth so again our goal is to use our predictor variables time and amount of sleep to split our data such that we minimize the overall Genie impurity a sort of Brute Force way of doing this is evaluating every possible split option that we have in our data set for example consider time we can just go through one record at a time and split based on each value we observe in this table so for example we take this first value of 721 am and we can split our data based on time being less than or equal to 721 am and the resulting split would look like this so here we have a sample with just one record this first one here and then we have everything else in this other sample here and then we can evaluate this split option by Computing its Genie impurity so basically what I mean by that is we calculate the genie and purity of this sample and the genie impurity of this sample and then we take their weighted average so here we just have one class so that is actually minimum impurity of zero and then this one is a bit of a mix so it's going to be pretty close to maximum impurity and then we will wait the average by the number of Records in each sample so this one will have a very low weight because it only consists of one record and then this one will have a very high weight because there are a lot of Records in this box over here so this split will give us a number corresponding to its genium Purity and then we can just continue this process and so now we split on 8 47 am calculate the average genium Purity we split on 9 30 am calculate average Genie impurity and so on and so forth for every possible value of time in this data set and then we do the same thing for amount of sleep we look at our first option which is 5.5 we get something like this 5.9 595 and so on and so forth for every single value we observe in our data set and so let's say after doing this and calculating all these average genium Purity values we discovered that the split option of sleep less than or equal to 6.75 hours is the optimal value this gives us the smallest Genie and purity of all the different split options that we observe in our data set and so now notice this node over here is pure it has minimum impurity so it doesn't really make sense to split this sample further but on the left hand side we still have some impurity in this node and we can do additional splits so let's do that here so now we have a smaller data set so instead of starting with all the data in our table we just have a subset shown by this smaller table over here and then we just repeat the same exact process as before we evaluate every split option let's say that after evaluating all the split options we discover that splitting on time less than or equal to 145 PM gives us the smallest Genie impurity and now notice again we have a pure node here but we still have some impurity here so we can just keep splitting the data until every single node is pure so meaning every single node just has a single class in it and it has Genie impurity equal to zero so at first while this might sound great you might think oh we can have a perfect classifier we can have a decision tree that is absolutely perfect however this is not such a great idea because this brings up a very well-known problem in machine learning known as the overfitting problem and overfitting is when you learn a machine learning model based on some data set but your model becomes over optimized on the data set it was trained on and when you try to apply that model to new data that it's never seen before you'll find that your model is actually very inaccurate and so instead of allowing our decision tree to grow without end and become hyper optimized to our data set we can control the growth of our decision tree using what are called hyper parameters put simply hyper parameters are values that constrain the growth of our decision tree and so just to look at a few examples let's say we have this original decision Tree on the left hand side but we find that it doesn't generalize well to different data sets and we actually want to tune it to this simpler decision Tree on the right hand side so in order to do this we can actually use hyper parameters and so here I'm going to show three different hyper parameters we can use to go from this original decision Tree on the left to the tuned decision Tree on the right so first we have the maximum number of splits so in the original decision tree you see that we have two splits happening but we could have easily constrained the size of this decision Tree by setting the max number of splits equal to one another hyper parameter we could have used was the minimum Leaf size so in the original decision tree we have a minimum Leaf size of two but if we would have set the minimum Leaf size to something like five this additional split could have never happened and then finally we could have controlled the number of splitting variables so in the decision tree from the previous slide we split on both hours of sleep and time of day but if we set the number of splitting variables to 1 decorative constrained our decision tree to the sex and so the key point is hyper parameter tuning can help avoid this overfitting problem and improve your decision trees generalizability so its ability to perform well on new data and then as a final note although this is a very widely used way to develop decision trees this is not the only way to develop decision trees and I talk a little bit more about alternative strategies for developing decision trees in the blog associated with this video so if you're interested in that be sure to check that out okay so with the theoretical Foundation set let's dive into a concrete example with code and data from The Real World so here we're going to do sepsis survival prediction using decision trees and so here we're going to use the scikit-learn python Library which is a very popular machine learning library in Python and then we're going to use a data set from the UCI machine learning Repository all this code that I'm going to walk through here is available in the GitHub repository which I will also Link in the description below the first step is we're going to import some helpful python libraries so pandas is going to help us with formatting our data numpy is helpful for doing some math and calculations we use matplotlib to make visualizations we import several things from sklearn and then finally we're going to import this smote function to help balance our data set which we will talk about here soon okay so with our libraries imported we can read in our data set so with pandas this is just one line of code and the CSV file used here is available at the GitHub repo as well as two additional CSV files that can be used for validating our decision tree okay so with our data read in we can plot the histograms for every variable in our data set so here we just have four variables the age of the patient whether the patient is male or female the number of sepsis episodes that the patient has experienced and then finally the outcome variable which is an indicator of whether the patient survived or died and so the first thing that we should notice here is that we have a imbalanced data set so what that means is we have a lot more patients that survive than that diet well this is a good thing from a human perspective this is not a good thing from a model development perspective because if we train our decision Tree on this data directly basically what will happen is that our decision tree will overestimate the a live class and underestimate the dead class and so one way we can correct this is using smote which stands for synthetic minority class over sampling technique I think I got that right and it's basically a way to over sample the minority class to make it more Equitable with the majority class and ultimately reduce bias in our decision tree so this is pretty straightforward so here we're just grabbing the predictor variable names and the outcome some variable name here we store the predictor and outcome variables into two pandas data frames namely X and Y and then finally with just one line of code we can use smote to over sample the minority class and then we can plot the results using matplotlib and look at that we have a more balanced data set all right so now that we have balanced our data set we can create our training in testing data sets and so basically the point of this is our training data set will be used to grow our decision tree and then the testing data set will be used to evaluate its performance and so here we use the 80 20 split so 80 of the data is used for training 20 is used for testing and then with that growing the decision tree is very straightforward we can do it with just two lines of code as we do here so first step is we initialize the decision tree classifier and then the second step is we fit our decision tree to our data and then that's it so we have our decision tree we can take a look at it using this built-in functionality inside like it learn and this is what it looks like needless to say this is a very big decision tree and it's hard to think that a doctor or any medical staff will be able to interpret this decision tree to extract anything meaningful but let's just put that point aside for now and evaluate our decision trees performance and so the way we could do this is using a confusion Matrix and so just looking at this one on the left what this is showing is the number of troop negatives true positives false positives and false negatives so in other words this is just comparing the decision trees predictions to the ground truth and so I don't want to get into too many details of interpreting confusion matrices and whatnot for this discussion I'll say when it comes to confusion matrices you generally want to maximize the diagonal elements and minimize the off diagonal element what that means is we want our predictions and the ground truth to agree as much as possible and we want them to disagree as little as possible possible so we see for both the training and testing data sets the performance seems reasonably well and then another way to evaluate performance is using three different metrics namely Precision recall and the F1 score which are defined by these equations on the left here so Precision is basically the number of true positives scaled by the sum of the true positives and false positives recall is a similar kind of thing but it is scaled by the number of true positives and false negatives and then the F1 score is the harmonic mean of the two and so in this case I'd say the Precision is something we care more about than recall because in this context we probably care more about false positives than false negatives and so the reason being is a false positive corresponds to the case where the decision tree predicted that the patient would survive and they did not and so for using this decision tree to quantify patient risk then there's a lot more downside to predicting that a patient would survive five that didn't then predicting that a patient would die who doesn't and so clearly which one of these metrics you want to look at and care about is highly context dependent sometimes you care equally about false positives and false negatives sometimes you care more about false negatives sometimes you might care more about false positives like the case here and so which metric you use to evaluate your model depends on the problem and the context you're looking at and then here's a handy function available in the GitHub that generates all this okay so coming back to this massive decision tree from a couple slides ago this brings up once again the overfitting problem so while this decision tree might work reasonably well on the data set here a decision tree that looks like this is prone to overfitting meaning it may not generalize well to new data sets and So to avoid this problem we can use hyper parameters and so here we're just going to use one hyper parameter which is the maximum depth and so here we're just going to set that equal to three so we ensure that the decision tree doesn't get too many branches setting this is super easy with sklearn we just pass this input argument into our decision tree classifier and then we fit our model just like we did before and then outcomes our tuned decision tree and so plotting out the decision tree it looks like this and already we can see this is much more interpretable we can actually read the text here and so just kind of looking at all these different splitting nodes we're seeing the age predictor is appearing a lot so this is indicating that age seems to be a very important risk factor when it comes to sepsis survival prediction and then also right here we're seeing that sex is playing a role which is a little surprising that we're not seeing number of episodes and surely if we were to increase the max depth or do some other hyper parameter tuning we would see that variable appear in additional splits okay so our hyper parameter tuned decision tree seems more comprehensible but how does it perform and so we can once again look at the confusion Matrix and those three performance match tricks and surprisingly the Precision is actually a little better for this hyper perimeter tune decision tree than the fully grown decision tree but notice that the recall in F1 scores are significantly lower than what we saw before but I would say in this case we may not care about that because Precision like I was saying before might be the metric that we're really trying to optimize in this context because we likely will want to wait false positives more than false negatives so that was a tremendous amount of information if you still want to read more check out the blog published in towards data science on medium be sure to check out the GitHub repo and steal the code and train your own decision tree model with your own hyper parameter optimizations and if you enjoyed this content please consider liking subscribing and sharing your thoughts in the comment section below and as always thank you for your time and thanks for watching
CTu8JNLq5ZU,2023-01-28T15:06:57.000000,5 Reasons Why Every Data Scientist Should Consider Freelancing,are you feeling stuck you got the degree you got the job you're working in data science you're killing it but for some reason you don't feel like you're progressing in life you're not growing you're not developing the skills that you want to develop or maybe you haven't even gotten the job yet you're desperately applying the jobs you're getting scared all the tech companies are laying people off what am I supposed to do about this how am I supposed to get a data science job with just a bachelor's degree with my Master's Degree with my PhD even if you resonated with any of these random points I just spat out at you then this video is for you hey everyone I'm Shaw and as someone who has worked in data science as both a freelancer and a full-timer I just wanted to share a little bit of my experience for those pondering and reflecting on their data science journey and trying to figure out where they want to go with it toward that end in this video I'm going to break down five reasons why every data scientist should at least consider freelancing so for my view these points are beneficial to anyone in data science but perhaps especially so for those just getting started in my personal experience freelancing accelerated my development as a data scientist and it played a big part in helping me get my current full-time role as a data scientist and even if your goal isn't to get a full-time gig at a company freelancing can serve as a main source of income or even give you insights into different industries that might help you develop a new business or a minimum viable product so if you like this content and you want to see more about data science productivity entrepreneurship go ahead and hit that subscribe button right now so the YouTube algorithm will know that you want to see this face on your computer screen and with that let's get into the Five Points okay so the first reason why every data scientist should at least consider freelancing is to work on new problems one of the greatest powers of data science that I personally just love so much is that data science is so often context agnostic so what do I mean by that basic basically all I mean is you can take one method one Technique One Piece of code and apply it to multiple different use cases so like a very simple one is logistic regression you can use logistic regression to solve binary classification problems which comes up in credit risk modeling will the person I'm giving a loan to pay me back or analyzing customer retention what's the probability that our customer will continue our services next month or even marketing analytics what's the probab ability that a user will buy our product after they watch our ad and so these are completely different context we're talking about credit risk and financial services lifetime customer value with retention analysis and then we're talking about ads and marketing with that last Point completely different context but you can use a single data science approach to solve all of these problems and these are just three off the top of my head there are countless use cases and applications for logistic regression or any common data science approach and so all that to say when it comes to freelancing you have the opportunity to use this basic toolkit in a wide range of contexts so when I was freelancing I was working as a graduate research assistant in the physics department but through my freelance work I gained exposure to different fields so one example was trying to classify sepsis subphenotypes so basically subtypes of sepsis using unsupervised machine learning techniques that I've used in count other contexts so that's one thing to consider when thinking about freelancing and data science you have the opportunity to work on different problems leveraging your experience and the skills that you've acquired in new contexts and it kind of enriches your understanding of those tools so every time you use a technique in a different context it helps you build an intuition of what else you can use that same method for so reason number two is developing soft skills so when you're freelancing you're really on your own you have to figure out how to put yourself out there how to get clients and how to communicate with a diverse set of people and so when you're freelancing you're trying to find gigs you often have to network you have to talk to people you have to connect with people and this really forces you to develop your soft skills you know sending out cold messages email etiquette talking to people at networking events reaching out to people on LinkedIn responding to potential clients reaching out to you because they see some work that you've done or they came across your profile while on some freelancing website like upwork or Fiverr and so all these interactions all these reps really allow you to develop these soft skills that you may just not have as much opportunity to develop in a full-time role where the work is more delegated to you and is more stable and you're typically interacting with the same handful of people constantly as opposed to in a freelance role you're constantly interacting with new people and brushing up on those skills so the third reason is fine-tuning your pitch and this has some overlap with the second reason it comes down to your ability to communicate and connect with people but fine-tuning your pitch is really about selling yourself and what I mean by this is fine-tuning your resumés your cover letter or proposals and your interview skills so this is another area where full-time roles and freelance gigs have a big difference and It ultimately just comes down to timeliness so for the full-time role you know you submit your resume and cover letter you spend all this time on it but it's not uncommon to not hear back from that application for weeks or even months sometimes so it's really hard to kind of gauge how effective your resume and cover letter were at conveying your skill set and experience but on the flip side in freelancing the time scale is just much faster if you apply to a gig let's say on a site like upwork the feedback is typically much quicker if someone wants to work with you you will a lot of times hear from them within a few days and if you don't hear back from them in a few days that probably means they moving forward with other candidates or the job's no longer relevant or something like that and so ultimately what this means is in freelancing as opposed to full-time roles you really can get a lot of reps in on your resume and cover letter and get much faster feedback and so what this allows you to do is fine-tune your resume and cover letter to convey your skills and experience more effectively and this is something I definitely benefited from so I was freelancing in grad school so constantly fine-tuning my resume and my cover letter and then eventually when I graduated and decided to apply for a full-time role my resume and cover letter went in a pretty good spot and I could just leverage what I'd learned from freelancing to apply to the full-time gig so I would say to anyone trying to break into data science you know you just graduated or you're about to graduate I would recommend freelancing even if you don't get any gigs and don't get any work through it at least you get these reps you get to fine-tune your resume and your cover letter and hopefully your interview skills through chatting with potential clients and you can leverage this experience and these reps and the feedback for applying to a full-time gig but overall the skill set of selling yourself being able to communicate your skill set and how your experience and skills are relevant to solving other people's problems is a very valuable skill set to have and essentially being able to sell yourself and your ideas is something that will be valuable in whatever context you find yourself in reason number four is flexibility and autonomy and so so this is one of the greatest benefits of freelancing and I feel one of the main reasons why people are so attracted to it in freelancing you essentially choose what you work on because you choose the clients that you work with and moreover freelancing gigs are typically on a much shorter time scale than full-time roles so you could be working with a client on a month-by-month basis and it could be going great for 6 months but then at a certain point the work May no longer be relevant or getting overloaded on contracts with a handful of other clients and then you have the option and opportunity to reduce your workload or refocus your efforts toward a specific type of work and also you don't just get to choose what you work on but you typically get to choose where you work how you work when you work and this is something that a lot of people have value in people who greatly value their autonomy their flexibility their freedom you know maybe they don't want to be bound to a certain city they want to be able to travel and you know this was something that got big during Co you know people were getting gigs online or during remote work they were living for months in different countries you know living in Europe or South America or Asia or something like that and so for people that that lifestyle is appealing to them freelancing is a great option for that okay so the fifth reason is networking and so I kind of touched on this before but here what I'm specifically talking about is building new relationships and new connections and so through my freelancing I've met a wide range of people that has given me insight into Worlds that I didn't even know existed so I've worked with medical doctors clinicians with people working in Special Forces military police officers business people you know so many different walks of life and backgrounds and has really enriched my own experience and my understanding of the world which I find a lot of value in relationships and learning from people is something I give a lot of weight to so this is one aspect of freelancing that I really enjoy okay and if those five reasons were not enough to make you consider freelancing in data science I've got two bonus tips to share so the first bonus tip is money so even if you don't really care about developing your technical skills expanding your experience and Horizons developing your soft skills building new relationships what else did I talk about fine-tuning your pitches your ability to sell yourself all these different reasons you could always just do it for the money and most of the time freelancing gigs are much more lucrative than full-time roles so just speaking from my personal experience before I entered into my current full-time role I had two offers on the table I had the my current role and I had a essentially a contractor role which could have been full-time and just comparing the pay of the two roles the contract role paid almost twice as much as my full-time gig I would say 80% yeah paid about 80% more than my full-time gig which is a lot of money I'm just saying that to give you an idea of how much more you could get as a freelancer as opposed to a full-time role and to those who are saying like oh freelancing is so great why didn't you take that why didn't you take the money and that was just a personal decision for me when I had graduated I had done the freelance stuff I'd worked in research but I never worked at a large company as part of like a big data science team the biggest team I had worked with was my research team which was about 12 people and I work on a data science and analytics team that's I want to say like 100 people if not more and so for me the reason I went with the full-time role is because it was a new experience for me it was also the opportunity to learn from other data scientists and data analysts that have been working in the field much longer than I have and then the last thing I'll say about the money is that you know the great thing about freelance is that you can customize the freelance workload so you can definitely be a full-time freelancer and reall the benefits there but if you're just trying to make some extra cash on the side and you have a full-time role you can probably just pick up one or so contracts every so often if you just want to make some extra cash on the side and the second bonus tip is that freelancing gives you options if you have freelancing experience or you've done it in the past you always have that option on the table so say you're working full-time and you want to make some extra cash you can always just go to freelancing or kind of given all the recent Tech layoffs it's kind of a scary thing you could just wake up one day and your full-time employer says we don't need you anymore or we don't see the value in data science anymore and they lay you off now what are you going to do well if you're freelancing on the side or a freelance in the past you have an immediate thing you can fall back on until you can either work up your client base or find another full-time role okay so that's basically it so in this video I give you five reasons why every data scientists should at least consider freelancing with two additional bonus tips and so if you enjoyed this content you want to read more take a look at the blog associated with this video published in towards data science on medium if you enjoyed this content please consider liking subscribing and sharing your thoughts in the comments section below and as always thank you for your time and thanks for watching
O72uByJlnMw,2023-01-14T15:08:23.000000,Causal Effects via Regression w/ Python Code,hey everyone welcome back my name is shahen and today we're going to continue the series on causal effects and talk about causal effects via regression this will actually be the fifth video in a larger Series so if you're just joining in now check out the other videos to get a larger context of what we're talking about here so the title of this talk is causal effects via regression so we might ask ourselves what is regression and regression is very simple it's a very widely used technique and Science and business economics progression is simply a way to learn relationships between variables using data and one of the most popular regression techniques is linear regression which you may have even seen back in high school in linear regression we use data to develop a linear model between two or more variables so a super simple example is if we have a variable X and we want to find its linear relationship with another variable y we can just Express the relationship through this equation so y = Theta x + B we know X and Y so through the regression process we actually learn values for Theta and B that best fits our data and so the result of a regression technique is what we call a model so a model is essentially something that we can use to make predictions so this is an example of a linear model so if you give me a value for x X I can make a prediction about the value of y so where the causal effects come in is if we have a linear model like this and we have a variable X and a variable y we can interpret this coefficient Theta as the causal effect of X on Y and so this is an important note here because if you've been watching previous videos of this series you might notice that this is a fundamentally different way to define causal effects as we've seen in past videos so namely before we defined the causal effect through the average treatment effect which was essentially the difference in mean outcomes for two different groups a treatment group and a control group here where xal 1 is representing the situation where someone receives a treatment like taking a pill and x equal 0 represents the situation where someone does not receive a treatment or does not take a pill for example so that's an important thing to know so in this video I'm going to talk about three regression based techniques for estimating causal effects so the first is linear regression which we've kind of already touched on but I'm going to talk more about in the next slide we have a more sophisticated technique called double machine learning and then finally we have another popular technique called metal Learners so for linear regression as we've seen here is where we train a linear model to predict the outcome variable Y in terms of the treatment variable X so again we have a linear model y = Theta x + B and so just defining some of these things Y is our outcome variable and X is our treatment variable so X could be whether someone takes a pill or not and Y can be an outcome such as headache status or something like that and then like I mentioned earlier we interpret Theta to be the causal effect of X on Y and then here we can interpret B to be an error term and so this is a very simplified view of the causal effect of X on y but we can go a step further and include confounders in our linear model so what do I mean by that so in this context a confounder let's call it Z is something that influences both X and Y so the way we can handle this situation is we can include our confounder Z in our linear model for y and then we can additionally write out a linear model for Z in terms of X or equivalently a linear model of X in terms of Z we can just move some terms around and get that equivalently but what this allows us to do is plug in our equation for Z into our equation for y and get a new equation for y that does not include our confounder and this gives us a different equation for our causal effect so no longer is our causal effect simply the coefficient of x in our linear model for y but it's got this extra term here which is representing the impact of the confounder and so this even still is a simplified example and for those who are interested there's a really nice book chapter by Andrew Gilman and Jennifer Hill called causal inference using regression on treatment variables which I will link in the comments and you can find the link in the blog associated with this video and I highly recommend this chapter it was a really helpful resource for me okay so linear regression is a very simple and easy to understand technique with which is helpful when using it in practice however the downside of using it is that since it is such a simplified technique it may not accurately capture the relationships between our variables of Interest so for example if X and Y are related by a non-linear relationship even a quadratic relationship or cubic or sinusoidal or whatever it might be and so for these situations we can turn to more sophisticated techniques and one such technique is called double machine learning and so the idea a here is we're going to estimate the treatment effect using a three-step process in which we develop two separate machine learning models and so double machine learning is a bit of a daunting technique at least it was for me when first hearing about the idea I came across the paper it's like 70 pages and just kind of reading through it's a lot of math a lot of abstract ideas but upon reading through the paper a couple times and then watching a really nice talk given by the first author uh Victor uh I'm not even I'm going to butcher this a really nice talk by Victor on YouTube which I can also link down below this whole process that may seem very daunting and complicated on the surface can be broken down into a simple three-step process so the first step is we train two regression models the first we can call the outcome model and the second we can call the treatment model so what this notation is representing f is a machine learning model that takes in covariant values and estimates the outcome value associated with those covariates and similarly G hat takes in covariant values and estimates the treatment value associated with those covariants so we trained two machine learning models fat and G hat and we can use any machine learning model we like we can use linear regression we can use neural networks we can use decision trees you know whatever we like which is one of the big powers of this approach okay so the second step is we compute residuals so essentially what that is is the difference between the True Value Y and the estimated value from fat and similarly the True Value X and the estimated value G hat and then this gives us our residual values u and v for each model respectively and then using these residuals we can compute the treatment effect directly and so we use this equation here which at first may seem like it's a lot going on but here we have essentially just three things we have our residual value for the treatment model V here and here we have our residual value for the outcome model U which is this thing and then we have our ground truth treatment values that we observe and so I is just indexing our samples and then n is the number of Records in our main sample so one important detail which I kind of left out of this three-step process but is critical to doing double machine learning is a process process called crossfitting so what we do in crossfitting is essentially we split our data into two subsets we have our main sample and auxiliary sample and then we use the main sample to train one of our models and then we use the auxiliary sample to train our other model and then once we do that we switch so if we used our main sample to train fat and our auxiliary sample to train G hat we switch we then use our main sample to train G hat and the auxiliary sample to train fat and so through each each of these steps like step two and step three these will give us a treatment effect and then so we can aggregate these two treatment effects to give us a final causal estimate this may seem like a unnecessary extra step but it's actually critical because some of the assumptions that it allows us to make it actually helps make this equation simpler than it would be otherwise and so if you want to learn more details about that I talk a little more about it in the blog and then the authors give a more detailed explanation in the paper that you can find on the archive here so the final regression based approach that I'm going to talk about are the so-called metal Learners so what these do essentially is use regression models to simulate unobserved outcomes and estimate causal effects so there are several types of meta learners but in this video I'll just talk about three which are the T learner or two learner the S learner or single learner and the X learner so what do these things mean so we'll start with the t- learner so the process of using t- Learners to estimate causal effects can be broken down into a two-step process so we start with our treatment group data so yxal 1 is representing outcome values for people who received treatment and ZX equal 1 are the covariates for people that received treatment and then similarly we have our control group which are the outcomes for people who did not receive treatment and covariates for people who did not receive treatment okay so once once we have our two groups our treatment and control groups the first step is to train two machine learning models that is estimating the outcome Y for the treatment group in terms of their co-variance and then we similarly estimate the outcome value for the control group in terms of covariant so this gives us two machine learning models fat and Gat and then the last step is we compute the causal effects so here we Define the individual treatment effect which we actually saw in the first video of the series titled causal effects but defining it in this metal learner framework we have the individual treatment effect of the I record is equal to the outcome value estimated by the treatment group model for the I record minus the estimated outcome value from the control group model for the I record saying it another way this is the estimated outcome for the I individual if they receive treatment minus the estimated outcome value of the individual if they do not receive treatment and so this difference gives us the individual treatment effect which we can then aggregate in this way to get the average treatment effect and then going a step further we can Define the conditional average treatment effect which is not defined for any particular individual but for a particular subpopulation defined by the covariant values that we plug into this equation okay so next we have the S learner so with s Learners we don't need to split our data into to a treatment and control group we can just use all data from all units and then we use this data to train a single model we'll call it fat which estimates the outcome variable y using both the treatment variable and our covariates so notice we're not restricting the values of the treatment variable here it can take all different levels so one upside of this is that we're not restricted to Binary treatment variables 01 we can actually have multi level treatment variables here okay and then in a very similar way we can compute the effects so we defined the individual treatment effect in a similar way as we did with the t- learner so the individual treatment effect for the I record is defined as the estimated outcome when the i unit receives treatment minus the estimated outcome when the i unit does not receive treatment and so instead of having two separate models one for the treatment group and one for the control group we can modulate our outcome value pred itions by manually setting our treatment input to be one or zero or whatever treatment value we like and then we can aggregate individual treatment effects using the average treatment effect exactly how we did before and then we can Define the conditional average treatment effect in a very similar way as before but now notice we're manually setting our first input value as one for the first term and zero for the second term and finally we have the X learner and so this has over with the t- learner process we saw before but it takes things a step further so first we train two models one for the treatment group and one for the control group just like we did with the t- learner then we impute outcomes and compute the individual treatment effects and so we have like this Crossing term here we use the control group model to impute outcomes so these are unobserved outcomes for the treatment group and so since the treatment group got the treatment we know their outcomes given treatment but we didn't observe their outcomes in the reality where they didn't take treatment and so that's what this model G allows us to do it allows us to simulate these unobserved outcomes and compute the individual treatment effect which we're calling di1 here and then we do a similar thing for the control group so since we observed what the outcome values are for the control group given that they don't take treatment we simulate what they would have been if they did receive treatment using our treatment group Model F and then the next step is we train models to specifically predict our individual treatment effects and so we'll call these models to hat one and to hat zero and so this will use covariant values Z to predict our individual treatment effects D1 and d0 and then finally we can Define our conditional average treatment effects estimator by combining tow hat one and Tow hat zero using a weight function W so in the paper by consel at all they suggest using a propensity score for this weight function W and we talked about propensity scores in a earlier video of this series which I will link down below for those who are interested and then here the conditional average treatment effect is equivalent to to so that's just how we Define it and so if you want to learn more about Metal Learners I found this resource very helpful the paper titled metal learners for estimating heterogeneous treatment effects using machine learning available on the archive I also talk more about this in the blog post associated with this video okay so now we're going to dive into a concrete example with code so this is a example that I've probably overused at this point but we're going to estimate the treatment effect of grad school on income so here we're again going to use the python doy Library which you can find at this link here and then the data source is the UCI machine learning repository so here's the data source it's a open access data source so you should be able to grab the data from here or additionally I have a kind of pickled version that is a pandas data frame available on the GitHub okay so first step is we import our modules and load in our data and so importing pickle so we can bring in our data Eon ml to do the double machine learning stuff and I think the metal learner stuff but we'll find out soon and then doy to do a lot of the heavy lifting for the causal effect calculation and then we import things from sklearn to help us train our machine learning models okay so this is a critical step specific to the dwide library we have to Define our causal model which essentially here is defining our treatment variable or outcome variable and our confounders or common causes and then we Define an es demand which is a recipe that tells us how to define our causal effects with this ground set we can go through each of the three techniques that I discussed earlier so we start with linear regression and the doy Library makes it super easy we essentially can get the linear regression console estimate with one line of code so we input our s demand and then we just say the method that we want to use so we use linear regression and then back door and if you want to learn more about the back door Criterion I talked about that in a previous video of this series but through that the result of this is the average treatment effect based on linear regression is 0.297 which is kind of in the ballark what we've seen in example analyses using this data set next we have double machine learning the very sophisticated and complicated approach relative to others but here we just again can get the double machine learning estimate through one line of code so we have our estimate and then we're defining our method name which is double machine learning which we're grabbing from the econ ml library and then we're defining our method parameters so here I just use linear regression for everything because the data here are so simple we only have three variables two of them are binary so linear regression seemed to give the best results here I would say double machine learning is a bit of overkill for this simple example but I think it would have a lot more utility in more complicated contexts and then here we see the average treatment effect is very similar to always out the linear regression approach which isn't surprising since we use linear regression for all the sub models here and then finally we have the X learner so this is one of the metal Learners we talked about and again it's the same type of thing we put in our estimand and we Define our method name which is from the econ ml Library X learner here and then we use a decision tree for the two submodels in our X learner and then that spits out an average treatment effect of about 0.20 okay so there's a bit of variation and I think that's one of the upsides of libraries like doy is that we can very easily and programmatically try a suite of different causal effect estimates for our data set and then we can have a wider distri distribution of average treatment effects or causal effects that we can use to inform our analyses so that's it for now if you enjoyed this video please consider liking subscribing and sharing with others if you have any questions comments or suggestions for future videos please share those in the comments section below if you want to read more about this subject check out the associated blog on medium additionally feel free to steal the code from the GitHub repository link below and as always thank you for your time and thanks for watching
-5c1KO-JF_s,2022-12-15T13:33:49.000000,Smoothing Crypto Time Series with Wavelets | Real-world Data Project,hey everyone welcome back my name is Shaheen and in this video I'm going to be talking about how we can smooth Financial time series data using something called wavelets so this is a little different than past videos that you may have seen on my YouTube channel what I usually do is videos in more of like a lecture type format where I start talking about Theory and Concepts and then usually finish up with a concrete example with code but here I wanted to do something a little different a real life use case came up when an old friend reached out for some help on a project that he's working on and I thought it was such a wonderful example of what data science looks like in the real world so in this video I want to walk through the background and the problem that my friend was running into and walk through my thought process and how I arrived to the final solution for this problem so I'm going to go through this Jupiter notebook which kind of has step by step the the whole story of what happened in the this specific project and you can find this Jupiter notebook along with the data used in this example in the GitHub repo that is linked Below in the description so if you want to follow along or use these solutions for a problem that you're facing feel free to download the code there okay so let's get into it so like I mentioned the whole setup for this problem was my friend approached me because he was running into an issue in a project that he's working on so the project was actually a tool that he's developing to automatically trade cryptocurrencies and he has some strategy for executing these Buy sell decisions that I don't fully know the details of but he reached out to me because his strategy was all based on time series data that looks something like this and so the problem that he was facing is that his strategy was starting to give undesirable results due to the volatility of this signal so I mean just looking at this we we see like these crazy fast oscillations in the signal so if your goal is to develop a automated trading system you want it to be robust in a wide range of situations and so I can see just looking at how noisy this time series is how a strategy might break down so this brings up a really good point it highlights a mistake that a lot of data scientists make early on in that they're almost too eager to jump into the the coding jump into the technical details jump into the math the techniques the algorithms and don't take the time to take a step back and really ask themselves what is the problem we're trying to solve why are we interested in this problem what is it that we are truly after or what is it that the client is truly after so kind of in line with that my initial thoughts from this ask is what is this data it's very noisy and it turns out to be you know quite a specific signal that's being extracted from cryptocurrency prices it's something like the time average z-score over the past 45 minutes of a currency's trading price so if that doesn't make any sense don't worry it's not going to be critical to the larger point of this use case okay so my second thought was why do you need to use this particular time series so my initial thought is if you're relying on a signal for your strategy and that signal is fundamentally unreliable or noisy are there other data that we can look at and use for our strategy and so in this case there probably are however my friend had already developed his strategy that had worked well for him in the past based on this data so that's the reason he wanted to stick with this particular signal so even if these questions like in this case didn't change the direction of the analysis it's good to make sure you have an understanding of the context and the bigger picture and where your collab operator is coming from and asking for your help with a data science project after some conversation understanding what this data is getting a better idea of why he wanted to use this particular time series and at least in the short term kind of ruling out alternative strategies my first thought was well why don't you just apply a moving average so that's a very simple way to take a noisy signal like this and make it more smooth and so that brings us to the first solution and as it turned out this was a solution that my friend had already tried and it was still not giving him the results that he was looking for and so you know without knowing too many details about his technique there are just three possible issues that kind of jump out to me with this solution so looking at this orange signal here we still see that there are rapid changes in particular sections so while this section looks pretty smooth we can look back a little bit and we can see we still have of time ranges where there's rapid oscillation and that is conceivably something that'll make a automated Buy sell strategy very unstable the second thing is there are time shifting artifacts when we do moving averages so for example we can see that we have a peak in the original signal right here however in the moving averaged in the smoothed version of the signal using the moving average that Peak has actually shifted slightly to the right and so we have these time shifting artifacts anytime we do a moving average and it's kind of like this trade-off you know you can make your moving average window very small and you'll have less of these time shifting artifacts but the signal is going to stay noisy so we can actually look at that a little bit so if I made the window size 10 and run this we can see that the orange signal sticks pretty closely to the blue signal we don't have the time shifting artifact as much then on the other end we can make the window size very large and now we have a really smooth signal but now the time shifting artifacts are very large so this is likely not desirable especially if you have a program that's executing Buy sell decisions automatically it might be a little late to the party and then the third issue is the optimal window size for the moving average is likely Dynamic so just as we saw like this window size dramatically changes how our smooth signal looks and anytime you have to fix a parameter like this it's easy to let's say over fit to a example time series that you use in development so in other words what that means is we might pick a window size of 25 and agree that it looks good on this data but then let's say tomorrow we get new data and then 25 doesn't give us the desired outcome for these reasons the moving average is likely not going to be a optimal solution for this specific use case and so the first thing that came to mind which was a complete failure which is funny because I was so confident that this would solve the problem that my friend offered to he offered to pay me for my time and I was like oh no no this is going to take like five minutes like don't even like it's not even worth it's not even worth it like the coffee you just bought me is uh is equivalent to the amount of effort I'm gonna spend on this problem and I was wrong and thinking through it a little more it kind of becomes obvious why this would fail so this is a great example of when expectations and intuitions can fail like the the gut reaction the gut instinct can a lot of times mislead you when you're in kind of new territory and so uh let's kind of walk through the solution so at a high level the strategy is we have this signal let's fit it to a polynomial so we're talking about like x x squared x cubed x to the fourth so on and so forth and we're just gonna fit it to enough polynomials that we capture the properties we care about of the signal and don't capture the noise essentially and so this was a complete failure so here we fit it to 15 polynomials so what that means is we had a constant term x to the first power x squared x cubed x to the 4 all the way up to x to the 15. we just do a polynomial fit and then we plot the fit and compare it to the original signal and so while this well the polynomial fit is a lot smoother than the original signal it doesn't capture any useful information from the underlying signal so this was a complete failure so in the face of this failure I had another thought I thought of course polynomials aren't going to work here polynomials want to shoot up to infinity or shoot down to negative infinity and here we clearly have a signal that is oscillating around zero so when I think of something oscillating around zero I naturally think of Sines and cosines which brings up the concept of a Fourier transform if you're not familiar with the Fourier transform is one of the most powerful and widely used techniques in Signal processing Math and Science and so basically what we're doing conceptually is we're taking a signal so our blue signal here and we're going to decompose it into a set of Sines and cosines of different amplitudes and frequencies and so notice this is very similar to what we did with the polynomial fit so in the polynomial fit we take the signal fill it fit it to a set of polynomials you can conceptually think of what we're doing here as taking the signal and fitting it to a set of Sines and cosines the result of that captures a lot more of the underlying signal but we still have a lot of noise this actually has even more noise qualitatively than the moving average solution so if I were to pick between the moving average solution and this solution I would pick the moving average because it is both simple and qualitatively better and so solution number two attempt number two was yet another failure and so now I'm starting to think more polynomials didn't work they smoothed the signal but they kind of smoothed it so much that we lost all the useful information the Fourier Transformer may have captured the signal but there's still a lot of noise is there something that's in between these things and that's what inspired using the third technique which is the wavelet transform what we're doing here is the same idea as with the polynomial fit and the Fourier decomposition but instead of fitting our signal in terms of polynomials or in terms of Sines and cosines we're going to fit our signal to a set of wavelets which are essentially these wave-like things that are localized in time and if you want to learn more about the wavelet transform and the 40 transform I have some videos on that I have some blog posts on that so that should be a good primer if you're completely lately unfamiliar with the technique and so here we do the same concept we kind of fit our signal to a family of wavelets and then we just drop these higher order these more detailed coefficients of the wavelet transform and something remarkable happens in that we have a kind of Goldilocks smoothing situation you know it's not oversoothed like the polynomial fit but it's not too noisy like with the Fourier transform fit and so I shared the solution to my friend he seemed really excited about it really happy with it so he's testing it out in his platform now and hopefully it works out well and so there's one key Point here that is one aspect of data science that comes up a lot that gets me really excited which is that so often data science techniques and solution are application or context agnostic so for those of you who did see the video I posted on the wavelet transform you might notice that the steps that we ran through to smooth this financial time series is essentially the same exact process we used to detect our peaks in ECG data it's a great feeling when you can spend a lot of time working on a project and then somewhere down the line like two years later you come across a different project and you can basically copy and paste code from a project you did in the past even if the context is completely different like with analyzing ECG data versus Financial time series data okay so that's basically it I've also created a user-defined function to implement this strategy and that is also available at the GitHub repo and then here's a zoomed in version of the time series so how would you approach this problem leave your ideas and suggestions in the comments section below if you enjoyed this video please consider liking subscribing and sharing with others and as always thank you so much for your time and thanks for watching
ASU5HG5EqTM,2022-12-02T13:54:12.000000,Causal Effects via DAGs | How to Handle Unobserved Confounders,n/a
dejZzJIZdow,2022-10-22T13:30:09.000000,Causal Effects via the Do-operator | Overview & Example,hey folks welcome back this is the third video in the series on causal effects in the last video we learned how to estimate causal effects with observational data via something called a propensity score while these approaches help us in the face of measured confounders there's still the problem of unmeasured confounders in other words variables that bias our estimate that we do not observe in our data in this video we will see how we can resolve this problem of unmeasured confounders to do this we will need to re-evaluate how we think about causal effects so with that let's get into it everything I'm going to talk about in this video concerns connecting observational data to Interventional data in other words connecting data that we might passively observe to data that we might measure more carefully through a randomized controlled trial or something similar and so this distinction between observational studies and Interventional studies was made in the previous video but just as a quick recap of what we're talking about here say we're trying to quantify the causal effect of a pill on headache status what a observational study would look like to investigate this is we passively observe a population of people with headaches some of them take the pill some of them don't take the pill and we just observe their natural outcomes with no intervention or influence on who takes the pill and who doesn't on the other side of this we have an Interventional study which is more akin to a randomized controlled trial where we carefully pick a population of people at random and then randomly split them into two groups an experimental group and a control group would give the experimental group the pill and we don't give the pill to the control group and then we can compare their outcomes so clearly observational data take less effort to measure while Interventional data take more effort to measure but as has been said in the past there's no such thing as a free lunch so even though I observational data are easier to capture takes more effort to estimate causal effects from them and even still it may not be possible to estimate causal effects from them we saw one way we can estimate causal effects from observational data in the previous video where we talked about propensity scores and in this video we're going to talk about an alternative strategy to doing this and so first we're going to take a step back and re-evaluate our definition of the average treatment effect which is what we've been using to quantify causal effects in this series formula one here is how we Define the average treatment effect in the context of a randomized controlled trial so what we have is this expression here and just quickly running through this e is denoting the expectation value y denotes the outcome variable X denotes the binary treatment variable x equals 1 means the subject took the pill x equals zero means the subject didn't take the pill so this first term is the expected outcome of the treated group because all the these people took the pill the second term is the expected outcome of the untreated group because they didn't take the pill so this is a very simple expression and if we have a randomized control trial we can obtain the average treatment effect in a very straightforward way but there's an implicit assumption here that people may not realize if just seeing this equation on paper or on a website or something so the assumption is that the treatment status is statistically independent of all other factors so that's why this equation is true for a randomized controlled trial that's the whole point of randomization so that who is in the experimental group and who's in the control group has no statistical relationship with any other factors but now we're going to look at an alternative way to formulate the average treatment effect and to do this we're going to use the do operator which we first saw in the previous video on causal inference so here we have a very similar thing we have this expected outcome of the treated group and we have the expected outcome of the control group but the only difference here is we now now have this do operator in each expression with the do operator represents is a physical intervention as opposed to on this side in Formula One this notation does not distinguish between a intervention and just a passive observation and so what we gain by expressing the average treatment effect in this way is that we are explicitly denoting that the treatment status is statistically independent of all other factors and so in the context of a randomized controlled trial these two formulas are equivalent but this isn't true in other situations in other words in an observational study if you use this equation without being careful about statistical dependencies you're likely going to get biased estimates of causal effects because there are likely systematic differences between the people that received the treatment and people that didn't receive the treatment okay and then as a final note in the previous video we talked about these propensity score based methods and there we were using this equation and we were using observational data so there the implicit assumption is that the propensity score based methods approximately remove the statistical dependence between treatment variables and measured covariates so that's what allows the propensity score based methods to work is that we're taking care of these other statistical dependencies okay so more on this do operator so the do operator again is a way to denote interventions and so this brings up another observational Interventional Distinction on the left here we have an observational distribution so this is what we saw on the left hand side of the previous slide in other words this is what we saw in Formula One it was the probability of Y given the variable X is observed to be X naught while in Formula 2 we had an Interventional distribution so that was the probability of Y given the variable X is artificially set to X naught I talk more about the do operator in the video and causal inference so check that out also this is a very good reference this is an introduction by Judea Pearl who invented the do operator and it's a big figure in the space of causal inference so often we don't measure Interventional distributions directly so what this would look like is doing the Interventional study doing the randomized control trial in other words doing the physical intervention and capturing the statistics however Formula 2 this alternative definition of the average treatment effect relies on the Interventional distribution so if we want to use this more general formula for the average treatment Effect one that's not just true in randomized controlled trials but is true everywhere it would be good if we can translate any Interventional distribution into observational distribution so translating the thing that we want to estimate in terms of things that we actually measure and that brings up the question of identifiability and so identifiability is all about answering the following question can the Interventional distributions be obtained from the given data so in the context of a randomized controlled trial we likely already have the Interventional distribution because that's what we painstakingly went through the process of measuring but in all other cases where we just have observational data what identifiability is all about is expressing the Interventional distribution in terms of observational distributions and so Pearl and colleagues developed a systematic three-step process for answering this question of identifiability okay so the first step in this three-step process is to write down the causal model and so taking the example from the previous video where we were estimating the causal effect of going to grad school on income this was the causal model that we had assumed age causes grad school and income then grad school causes income then the second step is to evaluate something called the Markov condition and so this consists of two parts the first is asking is the graph a cyclic so basically do any Cycles exist in this graph so if we start at Z and we follow the arrowheads there's no way we can get back to Z if we start at X we follow arrowheads there's no way to get back to X and if we start at y there's no way to get back to Y so indeed this graph is a cyclic the second part is that all noise terms are independent so basically the noise terms for age would be all factors that cause age that are not captured in our causal model similarly the noise terms for grad school would be all other factors other than age that drive someone's probability of going to grad school and then the noise terms of income would be all other factors other than age and grad school that have a causal effect on someone's income and so assuming that these noise terms are independent means that there's no cross connections between these external factors is this true in this case that is questionable but for the sake of this example we'll just assume that it is okay and finally step three we express the Interventional distribution in terms of observational distributions and so we can do this using the wonderful truncated factorization formula which expresses any Interventional distribution in terms of observational distribution so notice we have a do operator on the left hand side and there's no do operator on the right hand side okay but still on the left hand side we have Z and to estimate the causal effect of X on y we just want to isolate y in this conditional probability here so the way we can get rid of Z is by summing over it so basically setting Z equal to every value in our data set so Z equals one two three four five all the way up to you know 100 whatever we might have and then just evaluating this summation okay so now that we have the Interventional distribution in terms of observational ones we can compute the average treatment effect using our formula two from before and so all we do is just plug in one here stick it over here plug in 0 here stick it over here and then just compute the average treatment effect and so the truncated factorization model I showed in the previous slide was just for that specific example but it's a much more general formula as you can see here so I'm not going to spend too much time on this but this is from the introduction by Pearl and I also talk a bit more about this in the blog associated with this video so now the whole motivation of this video from the propensity score video was that propensity score based methods don't account for unmeasured confounders they only handle confounders or covariates that are included in our data so what can we do about unmeasured confounders so using everything that we've talked about in this video we can compute the average treatment effect even when we have data missing from our data set so we're going to run through this systematic three-step process again but now for a more complicated example so this is taken from the introduction by Pearl so suppose we have this causal model we have our treatment X our outcome Y and we have three covariates okay so step two is we evaluate the Markov condition we can see that this graph is acyclic there are no cycles and we will just assume that the noise terms are independent and then step three we'll Express the Interventional distribution so again we do this using the truncated factorization formula which is a lot longer now because we have five variables instead of three and then to isolate y on the left hand side we can just sum over variants but now suppose that Z isn't measured it's missing from our data set we can run through the same process again we write down the causal model we evaluate the Markov condition we express the Interventional distribution first with the truncated factorization formula then summing over covariates but now the problem is we don't measure Z2 we don't know these probabilities so what are we supposed to do so now we can just apply a little bit of magic and out comes the Interventional distribution without Z2 on the right hand side so what just happened here so what we just did there was the result of this expression here so this is the Interventional distribution via the parents of the treatment so on the left hand side we have the Interventional distribution of Y given the intervention in X and on the right hand side we just have observational distributions in terms of x y and the parents of X so the key inside here is we only need the parents of X to estimate its causal effect and so again this is from the introduction by Pearl referenced down here so that was a lot of information so I'm just going to try to recap some key points the first is given a markovian causal model with all the variables measured we can use the truncated factorization formula to express any Interventional distribution in terms of observational ones and thus we can express any causal effect using our new formulation the average treatment effect the second key point is that if we have unmeasured confounders we can always simplify the truncated factorization formula given a markovian causal model to include only the parents of X but now what if we can't measure the parents of X and so what that brings up is this notion of alternative covariate selection that's going to be the topic of the next video in this series so there we're going to further explore this notion of simplifying the trunk the factorization formula to only include variables that we measure or in other words to exclude the variables that we don't measure and then we'll get further insight to the magic that we saw a couple slides ago so there's a Blog associated with this video there's some details in there that I may have left out in this video and I'll drop a friend Link in the description so you can access the blog even if you're not a medium member and if you enjoyed this video please consider liking subscribing and sharing this content if you have questions please share those in the comments section below a big reason why I make these videos and write these blogs is for my own learning process and development and the questions that I receive is a huge help in this whole learning process and as always thanks for your time and thanks for watching
dm-BWjyYQpw,2022-10-14T13:37:55.000000,Causal Effects via Propensity Scores | Introduction & Python Code,n/a
BOPOX_mTS0g,2022-10-07T12:23:26.000000,Causal Effects | An introduction,n/a
5zeqo-R12vk,2022-10-01T02:01:57.000000,How to STAY dumb,n/a
WgmMK5fS0X0,2022-09-27T19:09:20.000000,How to do MORE with LESS - multikills,multitasking isn't a good strategy how do we solve this problem trying to get more done in less time one idea is we can seek guidance from our bird hating ancestors and try to kill two birds with one stone at first glance this might sound like multitasking but it's not multitasking is like when you do two things around the same time while killing two birds with one stone is getting two things done with a single action
gazeatME3dI,2022-09-26T22:07:08.000000,How to be more ANTI-FRAGILE,n/a
aD9ereUJBII,2022-09-24T20:12:19.000000,The Achievement TRAP,be careful with achievements it probably sounds silly since achievements are usually thought of as good things but they can be dangerous the more we focus on what we've achieved the less we focus on what we need to do now when we think we're so cool for hitting a goal or winning an award or being the best at something it's easy to get lazy about the habits that helped us achieve that thing in the first place don't fall into the achievement trap no matter what you've achieved there's always room for improvement
6A4qwVOkPF0,2022-09-23T19:42:12.000000,"It’s not DANGER behind the fear of conflict, but uncertainty",so now let's talk about fear the fear of conflict like many other fears doesn't usually stem from any kind of actual danger what it really comes from is the unknown consider the example of driving to work driving is probably one of the riskiest and most dangerous things we do on a daily basis but no one's afraid to commute to work that's because we do it every day it's familiar and things that are familiar aren't scary so in a similar way just like how every time you drive and you survive it becomes less scary every time a team goes through a conflict and survives conflict becomes less scary
W6TkOTsI7vM,2022-08-26T13:30:31.000000,What Is Data Science & How To Start? | A Beginner's Guide,i get a lot of data science questions naturally through my blogs and youtube videos but i've also been getting questions that are a bit more meta the questions are more career oriented people ask me how can i get into data science what do i need to become a good data scientist so on and so forth so the fact that people are reaching out to me with these kinds of questions is probably a good indication that there are a lot of other people out there with similar kinds of questions so that's the inspiration for this video here i'll give my take on what is data science and what is the best way to get into it so i prefaced this whole discussion with this is just my impression as someone that's worked in data science for the past few years in both an academic and now in a business setting and data science being the huge field that it is there are certainly a wide range of different manifestations of what it might look like across different organizations businesses etc however this is the kind of mental model that i've arrived at about data science and the different associated roles through my experience so we'll start with this first question of what is data science and to me data science fits into a bigger picture which includes a variety of different roles and responsibilities that all center around this core of data well again there might be different labels and details for these types of roles the way i see it there are generally five distinct roles in this data space to list them the first is data engineering second is data analytics third is data science which is what i do and the main topic of this video the fourth is machine learning operations and engineering and then the fifth is data management all these archetypal roles and responsibilities in the data space all serve this bigger data pipeline so data engineering is the starting point and then that goes all the way to ml operations engineering and then data management kind of sits on top of this whole thing okay so the first role data engineering is mainly focused on taking data information from the real world and bringing it to a place where data scientists data analysts can do something with that information and as a data scientist i love data engineers because they make my life a lot easier so every data analyst and every data scientist's dream is to just have a curated data set where everything is good to go the data is clean the data is pre-processed and all they have to do is make a visualization or develop a model so on and so forth so good data engineers are very valuable asset probably a big reason why it's such a big job these days okay so the second role i mentioned was data analyst and typically what a data analyst does is take data that is made available to them through whatever means and typically will generate interactive dashboards or visualizations graphs plots they will typically tell stories with data they'll make slides they'll give presentations to decision makers and business partners and stakeholders and a data analyst will typically use tools like microsoft excel tableau power bi and some kind of interactive sql application so a lot of these tools don't require much programming experience so in that regard a data analyst role is typically less technical for lack of a better word than a data scientist rule and probably the biggest difference between a data analyst and a data scientist is that a data scientist will do a lot more programming than a data analyst and so that brings us to the third role which is the data scientist what i do a data scientist will do a lot of the same things as a data analyst we will create visualizations we'll tell stories we'll communicate our findings but like i mentioned what i spend most of my time on is programming so i use python and that seems the most popular these days in the data science space but there's also r which is pretty big in the statistics community there's matlab which is big in the academic or scientific community and then there's also julia which is a kind of new up-and-coming programming language so it's not just a matter of data scientists use different tools than data analysts to do the same thing data scientists also will typically focus on developing models so what's a model a model is basically something that lets you do predictions you give it some information that you can measure and it will give you a prediction about something you care about that you can't necessarily measure right now so this could be something like given someone's past credit history what's the probability that they won't make a credit card payment in the future models are definitely a powerful tool in science in business in finance and sales and basically any industry that has data and has things that they would like to predict models are going to be helpful to that industry which brings us to the fourth role in this data space which is machine learning operations and what this role typically does is takes the model that the data scientist develops and deploys it into production so it doesn't matter how amazing and insightful your model is if it can't be used and deployed in the real world it's not going to have any value what this might look like using the same example from before what's the probability that someone will make their credit card payment based on past credit history so it's great if i develop a model and it's sitting on some server somewhere or sitting on my local machine or whatever it might be but how does that help the people making the decisions of whether to increase someone's credit card limit or to approve a new credit application so that's where the machine learning operations will be valuable the fifth and final rule sits on top of this whole pipeline so we start with the data engineers who take the data and information from the real world and bring it to a place where analysts and data scientists can use it the data analyst will typically take the data that is there and visualize it look at it in different ways and create stories and slides from it the data scientists will do a lot of the same things but we'll take it a step further in taking that data and developing models with it then finally the machine learning engineer will take that model and deploy it into production so this fifth role which i labeled as data management kind of sits on top and what this role focuses on is handling the metadata the meta information so not so much what the values in a specific data table might be but where's the data table located what variables are in this data table what do these variable names mean what is the description of each variable what data types are they using where is it coming from where is this data being pushed to eventually kind of keeping track of all the data about the data which is very important so this ecosystem like all these different types of roles are critical to make sure the whole process is working effectively like you can't just have data engineers you can't just have data scientists you can't just have machine learning engineers you really need every kind of piece of the puzzle to have data make an impact in your business or your research or whatever it might be as a final caveat even though i've presented these five different roles as beautifully distinct and specialized responsibilities in reality it's not like that at all there's a lot of bleed over there's a lot of overlap so just speaking from my experience as a data scientist i do the dating engineering work i do the data analytics work i'll do the machine learning development i'll do the data management stuff and that's just the reality of things so while it may be the case at some organizations that these roles are very specialized so a data scientist will only focus on doing data science type things or a data engineer will only focus on data engineering type things for the most part there is bleed over so if you want to get into any one of these roles it's good to have some knowledge and experience doing the other roles in this space okay so now we'll talk about the second question which is how can i get into data science so first preface this again that this advice is definitely biased by my experience and my data science journey so there are certainly other paths to get into a data science role but these are the tips that i would give to anyone that comes up to me and says that they want to get into data science i have three tips for anyone trying to get into data science so the first thing to do is get a technical graduate degree so probably 80 to 90 of the people that i work with in data science they either have a master's degree or a phd in some technical field so what do i mean by technical there's a lot of flexibility here so i have a phd in physics i've seen phds in math phds in statistics masters in statistics master's in data science and data analytics any training that has a focus on math science programming will be helpful to getting a job and while you don't necessarily learn all that you need to know to do data science in these different programs there's typically a good amount of overlap so in physics you learn a lot of math and science and you do a lot of modeling and so that's a pretty natural transition into data science but second and probably the most practical is that it's just something to have on your resume so the reality is when people are looking at resumes for data scientists they're getting flooded with applications and there just has to be a way to just narrow down the applications to help make a decision and typically the first filter that people use in picking data science applications is do they have a graduate degree and in what and so again i'll highlight even though this is the background of the bulk of people i see in data science they have a technical graduate degree there's certainly other people working in data science without them maybe they'll just have an undergraduate degree or maybe they got an undergraduate degree and did like a data science boot camp so it's certainly not the only way into data science but it is the bulk of what i've seen okay so the second tip i would give to anyone trying to get into data science is to learn the basics and what are the basics so i would say there are three kind of core things that you need as a data scientist one you absolutely need to know how to program i would definitely recommend to learn python the second basic basic thing you need as a data scientist is the solid mathematical foundation and so specifically what this includes is linear algebra statistics and calculus those are the three core mathematical topics that you need to be comfortable with and then the third basic thing you need as a data scientist which i personally think is the most important but is often the one that's focused on the least is communication if you want to be a good data scientist you need to be able to present technical information to a non-technical audience you need to be able to translate your insights into something practical into something that has implications for the real world and the reason i think it's the most important is it doesn't matter how profound or impressive your findings are if it doesn't make sense to people if no one understands what you're talking about it's going to have absolutely zero impact it's going to provide absolutely no value to anyone so again second tip is learn the basics learn how to program probably python's your best bet get the solid mathematical foundation and work on your technical communication okay so the third and final tip that i would give to any aspiring data scientist is to do it i think the best way to learn is by doing so the best way to learn data science is by doing data science and so what this might look like in the early days is just finding some data and doing a project ideally using real world data so not like curated and ready to go data sets that you might find on kaggle for instance but go online try to find data sets try to find data that is not clean use a web scraper to pull information off a website reach out to your local network do you know anyone doing research that is working with real world do you know anyone that owns or works in a business that is working with real world data try to get access to that data and come up with some projects that might be helpful to that person so if you want to get good at data science you got to do data science so try to come up with at least one or two data science projects and do it from a to z do the data engineering bit do the data analytics do the data science bit and do the machine learning engineering in other words take the data from the raw source and bring it into a place where you can analyze it create visualizations look at the basic statistics try to tell a story with the data identify a problem identify a project then develop a model do something that has real world impact that provides value and then try to deploy it maybe make a website maybe make an api so people can actually use your project and then i'll also say a lot of times it's not just about what you know but about what you show like let's say you do the whole shebang you do it all perfectly you get real world data you look at the data you get the basic statistics you make visualizations you develop a model and then you deploy that model and it has tremendous real world impact and it provides value but if you don't present that information no one's going to know about it so if you're trying to work in data science you want to get hired by someone to do data science you need to not just know your stuff but show your stuff show people what you've done what you can do and it's going to increase your chances of success so don't just do a data science project and keep it a secret make a portfolio put it out there and show people what you can do okay so that's basically it so if you watch this and you still have questions please feel free to drop additional questions in the comments section below the questions are really helpful for me i feel like i learn a lot about the people that watch these videos through the questions that i see in the comments section so i greatly appreciate those i hope you found some value in this video and after watching it you know what to do next to start your data science journey or you realize you don't want to be a data scientist at all and you're going to open a coffee shop or something either way i hope it was helpful and as always thanks for watching you
5ezFcy9CIWE,2022-06-16T13:50:15.000000,Persistent Homology | Introduction & Python Example Code,hey folks welcome back this is the final video in a three-part series on topological data analysis or TDA for short in this video I'll be talking about another specific technique Under the Umbrella of TDA called persistent homology the big idea behind persistent homology is finding the core topological features of your data that are hopefully robust to noise I'll start with a brief discussion of key points surrounding persistent homology and then dive into a concrete example with code of how to use it and with that let's get into the video there are many layers to persistent homology so I'll try to start super simple and build things up in a way that hopefully makes some sense like I've mentioned throughout this series TDA is all about looking at the shape of data so let's go back to preschool and talk about shapes or more precisely polygons like the ones shown here but not all polygons are equal there is one that is special and the reason it is special is because it is the simplest polygon we can construct the triangle and one neat thing about triangles is that we can use them to make any other polygon for example a square is really just two triangles stuck together a pentagon can be made from four triangles like this and a star is just the same Pentagon but with five triangles coming out of it so one thought is if we want to analyze the shape of our data maybe we can break it down into a bunch of triangles well as it turns out this is essentially what what we do in persistent homology but with one technical detail since most data sets live in more than just two dimensions that's to say we have more than just two variables flat two-dimensional triangles may not capture the full richness of our data's shape don't worry like most things mathematicians have generalized the notion of a triangle to any number of dimensions and they call these generalized triangles simplexes so the triangle that we know and love is called a two Simplex since it lives in two dimensions a line segment is the simplest shape we can construct in one dimension and it's called a one Simplex similarly a tetrahedron is called a three simplex and a point is a zero simplex and so on for all the other dimensions so just like a collection of triangles can make any two-dimensional polygon a collection of simplexes can approximate just about any complicated high-dimensional shape that may underly our data and so since you'll probably see it elsewhere the technical name for a collection of simplexes is called a simplicial complex and this is a key Concept in persistent homology okay so this gives us a clue as to how we can take unstructured Point clouds in other words data sets and translate them into shapes so now let's talk about how we might compare shapes together no matter how different or complicated they may seem so one way to do this is by looking at holes for example these three objects shown here we have a Taurus a loop and a coffee mug so while these may appear to be very different shapes they have something fundamental in common they all have a hole and this is like the joke that a topologist looks at a coffee mug and a donut and sees the same thing the reason being that one can continuously transform one into the other for the fadas out there this is called a homeomorphism but the fundamental thing here is the number of holes so one way we can characterize and group shapes together is by counting holes and just like before when we generalize triangles into simplexes we can generalize holes as well we can think of cavities as holes in 3D and we can think of singly connected components as holes in 1D and so these generalized holes form the basis of what are called homology groups and these give us a formal way to characterize different shapes so when we talk about homology we are essentially just talking about holes okay so now that we've talked about constructing shapes with generalized triangles and characterizing those shapes via generalized holes we can finally talk about persistent homology and the first step in persistent homology is to convert data into a simpal complex to see this consider a data set I.E a point cloud like this and one way we can construct a simplicial complex out of this is by drawing n dimensional balls around each point and since our data here is two-dimensional we just draw circles around each point which might look something like this so at the center of each of these gray circles we have a point we can form one simplexes I.E line segments by connecting the data points whose corresponding circles overlap which might look something like this and so now we have two shapes we have our original Point Cloud which is indeed a simpal complex where each point is a zero simp Lex and the shape we just constructed made up of both zero and one simplexes and then we can compare these two shapes by looking at their homology more specifically by counting the number of connected components which corresponds to the h0 homology group that we talked about in the previous slide and there we go so we can see that in our first shape on the left we have 20 separate connected components while on the right here we have 13 singly Connected components but there's nothing special about this radius Epsilon sub one so let's do this again but with bigger circles now we can start to see two simplexes appear in other words triangles and the number of connected components decreases but still there's nothing special about Epsilon 2 so let's go even bigger and now we see three simplexes appear I.E tetrahedrons and so on and so forth however there is a special radius value here which is when every circle overlaps with with every other Circle and we are just left with one big connected component and this is a natural limit to this process as we can see with each of these simplicial complexes the shape of our data is evolving and its evolution is captured and Quantified by the number of connected components in other words by the change in its homology so although only four different choices of radi are shown here corresponding to the four different shapes on the screen we can do this for every choice of radius between zero and the limit I mentioned earlier so this gives us a way to sus out which topological features of our data are significant based on how long they persist during this circle growing process in other words the holes that persist over a large increase in radi are more significant than the ones that persist over just a short period okay so how can we track the Persistence of these holes so so one good way to do this is by using a persistence diagram these look something like the plot on the left here which is showing the persistence diagram of a hollow sphere and looking at the plot each of these blue orange and green points corresponds to a topological feature or in other words a hole in blue we have the H Subzero homology Group which are the singly connected components in Orange we have the H1 homology Group which are closed loops and in green we have the H2 homology group in other words W cavities the x axis of this plot indicates the radius at which a hole appeared in the evolution of the data's shape in other words in this circle growing process that we showed in this previous slide and on the y- AIS we have the radius at which that hole disappeared so therefore a point that sits near this black dashed Line This yal X line corresponds to a hole that disappeared soon after it appeared conversely points that sit far away from this line represent holes that disappeared long after they appeared therefore two key points of a persistence diagram are the points close to this yal x line are noise while the points relatively far from this line are significant so in this example we have two points that are far from this line the blue one in the top left here and the green one right here so we can ignore this blue one here because this corresponds to when every and dimensional ball overlaps with every other ball so the significant topological feature of this data is captured by this Green Point here which is telling us that the data is characterized by one cavity and this makes sense since the data for this example are organized on the surface of a sphere okay so up until this point I've discussed only toy examples and meant to give you an idea of what's going on with persistent homology so now we'll switch gears to an example with with real world data so in this example we'll walk through how one could use persistent homology to analyze Market data and I suppose it's worth mentioning that this example is not meant as Financial advice I'm a physicist not a Trader never taken a finance class in my life however I hope this example gives you an idea of what an analysis using persistent homology might look like and Inspire ideas for analyses using data that you might be working with okay so similar to the last video we start by importing python libraries the notable libraries here are y Finance which gives us an API to grab Market data and the riper and pum modules which are part of the same pyit TDA ecosystem from the last video next we load in Market data over a 4-year period using Y Finance here we are grabbing four major Market indexes namely the s&p500 Dow Jones NASDAQ and Russell 2000 we have daily prices for these index organized in a pandas data frame so you can imagine four columns for each market index and many rows corresponding to each day that the markets were open over this 4-year period then we convert this pandas data frame into a numpy array and compute the log daily returns of each index and this choice of data prep follows the procedure used in the paper by gidia and cats which was the inspiration for this example and you can find it at the archive reference here okay so now we get into the TDA SEF so in this analysis we want to track changes in the shape of the markets by looking at how the homology of the market changes over time so to do this we start by initializing this object that constructs simpal complexes from data next we Define a Time window size which will allow us to grab a chunk of data to analyze the homology of so here we're sending this window size to 20 days next we Define the total number of these chunks we will have and finally we create an Umpire rate to keep track of a number that quantifies changes in homology okay next we go down to this for Loop and we do some persistent homology so first we take the first 20 rows of data to do persistent homology and create a persistence diagram that is we grow four dimensional balls around each point where each choice of radius creates a simplicial complex and we track the holes that appear and disappear using a persistence diagram so we do all that with just one line of code and we do the same thing but now for another set of 20 rows specifically the second row all the way down to the 21st row so now we have two persistance diagrams corresponding to two overlapping 20-day windows in which the market was open so next we can quantify the change in the overall homology between these two persistence diagrams using something called the washer Stein distance which is essentially a distance measure between two persistence diagrams so at the end of this whole process we get a single number and store it in the numpy array we created earlier then we repeat this whole process for all the rows in our data set okay so after this whole process we have a set of values which quantify the changes in homology between consecutive days that the market was open and so we can just plot this as a Time series which is what's happening in this block of code here and the plot will look like this blue line here which we can see there's this clear peak near the middle of the time series and then for some context we also have scaled S&P 500 close prices plotted in Orange just above and this vertical red line here is indicating when the crash of 2020 occurred and then as it turns out the peak in this waserstein distance time series seems to correspond very closely with when this crash occurred so did homology changes predict the crash of 2020 well I wouldn't go that far but this is indeed interesting one idea to investigate this further is one could try to use these waserstein distances to predict future market index prices so if past distance values predict future index prices then maybe there's something here so as you may be able to see from this example there is a lot of room for creativity when using persistent homology in practice and in some sense this is more art than science so that bring brings us to the end of our three-part series on topological data analysis A TDA is a young field with a lot of untapped potential so I hope this series was helpful in getting a better idea of what it's all about if you'd like to learn more check out the other videos in this series Linked In the description below there's also a corresponding medium article to this video and the others in this series which you can find in the description if you enjoyed this content please consider liking subscribing or sharing this video like many of you I am indeed still learning so if you have thoughts questions or concerns please feel free to share those in the comment section below and as always thanks for watching
NlMrvCYlOOQ,2022-06-03T10:02:25.000000,The Mapper Algorithm | Overview & Python Example Code,hey folks welcome back this is the second video in a three-part series on topological data analysis or tda for short in this video i'll be talking about a specific technique under the umbrella of tda called the mapper algorithm what this approach allows you to do is translate your data into an interactive graphical representation which enables things like exploratory data analysis and finding new patterns in your data i'll start with a discussion of how the algorithm works before diving into a concrete example with code and with that let's get into the video so in the previous video i discussed a famous problem in math called the seven bridges of koenigsberg so i won't go into all the details of the problem but it was eventually solved by famous mathematician leonard euler and the way he solved it was by drawing a picture and this picture is what we now call a graph and so a graph consists of dots connected by lines more technical term for these things the dots are called vertices and the lines are called edges another equivalent terminology is instead of calling this thing a graph we can call it a network and we can call the dots nodes and we can call the lines links so these are all equivalent terminology that i'll probably use interchangeably for this video and so graphs or networks they typically represent something from the real world so in this case euler drew a graph representing konigsberg where each of the nodes represented a land mass and each of the lines connecting two nodes represented a bridge and so what this does is it boils down the problem to its essential elements and this is what allowed euler to famously solve this problem and as i mentioned in the previous video this is essentially what we're doing when we do topological data analysis we are translating data from the real world into its essential elements or in other words into its underlying shape and so one way of doing this is via the mapper algorithm and the main topic of this video so the mapper algorithm allows us to translate data into a graph so key applications of the map or algorithm one is exploratory data analysis it allows us to take a data set and generate a visually engaging and interactive visualization another application is that it allows you to compress and visualize very high dimensional data so imagine trying to visualize a 500 dimensional data set with mapper algorithm we can take our data set compress it into a two-dimensional graph and then visualize it and try to highlight some key insights and we saw some examples of this in the previous video with discovery of cancer subtypes defining new roles in basketball and characterizing the evolution of the two political parties in the u.s so at a super high level the mapper algorithm takes data and translates it into a graph but how exactly does it work i've broken down the algorithm into five steps and i apologize in advance because it's a bit sophisticated but i will do my best to explain it in plain english so the first step is we start with our data set so here we have a two-dimensional data set because we have two variables x1 and x2 then the second step is we project our data into a lower dimensional space so here we're going from two dimensions and we're projecting down to one dimension and we can do this with any dimensionality reduction strategy like we do something standard like pca we can do something more sophisticated as we will see in the example later another popular strategy is to take just basic statistics to project out into one dimension in other words you could consider two variables x1 and x2 so each point will have a corresponding x1 and x2 value you could take the average of those two and organize them onto a one-dimensional axis you could take the max you could take them in so they're all these different strategies so we've gone from two dimensions down to one dimension so nothing too fancy yet so the next step is we define something called a cover so basically what this means is we're going to define two subsets indicated by this red circle and this green circle and we will have these two subsets have some overlap so we can see here that the red subset and the green subset indeed have some overlap and these are indicated by the yellow points in the center of this picture and so that's what we mean by cover we just define a collection of subsets that have some overlap which include the entirety of the data set another thing is we could do more than just two subsets we could have three subsets four subsets so on but just for this toy example i chose two because it's easy to see what's going on here okay and then the fourth step is we cluster the pre-image there's a lot of jargon here so i'm just gonna break it down so if we look at step three we have these red points green points and yellow points but if we remember that each of these points has a corresponding point in our original data set so what's being shown in this picture in step four is our original data set but the points are colored based on which subset they appear in from step three and so the next step is we're going to iteratively go through all our subsets so we only have two subsets we have a red subset at green subset and we're going to apply our favorite clustering algorithm we'll start with this red subset so in other words we're going to look at the red and yellow points only and we're going to do a clustering algorithm and let's say it looks something like this then we will go to our next subset which is the green and yellow points here and we will cluster those and let's say we get something like this so now we have these four clusters defined with some overlap between them now we're set up to create a graph so we can create a graph where the nodes are these clusters so four nodes corresponding to four clusters and then two nodes are connected by an edge if the clusters have shared members so this middle cluster shares members with the other three and so that's what's being shown here okay so this is just a toy example i hope that was somewhat clear of what's going on here but i'm going to try to make things more concrete with an example with code so in this example we're going to do exploratory data analysis of s p 500 data so our first step is to import some modules so we have the yahoo finance module which allows us to get the stock data we have this k-mapper module which allows us to do the mapper algorithm stuff so we're importing this umap module sklearn and then something from sklearn and we're using these for our dimensionality reduction and then we have numpy and matplotlib to do some standard math and visualization stuff okay so the first step as with any data science project is you're gonna get your data so this is pretty straightforward you just define your ticker names and you define the date range for which you want your data and then with one line of code you can pull all that data and so this code is available on the github everything should just work out of the box and once we have our data we can do some more preparation to make it ready to go to do our analysis so the first step is we're just going to look at adjusted close prices and so now what you can imagine is we have columns corresponding to ticker names and then we have rows which are corresponding to days that the markets open and then what we're going to do is convert this pandas data frame into a numpy array and then we're going to standardize each of the columns so basically what that means is we're going to consider a column compute its mean and standard deviation and then we're going to subtract the mean from each value in this column and we're going to divide it by the standard deviation and then the last step here is we do a transpose just because later this will allow us to compare takers together as opposed to days so we could also not do a transpose and then the analysis wouldn't so much be comparing different tickers together but comparing days that the market was open and then the last step here is we're going to compute the percent return of each of the tickers because later when we generate this interactive network we can color the nodes in the network based on the percent return value of each of the tickers okay so all this talking and explaining and we still haven't really gotten into any topological data analysis so if we think back to that visual overview from earlier this is all still step one we're still getting our data so now we can finally get into the mapper algorithm stuff first we will initialize this object next we're gonna do step two in the process which is project our data into a lower dimensional space so we actually have 495 tickers here and what we're going to do is project that down into two dimensions and the way we do this is a two-step process first we use iso map from this manifold library in sklearn so that'll take us from 495 dimensions down to 100 then we'll use umap which will take us further from 100 down to two dimensions so the nice thing about this syntax is we can define a custom data pipeline to do our dimensionality reduction so we can see this projection keyword is being set to a list and this list is actually a list of functions element in this list is manifold.isomap with all the input arguments there and then the second element of the list is umap with all its input arguments but we could have gone further we could have added a third element and made that pca which took us from two components down to one component or we could have done a completely different data processing pipeline so you can already start to see that you have a lot of flexibility in using the mapper algorithm in practice so we essentially will combine steps three four and five from the overview earlier into one line of code so defining a cover clustering the pre-image and generating an output graph is all compressed down to a single function call in that we pass in the projected data from the previous step the original data set and we defined the clustering strategy that we want to use here we use the db scan with a cosine similarity metric yeah you can also customize the details of the cover but here we're just using the default values and in less than a second it generates the graph and so the next step here i define a file id which isn't really necessary i just like to do it because every time i've used the mapper algorithm i'll try different choices of cover i'll try different projection strategies i'll try different clustering algorithms and so on and i'll typically have these going in a for loop and i don't want the output graphs to get overwritten so i'll define this file id which will automatically generate a unique file name for each output graph and then the last step is we visualize the network you just pass in the graph you define a file name you can give the graph a title you can have these custom tool tips which is the label for each of the members so basically this is our ticker names we can define color values which we will define as the log percent returns we can give a name to the color function and then we can also have multiple options of how these color values are aggregated though we could just do a simple average we could compute the standard deviation the sum the max the min and so on and so what the output of the mapper algorithm looks like is something like this it actually generates a web page which allows you to interact with the graph and blends itself very well to explore to our data analysis which we're doing right now okay so the code that we just walked through will actually generate an html file which we can go ahead and open so first look this does not look like the network i showed earlier but if we go to this help menu we will find different viewing options so we can click e on our keyboard to do a tight layout and already it's starting to look a bit nicer and then we can click p for print mode which will just give the graph a white background and then next we can click on any node we like and it'll start to kind of radiate this glow and then we can go over to this cluster details click on this plus sign and so remember that the nodes in this network are actually clusters of data points and then the way we do the analysis here is we actually have clusters of tickers or in other words stocks so down here you'll see the names of the members of this cluster listed and so this was generated from that custom tool tip option in that last function call we made and then here we also have a histogram which is showing distribution of the log percent returns of the members of this selected cluster so right now the weighted average of the log percent returns of each cluster is what generates each node's color but we could use other statistics so if we go over here to this node color function we can click this drop down menu we could do the standard deviation which doesn't look too exciting we could also do the sum which also looks pretty uniform but then we could also do max so now we're starting to see some variation we then might be curious about the clusters that contain members with high returns so we can click on this yellow node here and then we can look at these ticker names and maybe do some further analysis so i'm no financial expert so i don't have much intuition to offer here but when working with data that you are familiar with you may immediately start to see interesting patterns just by jumping around and you can really do this all day you can click on a particular node see what members are in that cluster and then you can click on adjacent nodes and see what members are in those clusters and then you can go back try out different projection strategies try out different clustering algorithms generate new graphs and then repeat this whole process all right so that's basically it again the code for this example is freely available at the github if you want to learn more check out other videos in the series in the next video i will discuss another specific tda technique called persistent homology if you enjoyed this content please consider liking subscribing and sharing this video like many of you i am still learning so i would also appreciate your questions concerns and feedback in the comments section below and as always thanks for watching you
fpL5fMmJHqk,2022-05-21T19:15:28.000000,Topological Data Analysis (TDA) | An introduction,n/a
poGxnBR3hEU,2022-02-23T20:52:15.000000,"Multi-kills: How to Do More With Less (no, not by multi-tasking)",in life it can often feel like we have too much to do with too little time and this puts us in a tough spot because we need to figure out how to get more done in less time and this drives people to do crazy things like multitask we all know multitasking it's when you try to do two or more things at the same ish time but really all this is is rapidly switching between tasks and it might work for small things like listening to music on a jog or drive but this can easily get out of hand like trying to do your taxes while cooking dinner there's a lot of stuff out there about the evils of multitasking but to me multitasking isn't so much evil as it is just not an optimal strategy the biggest deal breaker for multitasking is that it splits your attention put another way it doesn't allow you to focus your attention and performance and learning tend to be best when we can really hone in on the task at hand it's like that old saying it's better than a whole ass one thing than to half-ass two things so multitasking isn't a good strategy how do we solve this problem of trying to get more done in less time one idea is we can seek guidance from our bird bird-hating ancestors and try to kill two birds with one stone at first glance this might sound like multitasking but it's not multitasking is like when you do two things around the same time while killing two birds with one stone is getting two things done with a single action the key benefit of this is that we can get more done with less and we're not just talking about saving time here which some might argue multitasking does since the proverbial stone can be anything it can be time money energy attention social capital whatever and on top of all that since you don't have to split your attention in half you can really focus on one thing at a time with this strategy so referring to this idea as killing two birds with one stone is tedious and limited so let's generalize it and define a multi-kill as a single action that results in multiple desired outcomes this word multi-kill might sound foreign but people do this all the time and here i'll share three concrete examples of multi-kills that will hopefully inspire more ideas that you can implement in your life right now the first example is one of my favorite multi-kills which i'll just call a cleaning break the power of breaks is something that took me far too long to figure out and a cleaning break is when you take a break from something mentally tasked take a break from something mentally tax taxing mentally taxing like saying this word a few moments later a cleaning break is when you take a break from something mentally taxing and doing something that's not mentally taxing like tidying up around the house so this could be doing the dishes doing your laundry brushing your teeth you know whatever something that is automatic something that you don't really have to think too much about and notice this isn't multitasking because if you take a break from your computer screen to go wash the dishes you're only doing one thing but you're getting two desired outcomes you're one taking a break and two you're getting clean dishes the second multi-kill boils down to this get paid to do things you do anyway this is like what every adult tells you when you're younger and older too find your passion they'll say things like love what you do and you won't work a day in your life what they also meant to say is love what you do and you'll be multi-killing life if you enjoy your work and get paid for it that's one of the best multi-kills there is and not to mention other things you can get out of your work like developing skills learning new things social interaction like that's like a five thing multi-kill right there this third and last example relaxes the multi-kill definition instead of killing two birds with one stone will use one stone to kill one bird and just slightly injure another so this is like a single action that results in completing one task and then getting started on another so i'll make this a bit more concrete with an example so you may have noticed that each of my youtube videos has a blog associated with them and this isn't just because i like writing and talking about stuff but these videos usually start out as blogs so when writing these blogs i one get a blog out of it and two i get an outline and a starting place for each of these videos so multi kills can be less strict than one might think at first so just to summarize i'll mention three key takeaways one multitasking is not an effective way to get more out of your time two a more effective strategy is multi-kills which is a single action that results in multiple desired outcomes and three we can incorporate multi kills into our lives to get more out of our time and i gave three concrete examples that might inspire ideas of multi kills that you can implement in your life so if you enjoyed this video and you want to learn more check out the blog on medium where i talk about this idea of multi kills more and give a few more concrete examples if you have multi kills that you do already in your life or just some ideas please drop them in the comments section below that would be very helpful to me and others who are trying to get more out of our time and if you enjoyed this content please consider liking subscribing and sharing this video and as always thanks for watching
6m82mLNDCyg,2021-12-29T21:11:41.000000,How to Be Antifragile | 7 Practical Tips,it's been said that the only certainty in life is uncertainty and unfortunately for us uncertainty doesn't seem like something that's beneficial but what if there was a way we could gain from uncertainty in his book anti-fragile author nasim nicholas tele defines something anti-fragile as something that benefits from things like uncertainty harm and disorder and the story goes something like this what's the opposite of fragile i'll give you a second a few moments later most people will say something like robust resilient tough tenacious etc but think of it like this suppose you're sending some very expensive champagne glasses to a good friend in siberia get all the packing material you put the glasses in there and you make sure the contents are securely packed and the box is labeled fragile handle with care the key thing here is this label of handle with care what does this imply this implies things that are fragile don't like don't benefit from variability or disorder or being shaken up so if things that are fragile don't like change don't like variability it's opposite should love it things like robust resilient tough tenacious they don't quite fit under this category something robust doesn't care if there's variability or stability something robust is apathetic toward its environment in the book anti-fragile celeb defines the opposite of fragile as anti-fragile and this is something that benefits from variability and volatility and all these things that don't typically sound like good things so these three concepts of fragile robust or something like it and anti-fragile form what naseem calls a trinity and there are three stories from greek mythology which highlight these three concepts the first story is the sword of democracy which is the story about a servant who sees the life of a king with all his gold all his power and wants nothing more to be in his position and so one day the servant trades places with the king and what he soon realizes is that it's not so great being king because when you have everything everyone wants to take your stuff everyone wants to take your power and the sword sitting above the king's throne is a symbol for the constant threat you are under when you are in a position of power so being king is an example of being fragile because when you have everything you have nothing to gain and everything to lose any changes in the environment and situation will probably not benefit you the second story is the story of the phoenix which is a bird who when it dies it rises again from its ashes and this highlights the concept of robustness the phoenix doesn't care if it lives in a stable tranquil environment or an erratic constantly changing environment because worst case scenario if the phoenix dies it'll rise again from its ashes the phoenix is apathetic toward its situation and the final story is the story of the hydra monster which is the monster that hercules fought and whenever he would chop off one of its heads three more would grow in its place the hydra is anti-fragile because the more you harm it the stronger it gets so when you hear this idea of anti-fragility and you've spent any time in the real world which is constantly changing a natural question is how can i be anti-fragile i'll talk about seven points toward this idea of anti-fragility so i'll just briefly talk about these seven points and i'll leave further exploration and discussion for future blog posts and videos so the first point is having more upside than downside another way to think about this is limiting your losses and leaving your gains unbounded and really this is the core idea behind anti-fragility naseem gives a more technical definition of anti-fragility and fragility in terms of something called convexity which i talk a little bit more about in the blog the second point is having options as opposed to not having options or in other words having a specific and detailed plan in life unexpected things inevitably happen if you sit down and chart out your whole life your five-year plan like this is exactly what my life's gonna be and this is exactly what i'm gonna get out of it you're setting yourself up for disappointment because i hate to break it to you it's probably not going to happen exactly the way you think right now so detailed plans and having no options is fragile while having options helps you be anti-fragile the third point is bottom-up traditions as opposed to top-down rules this isn't so much a specific protocol but is a potentially useful observation bottom-up tradition is evolution at work things get passed down through generations based on usefulness or some kind of appeal and then each generation adds their little twist and nuance to it and updates it based on their situation in other words traditions can adapt and evolve over time based on the needs of the people that practice them as opposed to top-down rules which are rigid it's kind of like having a detailed plan there's only one way in which the top-down rules typically work and if things deviate from how they were when the rules were written they're probably not going to give you the desired outcome the fourth point will sound kind of silly but it's failure failure makes you anti-fragile and success makes you fragile it's just like the king from the sword of democracy story the king is fragile because he's successful he has nothing to gain and everything to lose while failure and mistakes help you be anti-fragile because when you're a failure a loser when you have nothing no one expects anything from you you don't expect anything from yourself so you have a lot more to gain than to lose just like when you hit rock bottom when you hit rock bottom there's literally nothing to lose and everything to gain point number five is in a similar vein and that is staying small as opposed to being big when you're small your mistakes are small but when you're big your mistakes are big it's like that old piece of wisdom mo money mo problems the bigger you get the bigger your problems get what would have just been a small inconvenience when you're small becomes a significant thing when you're big so the sixth point is diversification as opposed to specialization diversification helps you be anti-fragile and this is just like having options when you diversify you're essentially giving yourself more options naseem gives a specific protocol in the book called barbelling which is kind of like this 80 20 rule people typically hear it in the context of business like 20 of your clients earns you 80 of revenue or just something like that when naseem calls bar billing is when you put eighty percent of your resources in a low risk situation while twenty percent of your resources in a high risk situation so a concrete example could be working a nine to five job that pays you a steady salary which you spend 80 percent of your time on but 20 percent of your time you spend on something more risky like a new business venture or side hustle the problem with specialization is the same problem as having a detailed plan if something happens your specialization is no longer needed and in some ways you are no longer needed or if you put all your eggs in one basket you just specialize all your assets you specialize all your resources into one venture you put all your money into that high risk side hustle business venture there's a chance that you're gonna lose everything the seventh point is this idea of independence as opposed to dependence on others when you're independent either financially emotionally or however you want to look at it you're setting yourself up to be anti-fragile while if you're dependent on others for a house a car money a job a hobby whatever it is you're setting yourself up to be fragile so what this ultimately comes down to is if you lose access to these external things to other people or other things you're going to find yourself in an undesirable situation well if you're self-reliant you're not dependent on external things you're independent you're setting yourself up to be anti-fragile because you're not going to lose there's no downside to when your environment changes so as usual i probably rambled on way too much so i want to leave you with three key takeaways the first is something anti-fragile is something that benefits from things like uncertainty harm and disorder the second takeaway is we can classify things into these three categories as fragile robust and anti-fragile and by classifying things into these categories we can study them and hopefully learn what we should do and should not do based on our goals the third and final takeaway is we can do things that make us more anti-fragile and i highlighted seven points that can help us on this journey so there's no way i could have gone into all the details of such a big idea in one video that's why this video is just part of a larger series of videos and blogs exploring this idea of anti-fragility and trying to answer this question how can i be more anti-fragile so if you enjoyed this video please consider liking sharing subscribing and commenting your thoughts if you have ideas or answers to this question how can i be more anti-fragile please leave that in the comment section i think others and myself would love and benefit from hearing that and as always thanks for watching
tufdEUSjmNI,2021-10-26T13:23:44.000000,Causal Discovery | Inferring causality from observational data,hey folks welcome back this is the final video in the three-part series on causality in this video i'll be talking about causal discovery which aims at inferring causal structure from data start by introducing what causal discovery is sketching some big ideas and then finishing with a concrete example with code in python so with that let's get into the video in the previous video i was talking about causal inference which aims at answering questions about cause and effect we talked about a lot of great things we talked about the do operator which stimulates interventions we talked about confounding which talked about estimating causal effects and all these things were great however there was one key assumption that was necessary in order to do causal inference which was a causal mod and obviously a lot of times in the real world we don't have a causal model in hand when we're starting our analysis and it's not always clear which variables cause which causal discovery is one thing that might help with obtaining a causal model and the goal of causal discovery is to find causal structure in data so basically given data inferring the underlying causal model so causal discovery is an example of a so-called inverse problem and inverse problems can be understood in contrast to forward problems for example imagine you have an ice cube sitting on your kitchen counter you know the shape of the ice cube you know the volume and if you were to let that ice cube sit there for a few hours you could probably predict with some reasonable degree of accuracy what the resulting puddle of water would look like the inverse problem is like the opposite of this in other words the inverse problem would be given a puddle of water on the kitchen counter predicting the shape of the ice cube that made that puddle and clearly this is a hard problem because there are several different shapes of ice that could create the same puddle of water connecting this to causal discovery the shape of the ice cube is like our causal model and the puddle of water is like the statistics that we observe in our data so following this analogy there are several causal models that could potentially generate the same statistics we observe in a given data set the approach to solving inverse problems is to make assumptions basically we narrow down the possible number of solutions through assumptions and although assumptions help they often do not fully solve the problem this is where we need to use some tricks to go a little further here i'll talk about three different tricks for causal discovery the first trick is conditional independence testing i start here with a definition of statistical independence which is shown here in other words two variables x and y are said to be independent if their joint distribution is equal to the product of their individual distributions from this we can get a definition of conditional independence which is basically the same thing however now we look at distributions of each variable when conditioned on a particular variable say z we can use this idea of conditional independence testing to do causal discovery and this is actually the main idea behind one of the first causal discovery algorithms called the pc algorithm which is named after its authors clark glymore and peter spears i probably butchered that so i apologize but there's a reference to a review paper by them at the bottom here so i'll just briefly go through the main idea of the pc algorithm more details can be found in the blog linked in the description the first step is to form a fully connected undirected graph so we have a node for each variable in our data set and we connect undirected edges between each of these nodes in step two we do pairwise independence tests so we do an independence test between every possible pair of variables and if two variables are independent we delete the undirected edge between them the third step are conditional independence tests so basically we do the same thing however we pick a variable to condition on then if two variables are found to be conditionally independent we delete the edge between them and we add that conditioned node to the separation set and we continue these conditional independence tests until there are no more candidates for conditional independence testing then in step four we orient colliders so if we have three variables say i j and k we form a collider out of them meaning we make directed edges pointing from i to k and j to k given k is not in the separation set of i and j then in step 5 we add more directed edges to the graph following two constraints namely we do not create any new v structures in our graph nor do we create any directed cycles and hopefully after all that we output a directed acyclic graph which represents the causal connections of our system again more details on the blog and the two references at the bottom here have a great description of the pc algorithm so trick two is a greedy search of the dag space so there are three key concepts here first is a dag which should be familiar since they've been discussed in the previous two videos next is a dag space or in other words the space of all possible dags for example consider the space of dags with two nodes in one edge which is shown here there are only two possibilities x could point to y or y could point to x then finally we have the notion of a greedy search which is a widely used idea in optimization in short a greedy search is an optimization strategy that picks what's best in the short run as opposed to the long run and this is usually done using a heuristic or rule of thumb for example suppose you're trying to get out of a forest you may think i'm trapped in a forest forests have trees so to get out of the forest i should go where there aren't any trees in other words every step you take should be in the direction with the least number of trees so you repeat this strategy and go all the way along this black line until you finally get out of the forest which we can call the greedy path because it is the result of a greedy search however if at the start you were to go in the exact opposite direction of the greedy path you would make it back to civilization much faster so you might say why would we ever want to use a greedy search from the look of it they just seem to give some optimal solutions well the problem is that a lot of the time computing the optimal solution is intractable meaning if you ran an algorithm that tried out all possible solutions all possible paths out of the forest and compared them to each other you would be waiting a long time for it to run and maybe your grandkids would see the solution in their lifetime and this is the problem we face in causal discovery we want to find the optimal dag that best explains a given data set the problem is if we tried an exhaustive search we have to deal with the fact that the number of possible dags is a super exponential and the number of nodes in a graph in other words if we have just three variables the number of possible dags is 25 if we have six nodes we're already over three million possibilities and if we have a measly ten variables or ten nodes in our graph the possible dags is on the order of 10 to the power of 18. so even though greedy searches do not guarantee the optimal solution at least they give us a solution in a reasonable amount of time and so a causal discovery method that uses a greedy search is the so-called greedy equivalence search algorithm so the basic idea of this algorithm is you start with a complete unconnected dag and you iteratively add edges to this unconnected graph such that you maximize a score value so in other words you start with a set of nodes that correspond to each of your variables but no edges between them then you add edges one by one according to some score so the question is what is the score that i'm talking about basically this quantifies how good your dag is or how well the dag explains the data so there are a few options to defining this score one is the so-called bayesian information criterion and source number one reference at the bottom here has a brief discussion for anyone that is interested then you can repeat this process until you reach some stopping criterion which could be some number of edges have been added or the score stops increasing or whatever that may be okay so the final trick is exploiting asymmetry and so as i discussed in the first video asymmetry is a fundamental property of this causality framework so it's natural to think maybe we can leverage asymmetry to help us find good causal models from data and there are three flavors of asymmetry that i've come across and i'll say as a disclaimer that these aren't any kind of standard classification for these things these are just some labels i'm putting on some themes that i have gleaned from looking at this stuff so the first flavor is what i call time asymmetry which is based on the idea that causes precede effects this is what is used in granger causality which is a method to quantifying a asymmetric relationship between two variables based on prediction and more information about grainger causality can be found in this first reference here and there's a lot of stuff out there on grade your causality you can just do a simple google search and you'll probably find a bunch of stuff the second asymmetry is what i call complexity asymmetry which is basically the occam's razor principle that simpler models are better so going back to our ice cube example from earlier following this principle we would say the ice cube that's actually cube is preferred over the more complicated eyes because it is simpler and finally the third flavor is what i call functional asymmetry where better functional fits are better candidates for a causal model so one method that uses this is the non-linear additive noise model the way this works is suppose we start with two statistically dependent variables x and y we then model y in terms of a non-linear function of x then we compute a noise term by taking the difference between y and this non-linear functional fit and then finally we test whether the noise term n is independent of x if it is we accept the model and say x causes y and if not we reject it and then we can do the same thing in the opposite direction where we model x in terms of a non-linear function of y and repeat the same procedure and more details on this method can be found in reference number three and generally all of these are great resources if you're trying to learn more about causal discovery okay so wrapping up these tricks we have a trick based taxonomy and it's important to note that these tricks are not mutually exclusive in all cases there are indeed algorithms that will mix and match different ones for causal discovery as shown in the bottom row of the table here and as a bit of commentary causal discovery seems to me at least to be a relatively young field so there still has not emerged a single or small set of causal discovery algorithms that beat out all others in all situations and i'll also say that this is by no means an exhaustive list of causal discovery techniques however this is probably a good start for anyone trying to get into causal discovery and the references given at the bottom here can get the ball rolling for you okay so i will conclude with a concrete example like in the previous video so we're going to be using the same census data set as before but instead of having just three variables of age education and wealth we're going to include more variables and instead of using the microsoft do y library for causal inference we're going to be using the causal discovery toolbox so again first step is importing libraries loading data then for a lot of these causal discovery algorithms it helps to start with a so-called graph skeleton so this is like step two that we saw with the pc algorithm where we do the pairwise independence testing and we have bi-directed edges or undirected edges between variables that are statistically dependent and then you can visualize the network pretty easily using network x so the first causal discovery algorithm that i use here in this example is the pc algorithm so again we just do that in two lines and it spits out this causal graph the graph is somewhat reasonable it's not perfect but we can see that we have has graduate degree which is like our education variable causes a variable greater than 50k which is our income variable and then we also have age causing our income variable which is what we expected but what was not expected is our education variable has graduate degree is pointing toward age so this is saying whether or not someone has a graduate degree has a causal impact on their age which is not true if you give someone a graduate degree it's not going to have any effect on their age another interesting thing is we have several variables having a causal effect on the number of hours that someone works in a week so whether or not they have a graduate degree has a causal effect on the number of hours they work their age has an effect and whether or not they're female so this is basically their two options male or female in this data set and then the ethnicity information captured by is white is a bi-directional so the pc algorithm wasn't able to break that symmetry but what's interesting is uh hours per week causes a single variable which is in relationship so what this is saying is the number of hours you work per week has a causal effect on whether you're in a relationship or not so we could look at this all day and kind of craft whatever stories we want in our minds but this should definitely be taken with a grain of salt so the next algorithm that we try out is the greedy equivalent search algorithm which uses trick number two greedy search of the dagspace and this gives us a causal graph that is somewhat similar to what the pc algorithm gave us notably that edge between hours per week and is white the symmetry was broken so it's not a bi-directed edge and then finally we use the lingam algorithm and this one doesn't really give us something sensible it's basically everything is causing past graduate degree so whether you make more than 50 000 impacts your graduate degree how many hours per week you work has an impact on your graduate degree and these edges seem backwards this algorithm doesn't seem to do a great job and that's because it's assuming linear relationships between variables and since most of these variables are boolean that's not something that necessarily makes sense code can be found at the github linked at the bottom here put the link in the description feel free to take this data run with it feel free to leave a comment i'd be interested to hear the results of your analysis so that concludes our series on causality we started in the first video introducing this new science of cause and effect the second video we talked about causal inference and finally in this video we concluded with causal discovery if you enjoyed the series please consider liking subscribing sharing commenting your thoughts if you're interested in learning more check out the blog check out the github to get your hands on the example code discussed in this video as always thanks for watching [Music] you
PFBI-ZfV5rs,2021-10-15T22:02:16.000000,Causal Inference | Answering causal questions,n/a
WqASiuM4a-A,2021-10-04T20:02:15.000000,Causality: An Introduction | How (naive) statistics can fail us,hey folks welcome back i'm finally sharing another data science series this video is the first in a three-part series on causality so this idea of causality is mainly based on the work of judea pearl and other researchers working this space pearl actually has a very accessible book out called the book of why geared toward a public audience which i will share in the description in this video i will introduce this idea of causality kind of highlight why traditional statistics isn't the most helpful for understanding it and then finally introducing a new mathematical formulism for understanding causality if you want to dive a bit more into the details check out the blog which i will link in the description and without any further ado let's get into the video well you're probably thinking is why is there a banana on the screen why did i click on this youtube video so we are constantly asking ourselves why why did this happen what is the cause of this or where is this going what's the effect we ask ourselves why to help us craft stories narratives to help us make sense of the world and even though this is a very natural thing for us in understanding and reasoning one of our most powerful tools in statistics is in many ways inadequate for handling cause and effect i'll try to highlight these inadequacies with what i call the three traps of statistics the first trap we have is spurious correlation so this is a statistical correlation with no causal implication so this is like the old saying correlation is not causation and you don't have to look far to find examples of this uh there's a website uh tylervegan.com i have it at the bottom left here and i'll link it in the description as well so here we have a case where we have a spurious correlation so the number of people who drown by falling into a pool correlates with the number of films nicholas cage appeared in so even though this relationship is hilarious it is not causal because we know these two things are not causally uh related to each other correlation is not causation which is something that we all know so sperry's correlation is pretty well known we've all heard correlations not causation uh however trap number two is less well-known and this is simpson's paradox which basically um highlights that how you look at your data matters so let's imagine we do a study for an experimental treatment for heart disease and we collect a bunch of data and we plot it so on the x-axis we have our experimental treatment this could be a drug or behavioral protocol the y-axis we have risk of heart disease and if we look at the plot we would say to ourselves this is a terrible treatment for heart disease it seems the more treatment someone gets the higher the risk of heart disease however if we were to look at two subpopulations say men and women we would get the exact opposite effect so this is summarized nicely by a quote from the man himself judea pearl who said we have a treatment that's good for a man good for a woman but bad for a person here's another example of simpson's paradox but with numbers i took this from the wikipedia page on simpson's paradox so here we have batting averages of derek jeter and david justice over the years 1995 and 1996 so if you look at those two years individually you see that david justice has a better batting average but if you were to combine those two years together derek jeter has a better batting average so again how you look at your data what variables you condition on how you slice your data set has an impact on the conclusions that you can make the final trap of statistics is symmetry which from many perspectives isn't much of a setback but when you're talking about something like causality which is inherently asymmetric it can cause some issues so i'll highlight this by an example let's say we want to model the causal effect between a disease and the severity of symptoms so we model this by a linear expression so y is the severity of our symptom x is the severity of a disease b is all other factors involved and m is just a coefficient that relates x and y but here we have an equal sign so the left left-hand side equals the right-hand side that's what equals means so that means using algebra we can rearrange this expression to get a equation of x in terms of y but here's the problem if we interpret the first equation as diseases cause symptoms then we have to interpret the second equation as symptoms cause disease which is not true we know that's not true so this fundamental symmetry makes algebra perhaps not the best formulism for representing causality so this whole video is supposedly an introduction on causality and i have not defined what it is there are few ways we can define causality the one i like is x causes y if when all confounders are adjusted an intervention in x results in the change in y but an intervention in y does not necessarily change x so i have a little cartoon here let's say we have four variables x w z and y if we intervene in x that means we jiggle it a bit we if x causes y we'll see why jiggle as well however if x causes y but y does not cause x if we intervene in y that is we jiggle y a bit x will not respond so that's causality it is fundamentally asymmetric so if we can't use algebra which relies on symmetry it has this equal sign how can we represent causality so there are the so-called structural causal models which is the kind of way we can represent causality and this consists of two parts one is a directed acyclic graph or a dag so this is a type of graph which comes from the mathematical field of graph theory which consists of vertices these circles here and edges which are these arrows and this is called a directed graph because the lines connecting the different uh circles together have arrowheads on them so that's called a directed graph because the information so to speak flows in one direction and it's acyclic because if you start at a vertex or one of these circles and you follow the arrow heads you'll never return back to the same variable or the same vertex so that's a directed acyclic graph and then there's a second part which are the structural equation models so these are equations that kind of outline the details of the causal connections and so they have these funny looking equal signs here which is basically saying you can't invert these expressions for example you can't invert f sub 1 to get an equation for x in terms of w so these are two key pieces of causality so that was the first video in the three-part series on causality in the next video we will be applying this idea to answering practical real world questions with causal inference if you like this video please consider liking subscribing sharing and commenting your thoughts if you're interested in diving a bit more into the details check out the blog on medium thanks for watching
m19FqRrmvIE,2021-07-18T23:47:21.000000,Why Conflict Is Good & How You Can Use It,n/a
OmNVB3ff98s,2021-05-10T20:13:32.000000,"Shit Happens, Stay Solution Oriented",n/a
zJsl4lFyr6w,2021-04-05T19:19:10.000000,Kmeans-based Blink Detecter DEMO,n/a
GgLaP4Des1Q,2021-03-17T14:53:33.000000,Independent Component Analysis (ICA) | EEG Analysis Example Code,hey guys welcome back this is video number two in the two-part series on principal component analysis pca and independent component analysis or ica ica is the topic of this video so much like the last video i'll start with a brief introduction into the technique dive into the math a little bit i will compare the two approaches talk about their similarities and differences and then i'll finish with a concrete example of how you can use ica with some example code provided in the github repository so let's get right into it okay so the standard problem for independent component analysis is the cocktail party problem so in its simplest form you can think of two people having a conversation at a cocktail party and for whatever reason you happen to have two microphones kind of set up next to these two speakers both microphones are going to pick up audio from both of the speakers uh kind of like our purple microphone and pink microphones here the purple microphones a little closer to the blue speaker so it picks up more of the blue speaker's audio relative to the red speaker and vice versa the pink microphone picks up more audio from the red speaker than the blue speaker so then the problem is how can we take these audio recordings that have both speakers kind of side of the conversation mixed together and separated out um to two audio files uh each of which only contain audio from a single speaker well that's exactly what independent component analysis does it trans uh transforms a set of vectors so you can think of the raw or you can think of the recorded audio by these two microphones into a maximally independent set so so that's what's being represented here um so you have the purple and pink audio signals then they get translated uh to the original the sources of the audio which was uh the speech from the blue speaker and then the speech of the red speaker respectively so again the purple and pink are your measured signals the blue and the red are the independent components or the source of the information or the audio okay so there are a couple key assumptions to independent component analysis so assumption number one your independent components are statistically independent and that's defined in the typical way in statistics the joint distribution of two variables x and y is equal to the probability distribution of x times the probability of y and then the second key assumption is that your independent components are non-gaussian which might be a little strange you know in statistics and science we love to say everything's gaussian it makes things much nicer and it allows us to do a lot of rigorous analysis but this is one of the instances where we actually need the independent components to be non-gaussian for this to work okay so we have our measured signals again that's from your microphone example and then the independent components which is what your speakers are saying in the cocktail party problem so we can use our independent components we can combine them in some way so that's what's being represented by this expression to kind of recreate our measured signals x you can think of the independent components as being sources hence that's why this vector is an s they are sources of information or audio that are being combined in some way to generate what's being measured at your microphone for example so we have x1 and x2 your measured signals and then your independent components or the sources of your signals s1 and s2 but you can also kind of turn this around and you can combine your measured signals to express your independent components if this is the case if you can just have some linear combination of your measured signals to derive your independent components the set of values defined by w is all we need in order to do ica okay so mathematically the goal is as follows so given some uh measured signals given some data x we want to solve for the matrix w such that the set of independent components or the set of source vectors s sub i are maximally independent this concept of maximally independent what does that mean how do we quantify that so there are two ways you can define w in such a way that it minimizes the mutual information between all your independent components or you can maximize the non-gaussianity of the independent components defined by uh this w matrix um i'm not going to go any further than that if you're interested in more information check out the blog post linked in the description i kind of go into a bit more detail on the math pc and ica they're similar techniques in a lot of ways but ultimately they're distinct they are different approaches that kind of aim at different tasks or they make different goals so pca typically compresses information if you saw the pca video the example was hot dogs and hot dog buns those are two quantities that are heavily correlated so instead of representing that information with two variables you can represent it with just one and so that's where pca is a good thing to use because it will compress those variables into those two variables into a single variable on the other hand ica separates information it's going to take two variables for example like your two speakers or the the audio picked up by two microphones placed close to two speakers and it's going to separate out the independent components or the sources or the independent drivers of those measured signals so kind of similar but they are different goals and different final outcomes a commonality between pc and ica is auto scaling so this is a critical part of the preprocessing so auto scaling is for each variable you have to subtract the average of that variable and divide each element by the standard deviation of that variable and then that's one of the reasons why it's typically advantageous to apply pca to your data set before applying ica because it kind of all the pre-processing is already handled for you pca will clump all the information together the correlated variables and then ica will come in and separate out independent drivers if it's applicable okay so as always i'm going to include a concrete example so here this is something that that's relevant to my research and this is where i kind of came across the whole technique of independent component analysis was to solve this specific problem in my research we deal with uh eeg data so what is eeg uh eeg is a technique of measuring brain activity uh by placing a set of electrodes on the head you know eeg is a very powerful technique because it has a very good temporal resolution and it's also non-invasive people can kind of move around with the this cap on but that kind of also leads to one of its more fundamental weaknesses is that since the electrical signals that it's trying to measure from the brain are so weak eeg has to be very sensitive to these kind of fluctuations and voltage which makes it very prone to artifacts or perturbations oscillations in your signal that do not come from brain activity so this could be like blinking which is the what we're trying to resolve in this problem or motion artifacts people talking or other kinds of noise that can kind of get injected into the data so here we have a plot of the voltage versus time sorry i should have had labels on these axes but the y-axis is voltage in millivolts the x-axis is a time index essentially and this is for the fp1 electrode which sits near the front of the head on the left side so on your left forehead and so this electrode is particularly prone to blink artifacts because it's the one of the closest electrodes to your eye and then you can actually see the blinks occurring because you'll have these giant spikes in the signal so we're trying to get rid of that because with eeg we're trying to measure brain activity not blink activity okay so the first step here is applying pca so here we have 64 electrodes on our eeg so that translates to 64 variables so we can use pca to kind of clump that down to just 21 variables and i just did some trial and error to find the right number of principal components to go down to and at the bottom here you can see the explained variation is 99.5 percent and in matlab it's really simple you may notice i don't explicitly auto scale the data that's because the pca function in matlab does this automatically which is pretty nice but it's all done in one line in matlab and it can be done in one line in psyc or a couple lines in psychic learn and if you didn't check out the previous video on pca that'll share some example code on how to do that okay and then again we can apply ica to the set of principal components that we got from pca so that's what's being done here so now we can just plot all the independent components okay so again we had 64 electrodes on our eeg cap that translates to 64 variables we then use pca to reduce the dimensionality from 64 variables to just 21 and then finally we applied ica to those 21 variables to kind of separate out the independent components and then just looking at this visually uh kind of independent component 10 5 and 12 are reminiscent of those blank artifacts we saw in that initial plot and again these aren't the just the independent components themselves or the raw independent components i actually squared them so that all the values would be positive and then the blank artifacts would be a bit more prominent okay so i just used a rough heuristic basically i picked out the independent components which had four prominent peaks and so this isn't a robust way to do it i was doing something fast and wanted it to be repeatable so it picked out independent components 10 and 12 to correspond to the blanks okay so 10 and 12. i'll buy that maybe five should have been included but we'll see how it turns out okay and then we can essentially just drop independent components 10 and 12 because they contain blink information which we're not interested in we only want brain information so we can drop those two components and then just work backwards we'll reconstruct our score matrix basically the output of pca and then we can reconstruct our original 64 variables by going backwards in pca so doing that and plotting everything before the blink removal fp1 had these four prominent peaks corresponding to blinks and then afterwards uh they went away so it's kind of like magic and this is a rough way to do it there are other ways to do it but this is more so just as an example of what ica can be used for so that concludes the two-part series on principle component analysis and independent component analysis if you found this video helpful please like subscribe comment share i would very much appreciate that and i would love to hear your feedback i look forward to seeing you guys in the next video and thanks for watching
WDjzgnqyz4s,2021-02-22T19:39:27.000000,Principal Component Analysis (PCA) | Introduction & Example (Python) Code,n/a
MX7ymkYGiZ0,2020-12-21T00:24:45.000000,The Wavelet Transform | Introduction & Example Code,hey guys welcome back in this video we'll be talking about the wavelet transform hey guys welcome back uh this is the final video in the three-part video series on the 48 in wavelet transforms in this video we'll be talking about the wavelet transform which basically takes the ideas we learned in the fourier transform and extends it to a thing called wavelets so let's get right into it alright so in the last video we were talking about the 40 transform which was basically decomposing any signal into a basis of waves you can do the same basic idea but instead of waves you use something called wavelets so wavelet is simply a wave-like oscillation that's kind of localized in space or time whatever your x-axis is so here's an example we have the first derivative of the gaussian function or a bell curve and the equation is given here to the right of the plot so there are two basic properties of wavelets just like there were two basic properties of waves amplitude and frequency wavelets have two basic properties which are scale which is basically how stretched or squished our wavelength is and location since wavelets are localized and not extending from negative infinity to positive infinity like waves we need to know where the wavelet is located in space so here are some examples of kind of our example wavelet at different scales so if it has a smaller scale we're essentially squishing our wavelet if it has a larger scale we essentially stretch our wavelet and this is controlled by the parameter a in our first derivative of the gaussian function given above and then here's the same example with different locations so we can shift our wavelet a little to the left by changing this by tuning this b parameter or we can do this double whammy situation where we shift our way to the right and we squish it and then here's a reference if you want to learn more about wavelets okay so what's the wavelet transform so just like before with the fourier transform we decomposed our signal into a basis of waves we can decompose our signal using wavelets of various scales and locations so the basic idea is you get a wavelet you pick a scale and then you just slide it across your signal and then at every kind of time step or every location in space you multiply that wavelet by your signal and then you do this for one scale then you pick another scale and then you just keep going like that so here's a visualization of what it looks like and if you're familiar with convolutions that's exactly what we're doing here uh you're just convolving your signal and wavelets of different scales so why are wavelets useful if the fourier transform is so powerful as i was uh going on and on about in the first video uh why do we need the wavelet transform well one thing about the fourier transform is that it gives you global frequency information so uh sines and cosines are defined from negative infinity to positive infinity so it gives you a kind of global average of the frequency information in your signal and if you're interested in uh kind of oscillations that happen over a short time scale the 40 transform may obscure that information and that's exactly what the wavelet transform is good at extracting localized information on the second point the wavelet can not only extract the local spectral information but it can also extract temporal information at the same time which is really nice so you kind of have this trade-off of frequency information and time information but the wave of the transform is kind of like a happy medium of uh between those two and there's also a variety of wavelets to choose from so let's say the signal the sub signal inside of your audio signal that you're interested in has a characteristic shape then you can kind of choose your wavelet in a clever way such that it kind of matches the signal of interest the thing you're trying to extract from your signal and then this is just a lot of math i'm not going to spend too much time here but there are two basic kinds of wavelets there's the continuous wavelet transform and the discrete wavelet transform the major difference is the continuous wavelength transform you pick basically every possible scale and location to do your wavelet transform so you do scale 1 and 1.1 1.2 whatever your resolution is every possible scale uh while the discrete wavelet transform there's only a discrete number of wavelets uh discrete number of wavelet scales and locations that you use to do your transform and then here's just you know just some more general information wavelets create a complete set which means you can represent any function in terms of wavelets which is nice okay and then we're gonna try to bring everything together with a concrete example so this is something relevant to my research which is uh extracting our peak information which is basically the biggest peak you see in uh your favorite medical tv show in ecg which is basically the um electrical activity resulting from your heart beating so we can extract this characteristic our peak or this giant jump in the ecg signal using wavelets so we're using a specific kind of discrete wavelength transform called maximal overlap discrete wavelength transform in this example and you can find the this example code with all the plots and a nice pdf of it at the github link here okay so the first step is we do the wavelet transform here i picked a number of levels or a number of different scales to b6 and then we use the sim 4 wavelet for our wavelength transform so at the top we have our original signal you can kind of see the characteristic the signal of interest which is like this localized peak that's the rpeak we're interested in but the signal is really noisy so it's kind of hard to you know do a simple fine peaks function to extract the rpeaks but if we do this wavelet transform we can see at the first scale the smallest scale which will correspond to the highest frequency because our wavelength is very squished it just looks like a lot of noise so that's not really helpful we go to the next scale we can kind of see the rpeak signal coming out a little bit but still really noisy the next scale is a little bit better um the fourth scale is kind of the goldilocks we we see a really large oscillation near our peaks and then everything else is basically uh zero so that's promising and then we just keep going down and then you can see for the largest scale we have like this low frequency oscillation which makes sense big scale means lower frequency so the rest of the code is just making this plot here so now we can reconstruct our signal using the inverse maximal overlap a discrete wavelet transform uh so that's what that's what's happening here um you know just to run through the code really quickly we are only going to use that fourth scale which was oscillating a lot around our peaks and it was zero everywhere else so that was really promising so we're only going to use that information to reconstruct the signal and then matlab and a lot of programming languages there's probably equivalent functions in uh python or r uh that let you do the inverse maximal overlap discrete wavelet transform in one line so we're going to reconstruct it using only the fourth scale the two to the fourth scale and then we're going to use the sim 4 wavelet again and then the rest of it is just plotting this this image on the right here so the on the top we have the original signal so it's pretty noisy but you can still see where the rpgs are visually but then our reconstructed signal is a lot better behaved and you can see it's almost trivial um picking out the r peaks in the reconstructive system and then this last bit of code is just leveraging that so our reconstructed signal it's really easy to pick out the art peaks so we can just do a simple fine peaks function and then we can plot those peaks on our original signal and it does a reasonable job and so again you can find this code on the github if you want to take a closer look at it if you want to just completely steal it that's completely fine with me i kind of took a lot of this code from a matlab example that did a similar thing with a different data set so i encourage you to take a look at that so that was wavelets i hope you guys enjoyed these three videos found this video helpful like subscribe comment i'm always trying to get better improve my understanding learn grow all that fun stuff yeah thanks for watching you
rPUytg38b6Q,2020-12-04T01:10:36.000000,The Fast Fourier Transform | How does it (actually) work?,hey guys welcome back in this video we'll be talking about the fast fourier transform if you missed the last video it was a bit of a primer introduction on time series signals and the fourier transform so if you missed that definitely check that out and stay tuned for the next video which will kind of extend this idea to a thing called wavelets in the wavelet transform so let's get right into it alright so in the last video we were looking at the fourier transform so if you give it some function in terms of time or space it'll spit out a function in terms of frequency or wave number but most of the time we don't know a functional form of anything we're dealing with in the real world so that's where this idea of the discrete for you transform is very useful like we were looking on the previous video you can have an audio signal so you're playing your favorite record you record it with a microphone and then you can kind of digitize it so you have it in a form that your computer can understand and just how you can digitize your audio signal you can discretize your fourier transform so basically you convert your infinite integral to a sum over n elements so let's take a closer look at this expression so we have f f sub k is equal to the sum over zero to big n minus one uh with x sub n times e to the minus 2 pi i kn over big n so we can define this term let's call it big r sub k n and then we'll set that equal to e to the minus 2 pi i k n divided by big n so we can rewrite our sum in terms of these two elements and then just so we're all on the same page f sub k is an element of what i'll call a column vector x sub n is an element of a column vector and then r sub k n is an element of a matrix so because of the nice properties of matrix multiplication it turns out we can write our sum equivalently as matrix multiplication so that's nice discrete fourier transform is just matrix multiplication but so what how does this help us if we're coming to code this whole thing up we're still going to naively need two for loops to do this double sum this is what they call n squared time so that means the time complexity of this operation will take n square so if you have n elements you're going to have to do n squared computations so we converted our discrete fourier transform to matrix multiplication didn't seem entirely useful but let's just consider a concrete example so let's assume the case where big n equals big k so big n is the size of our column vector where x sub n was an element of and big k is the length of the column vector where f k is an element of that uh so we recall that big r sub k n is equal to e to the 2 pi i k n over big n and then we can just plug it in so we can plug in k and n which will correspond to the indices of our matrix and then big n is equal to four so we plug in those values and we get this matrix on the right here lots of ones lots of minus ones lots of eyes and minus sighs and just looking at it and this matrix is symmetric which means that if you were to swap the rows and columns it would be the same matrix as you started with so that's pretty nice and then this is a special element in the matrix so this element kind of defines the resolution of the discrete fourier transform so we can call it something special like k9 and then we can write our any fourier matrix in terms of this k naught and so this is for the four by four case that we have here and then we can write a few more examples so we have the eight by eight case which uh looks like that big scary matrix on the top there um where k naught is defined on the right hand side here uh we have the four by four case which is the same thing as before and then we have the two by two case and then again k naught is defined as e to the minus two pi i over big n and this extends to any fourier matrix of any size now where n where your fourier matrix has to be square and n gives you the dimensionality of that square matrix so this is the trick so we had the n squared there was a lot of symmetry in our four-year matrices we saw the symmetric matrix there were a lot of redundant terms a lot of ones a lot of minus ones a lot of eyes minus size uh in the 2x2 case the 4x4 case the 8x8 case so it would be really nice if we could leverage this redundancy and symmetry to make our algorithm a bit more efficient and that's exactly the idea behind the fast fourier transform so it's just a fast way of computing the discrete fourier transform and the basic idea is given by this expression so r sub n is our n by n for your matrix any fourier matrix you want that's n by n with the added caveat then n is a power of two that's an important point um and then it turns out you can write any for your matrix uh that's m by n by this expression here i can define these terms individually but i think it's better to just look at a concrete example so again let's return to our four by four case um so here we have i sub big n over two so big n is four so we're looking at i sub two and that's just our two by two identity matrix then we have d sub two which will be the first two diagonal elements of the four by four fourier matrix so we have uh one and minus i respectively and it's a d is defined to be a diagonal matrix so we only keep the diagonal terms from the fourier the four by four fourier matrix and we set everything else to zero and then finally we have the 2x240 matrix which i flashed on the previous slide but it turns out to be this and then the last thing is we have this permutation matrix which is defined by this expression on the right here so basically you if you have a permutation matrix and you multiply it some vector by it it'll just reshuffle the elements in your vector such that the even indexed elements are in the top half and the odd indexed elements on the bottom half okay so it's just plug and chug so we're just plugging in the two by two identity matrix the two by two diagonal matrix and these slots here we plug in the 2x240 matrix here and here and then we have our 4x4 permutation matrix so it's just we're just plugging into this expression um and it may not be immediately obvious but one thing that is promising is that these matrix have a lot of zeros in them which is good we like zeros because that means there's no multiplication to do so on the left hand side again the time complexity is n squared uh but it turns out on the right hand side we don't have n squared time complexity uh so let's take a closer look this permutation matrix is basically free so if you represent your data in a smart way in a nice data structure and use a good algorithm you can basically do this in one step for this first matrix half of it is just an identity matrix basically and then the other half is two diagonal matrices stacked on top of each other so that means half the elements so in this case 40 elements are zero so that means there are only four non-trivial terms we have to worry about so that's where this time complexity of order n comes from and then for this middle matrix half the elements are zero so there's only um two n or eight elements that we have to worry about uh so it turns out as n gets larger and larger this gives you uh you can devise an algorithm that has n log n time complexity uh so the fast fourier transform is just efficient matrix multiplication and so you know it might not be immediately obvious why this is order n log n but i want to give you some uh intuition of what's going on here so let's look at another example we have the 16 by 16 for your matrix which is given we're just plugging into the expression that i showed on the previous slide but there's nothing stopping us from playing the same game for the eight by eight fourier matrix in this middle matrix right here so we can plug that in and then we get a whole bunch more zeros again we go from uh n squared uh time complexity to order n time complexity plus order two n time complexity plus constant time complexity and then we have a 4x440 matrix here so we plug that in and we pick up even more zeros so this is kind of the intuition of what's going on we just recursively apply this matrix multiplication we rewrite our kind of n by n for your matrix in terms of the n over 2 by n over 2 for your matrix and some other terms and we get the n log n time complexity let's try to bring everything together with a concrete example uh so i wrote this example in matlab the code is available at the github page linked here and in the description so we're going to look at the spectrum of an audio signal so in this case i'm just playing the low e string on electric guitar so we can see what that looks like the first step is we read in our audio file then in one line of code in matlab we can apply the fast fourier transform uh here we're just defining some terms we're getting the length of the audio signal we're converting our frequency indices to actual frequency values uh we're computing the two-sided and then from the two-sided the one-sided power spectrum in these lines here and then we just plot everything so at the top we have our audio signal and then the bottom we have our fourier transform so you see we have these kind of discrete peaks at different frequencies so the first one is e2 the low e string 82 hertz so that's what we expected but the cool thing is we're getting exactly or approximately the harmonic series so you're just getting integer multiples of the fundamental so the fundamental frequency in this case is a 282 hertz and then we're just getting integer multiples of it so we start with 82 164 247 330 415 and so on and um you know as someone that is into both physics and music it's really nice when those two fields come together and if you keep going down the harmonic series it turns out you approximately get a major scale which is pretty cool for the physics and slash music personalities that exist inside my brain so that was the fast for you transform if you found this video helpful like subscribe comment i'd love to hear your feedback stay tuned for the next video which will extend this idea to a thing called wavelets in the wavelet transform thanks for watching
mj86XmfOniY,2020-11-15T20:41:53.000000,"Time Series, Signals, & the Fourier Transform | Introduction",n/a
Gwz4zXPeP_Q,2020-11-12T22:58:00.000000,biometricDahboard3 DEMO,n/a
lciC1s4FO0g,2020-09-23T13:02:57.000000,biometricDashboard2 DEMO,n/a
